{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Topics and Concepts\n",
    "------------------------------\n",
    "**General Concepts**\n",
    "* ANOVA/t-test\n",
    "    * hypothesis testing - comparing means/variance between groups  in a data set\n",
    "    * error between groups is greater than error within groups = significant difference\n",
    "* p-value\n",
    "    * used in hypothesis testing\n",
    "    * probability of getting observed value of the test statistic or greater if/assuming the null is true. \n",
    "    * null is true means theres no difference between groups\n",
    "* Conditional Probability\n",
    "    * probability of A given B\n",
    "\n",
    "* Central Limit Theorem\n",
    "    * distribution of sample means approaches normality, the greater the amount of sample means\n",
    "    * most statistical analysis relies on assumption of normality\n",
    "    * allows us to model data on samples, despite not always being a normal distribution\n",
    "* SQL\n",
    "    * WHERE vs HAVING \n",
    "        * order of operations: SELECT, FROM, WHERE, GROUP BY, HAVING, ORDER BY\n",
    "    * JOINS (Left (left circle including overlap), Inner (overlap), Full Outer (everything), can specify nulls to exclude overlap)\n",
    "    * e.g. list of employees .. \n",
    "* Normality - Normal Distribution\n",
    "    * data is symmetrical around a mean \n",
    "    * has a bellcurve shape, with values on the x and frequency or counts on the y\n",
    "    * data around the mean are more common than data further from the mean\n",
    "    * at perfectly normal - median and mean are the same\n",
    "* Law of Large Numbers\n",
    "    * greater number of trials/observations - the closer the approximation to the population statistic or the expected value\n",
    "\n",
    "\n",
    "#### Feature Engineering \n",
    "Outliers\n",
    "* remove outliers - based on domain expert or intuition, skews data\n",
    "* visualizing single/multiple variables - boxplots/scatterplots\n",
    "* create z-scores and eliminate based on 3+ SD \n",
    "    \n",
    "Missing\n",
    "* ensure actually missing or encoding is correct \n",
    "* drop if not significantly affected\n",
    "\n",
    "Data Prep\n",
    "* Log transforms (for skewed data, removes extreme values)\n",
    "* Binning (reduce features)\n",
    "* Scaling (0-1) \n",
    "* Dummy variables\n",
    "* Caclulating new variables from existing ones\n",
    "\n",
    "#### Linear Algebra\n",
    "* multiplying matrices\n",
    "    * the rows of A must be equal to the columns B\n",
    "    * gives dot product (mult each element across rows and columns and sum)\n",
    "* determinants only for square matrices\n",
    "* rank is number of linearly independent columns/rows \n",
    "* eigenvalues and vectors\n",
    "    * comes from decomposing a square matrix\n",
    "    * multiply a matrix by eigenvalue/vectors only changes in scale\n",
    "    * eigenvectors - stays on the line that it spans out, doesn't change direction\n",
    "    * eigenvalue - degree to which that vector spans out, 1 meaning it remains the same\n",
    "* SVD - Singular Value Decomposition\n",
    "    * for all matrices - reduces matrix into its parts\n",
    "    * aka 'matrix factorization'\n",
    "    * can get a square which you can compute eigenvalues and vectors\n",
    "\n",
    "### Machine Learning (General)\n",
    "* AI > ML > Deep Learning\n",
    "* a computer program that accepts a set of features (inputs) and predicts or classifies a target variable \n",
    "* supervised (labels are given)\n",
    "    * regression-linear/logistic\n",
    "* unsupervised (no labels)\n",
    "    * finds structure in the data\n",
    "    * clustering, generative networks, dimension reduction\n",
    "* semi supervised\n",
    "* reinforcement learning\n",
    "\n",
    "**bias vs variance - approximation error vs estimation error**\n",
    "* bias - difference between prediction of the model vs correct value\n",
    "    * high bias = underfitted, oversimplified model\n",
    "    * high error between prediction and correct\n",
    "* variance - variability of model prediction for a given data point\n",
    "    * high variance = overfit, pays too much attention to the  training data and doesn't generalize\n",
    "    * performs well on training but high error on test \n",
    "    * model captures too much noise\n",
    "* low variance/high bias - model is very precise but not accurate, underfitted\n",
    "* high variance/low bias - model is accurate - but not precise - overfitted \n",
    "* models should have low variance, low bias \n",
    "* want to fit enough of variance (but not too much that it overfits) - with low bias\n",
    "* Approximation error \n",
    "    * how well the model is fitting with the training data\n",
    "    * is the model learning the underlying relationship\n",
    "    * related to variance \n",
    "* Estimation error\n",
    "    * how off are the predictions from the actual values\n",
    "    * related to bias \n",
    "\n",
    "Validation methods \n",
    "* split data into a train/test\n",
    "* protects against overfitting - model memorizes the data rather than extrapolating the underlying relationships\n",
    "* cross validation - take each slice of the cake \n",
    "\n",
    "#### Dimensionality Reduction\n",
    "* useful for visualization, lower complexity, improve model performance and reduce over-fitting \n",
    "* Dimensionality Reduction\n",
    "    * PCA\n",
    "    * LDA\n",
    "* Variable Selection\n",
    "    * Filter\n",
    "        * low/high variance\n",
    "        * feature similarity - correlation b/w features\n",
    "        * correlations with target variable\n",
    "    * Wrapper methods\n",
    "        * train model with different combos of features\n",
    "        * find the best subset - trying different combos\n",
    "        * forward selection - starting with no features\n",
    "        * backward elimination - start with all and remove one\n",
    "        * stepwise selection - combines both, until optimal set of features\n",
    "\n",
    "#### Principal Component Analysis \n",
    "* using linear algebra to find 'arbirtrary' components from a set of features to represent the data\n",
    "* used to reduce the dimensions or number of features in your data set \n",
    "* done after some transformation and exclusion of features that are correlate with each other\n",
    "* Scree Plot to parse out how much each component explains variance in your dataset - and figure out optimal components to use\n",
    "* can be used to cluster data\n",
    "* PC1 goes through the most variation, which is the eigenvector with the highest eigenvalue (most variance)\n",
    "* PC2 goes through the second most variation - pass through the mean (origin) and is perpindicular to PC1\n",
    "\n",
    "#### LDA (less covered)\n",
    "* supervised used when target variable is categorical\n",
    "* minimizes intra-class variance, and maximize inter-class variance \n",
    "------------------------------------------------------------------------------------\n",
    "##### Clustering - Unsupervised\n",
    "* K-Means (Nearest Neighbours)\n",
    "    * specify number of clusters\n",
    "    * finds centroid from k number of points, can start at random spot\n",
    "    * find the mean of clusters - taken as new centroid\n",
    "    * compare values between centroids\n",
    "    * reassigns points to new clusters depending on how far those points are from the new centroid \n",
    "    * tries to minimize within SS difference from centroid (inertia), and maximize SSdiff between centroids (silhouette score)\n",
    "    * distortion plot - plots SSerror on y axis, SSerror from centroid is lower, lower intertia, the more clusters - \n",
    "    * SSerror - how big is the circle, are the datapoints far from the middle of the cluster \n",
    "    * based on 'euclidean' distance (hypoteneuse) \n",
    "* Hierarchical Clustering\n",
    "    * each point begins as a cluster > two closest are clustered > repeat until have one big cluster\n",
    "    * dendrogram created to visualize the number of clusters possible\n",
    "    * agglomerative (bottom up) \n",
    "    * divisive (top-down)\n",
    "* DBScan\n",
    "    * Density-based spatial clustering applications with noise\n",
    "    * good for non-spherical/arbitrary shape clusters\n",
    "    * no initial k-input - but based on radius or epsilon, and minimum number of neighbours to get a corepoint\n",
    "    * stops when reaches borderpoints that doesn't meet the criteria for minimum number of neighbours\n",
    "    * excludes outliers\n",
    "------------------------------------------------------------------------------------\n",
    "### **Supervised**\n",
    "#### Linear Regression\n",
    "* predicting a continuous (number) value\n",
    "* split data into training/test\n",
    "* bias error - high bias = underfitting - misses relevant relationships\n",
    "* variance - error from sensitivity to small fluctuations \n",
    "    * high variance causes algorithm to model random noise = overfitting\n",
    "* assumes linear relationship between features and target \n",
    "* assigns a weight/coefficient to each feature - how much they contribute to predicting the target\n",
    "* uses Least Squares Error (LSE) meaning it tries to minimize the sum of squares of each data point from the model/line \n",
    "* results in an R2 or Adjust R2 - proportion of the variance in the dependent variable explained by independent variables in the linear regression model\n",
    "* Polynomial Regression\n",
    "* when data is non-linear (e.g. quadratic)\n",
    "* but essential uses polynomial features - to still create a linear model\n",
    "\n",
    "#### Logistic Regression\n",
    "* similar to linear regression but for classification problems\n",
    "* linear model (weights*X + intercept)\n",
    "* learns weights associated with each feature and constant/bias \n",
    "* weights tunes the hyperplane, tilt/orientation\n",
    "* creates a 'decision boundary' splitting the data in half\n",
    "* hard predictions of -1 to 1 or soft-predictions like probabilities\n",
    "* works via a sigmoid function - squishes the output between 0 and 1 \n",
    "* really large negative values become 0, really large positive values get mapped to 1\n",
    "* after squashing - can predict probabilitiy along the sigmoid\n",
    "* weights are accessible \n",
    "* can be done through multiclass - by doing one versus rest strategies - compares one versus rest, and combines the prediction at the end\n",
    "* falls short in larger features, more complex with multi-class \n",
    "\n",
    "#### Decision Trees\n",
    "* essentially a flow chart based on features\n",
    "* decides a way to split the data, based on a series of if functions\n",
    "* can overfit more than logistic regression\n",
    "* max_depth - hyperparameter that's tuned - too high and will split the data to each single data point\n",
    "* advantage: easy to interpret and visualize, little data prep, can work with missing\n",
    "* loss function = Gini impurity \n",
    "    * 0 to 0.5 for two classes\n",
    "    * quantifies quality of split \n",
    "    * calculate impurity of each split, no mislabeled = 0\n",
    "    * when splitting - results will either belong to one class or another - if the split leads to leaf with only one class - pure, if mixed of more than one class - impure \n",
    "    * 1 - probability of one class^2 - probability of another class^2\n",
    "    * if all in one class = 1^2 = 1 leading to Gini impurity of 0\n",
    "    * maximize the Gini Gain - subtract weighted impurites from original impurity \n",
    "    * in numerical feats - ordered - and calculate gini impurity based on average of pairs, lowest impurity chosen as the split\n",
    "* In regression Trees - loss function is MSE, MAE\n",
    "    * tries to be minimized across split\n",
    "    * good for stepwise data - can't be fit easily with linear regression\n",
    "* top of the tree/root is one with the lowest impurity \n",
    "\n",
    "#### Random Forest\n",
    "* address the overfitting in decision trees\n",
    "* creates a bunch of decision trees - and inject some randomness \n",
    "* takes the most common prediction from all trees - and uses that as a final prediction\n",
    "* injects randomness through bootstrap samples - resampling the data with replacement - so that each point can be repeated multiple times and you get a different but similar dataset\n",
    "* at each split - consider only a random subset of features - not all\n",
    "* averaging over many trees - reduces the variance\n",
    "* slower to train multiple trees - but can be done in parallel\n",
    "* number of bootstrap samples can be optimized \n",
    "\n",
    "#### Ensembles\n",
    "* Bagging\n",
    "    * bootrstrapping and aggregating\n",
    "    * similar to random forest\n",
    "    * addresses overfitting \n",
    "* Boosting\n",
    "    * adds one model at a time, aggregation is done during training and not after\n",
    "    * addresses underfitting \n",
    "    * each tree gets a more 'weighted' vote\n",
    "    * learns sequentially - next models learns from the previous\n",
    "    * done with XGboosts/Adaboosts\n",
    "* Stacking \n",
    "    * uses a variety of weak models as inputs\n",
    "    * similar to bagging but different models \n",
    "* more of a black box\n",
    "* not sure what each model is doing \n",
    "* when combining models - use one that overfits - and one that underfits\n",
    "\n",
    "#### Boosting/XGBoost\n",
    "* focused on reducing bias - better prediction\n",
    "* Adaboost\n",
    "    * weighted sum of weak learners\n",
    "    * adds learners one by one \n",
    "    * better weak learners adds more to the final model\n",
    "* Gradient Boosting\n",
    "    * calculates pseudoresiduals - error between model and prediction\n",
    "    * tries to reduce pseudoreisudals\n",
    "* XGBoost\n",
    "    * minimize training time\n",
    "    * finds ways to split data and iterates to improve fit\n",
    "    * calculates a similarity score\n",
    "        * mean squared error + lambda hyperparameter \n",
    "    * gets the gain from each split - comparing similarity scores between leafs (split) and the root (one before) \n",
    "    * if gain is larger at the leaf than at the root or prior leaf then that split is kept and used - and repeated after each split\n",
    "    * default levels = 6\n",
    "    * if gain is negative - it is pruned, gamma=0 for pruning \n",
    "    * lambda - regularization parameter\n",
    "        * greater - lowers gain, makes it easier to prune trees\n",
    "        * prevents overfitting\n",
    "        * especially if only one observation at a leaf\n",
    "\n",
    "#### Naive Bayes\n",
    "* based on bayes theorem, and conditional probability\n",
    "* classifier or regression\n",
    "* based on statistical inference, good for spam detection\n",
    "* assumes variables are independent - and has equal weight\n",
    "\n",
    "#### Support Vector Machines (SVM)\n",
    "* boundaries are non-linear\n",
    "* decision boundary is based on support vectors\n",
    "* separates based on largest margin/hyperplane\n",
    "* first select import support vecotrs - one that are closest together - but belonging in separate classes - splits those points such that distance from that line or boundary called the margin - is largest between the two support vectors\n",
    "* parameters - are weights and intercepts - learned from training\n",
    "* hyperparameters\n",
    "    * gamma - controls complexity - larger = more complex (overfitting)\n",
    "    * C - larger = more complex (overfitting) \n",
    "        * changes which SV are chosen to develop hyperplane - allowing for misclassification \n",
    "* does a kernel trick for non-linears - where dimensions are transformed to create a linear boundary then re-transforms the data \n",
    "\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "#### Gradient Descent\n",
    "* algorithm that optimizes the parameters in a models\n",
    "* works by minimizing loss or error, in simple terms the difference between prediction and actual value\n",
    "* works through trial and error then selecting the best parameters that minimize the loss \n",
    "* adjusts parameters via a learning rate that reduces loss \n",
    "* finds the optimal weights for your linear regression\n",
    "* to avoid overfitting\n",
    "* intializes a value (for the parameter) -> evaluate fit based on some loss function (SSresiduals) -> if plotted this will create a curve, at different values, loss function will go down and up -> using calculus you can then find the derivative of this curve to find the minimum, so the value that gives the smallest loss function (SSresiduals e.g.)\n",
    "* the derivative - calculates the slope at each point of the curve, and adjusts the parameters based on step size (slope * learning rate) \n",
    "* adjustment starts at large steps when far, and smaller when closer to the optimal value (the adjustment is based on a 'learning rate') that is preset\n",
    "* stops when step size is very close to 0, i.e. when slope is close to 0\n",
    "* as long as derivative can be taken - it can be done\n",
    "\n",
    "**Stochastic Gradient Descent**\n",
    "* when there's large amount of data\n",
    "* selects a one point data at every step instead of using the full data set to get the gradient\n",
    "* reduces calculation time of derivatives (less derivatives to calculate)\n",
    "* randomly picks one sample and uses that to calculate the derivative\n",
    "* can better escape local minimums - because it's done randomly and selects a different point to calculate derivative from \n",
    "\n",
    "**Mini-batch Gradient Descent**\n",
    "* instead of one point, calculates from a batch of data points\n",
    "\n",
    "#### Regularization\n",
    "* done to minimize overfitting \n",
    "* used in many algo\n",
    "* punishes for more features \n",
    "* Ridge Regression (L2)\n",
    "    * add a weight 'lambda' and multiply by sum of squared errors\n",
    "    * more features, higher penalty\n",
    "* Lasso Regression (L1)\n",
    "    * add a weight 'lambda' and multiply by sum of absolute value of errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "* MSE, MAE, R2\n",
    "* Accuracy\n",
    "    * True Positive + True Negative / (All)\n",
    "* Precision **\n",
    "    * true positives from all *labeled/predicted* positive\n",
    "    * TP / (TP+FP)\n",
    "    * high precision - low false positives rate\n",
    "* Recall/Sensitivity **\n",
    "    * true positives from all *actual/true* positives\n",
    "    * TP / (TP+FN)\n",
    "    * high recall - low false negatives\n",
    "* Precision/Recall - trade off **\n",
    "    * churn example\n",
    "    * increasing precision - get better at detecting false positives and reducing false positive \n",
    "        * shoplifter at a store - increase precision, ensure that is actually a shoplifter\n",
    "    * increasing recall - better at detecting false negatives \n",
    "    * case of cancer - might be better to optimize for recall - in order to be better at detecting false negatives - because it would be more costly to the person - to have cancer and not have it get detected\n",
    "* F1 score - Recall/Precsion\n",
    "* Lift\n",
    "* ROC/AUC\n",
    "    * plots true positive rate and false positive rate of model\n",
    "    * compute area under the curve - want AUC to be high - meaning that TPR increases faster than the FPR\n",
    "    * another way of representing precision of the model \n",
    "    * straight line = 0.5, no predictive value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning - Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Series \n",
    "* FBprophet, time series analysis\n",
    "* turning time series into a machine learning problem - using previous data as y, by shifting the data by a period (days, weeks or months)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhl_env38",
   "language": "python",
   "name": "lhl_env38"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:14) \n[Clang 12.0.1 ]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
