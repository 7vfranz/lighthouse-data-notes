{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJp-D51g0IDd"
      },
      "source": [
        "## **1) Importing Python Packages for GAN**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k5mFBuzzl2a",
        "outputId": "22f886b2-ea07-4890-d1d0-6ccf2e33065d"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dense, Reshape, Flatten\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "!mkdir generated_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr-eZOzg0X79"
      },
      "source": [
        "## **2) Variables for Neural Networks & Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RThZMDruz9cB",
        "outputId": "a7232560-56b3-42c3-f52e-2bd1c2b5bc5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/lhl_env38/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "img_width = 28\n",
        "img_height = 28\n",
        "channels = 1\n",
        "img_shape = (img_width, img_height, channels)\n",
        "latent_dim = 100\n",
        "adam = Adam(lr=0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3bcJZZg0cqy"
      },
      "source": [
        "## **3) Building Generator**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdiqZpri0iQh",
        "outputId": "166dfc32-17f5-4613-d90a-d523b538d76f"
      },
      "outputs": [],
      "source": [
        "def build_generator():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Dense(256, input_dim=latent_dim))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(256))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(256))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "  model.add(Reshape(img_shape))\n",
        "  return model\n",
        "\n",
        "generator = build_generator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt6QsJCW0mcI"
      },
      "source": [
        "## **4) Building Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2JzEAPv0lKt",
        "outputId": "c5dc2705-5b5f-4c83-89f5-731ae33203ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               401920    \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def build_discriminator():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Flatten(input_shape=img_shape))\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(256))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbcKcKmA0q2S"
      },
      "source": [
        "## **5) Connecting Neural Networks to build GAN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0Ue3TEd0xLy",
        "outputId": "491bfb8d-db71-4ef5-c636-8a3c918b64d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential (Sequential)     (None, 28, 28, 1)         362000    \n",
            "                                                                 \n",
            " sequential_1 (Sequential)   (None, 1)                 533505    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 895,505\n",
            "Trainable params: 360,464\n",
            "Non-trainable params: 535,041\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "GAN = Sequential()\n",
        "discriminator.trainable = False\n",
        "GAN.add(generator)\n",
        "GAN.add(discriminator)\n",
        "\n",
        "GAN.compile(loss='binary_crossentropy', optimizer=adam)\n",
        "GAN.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WaNhBDwRwTG"
      },
      "source": [
        "## **6) Outputting Images**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HQEJ0WbjRppy"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "## **7) Outputting Images**\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import imageio\n",
        "import PIL\n",
        "\n",
        "save_name = 0.00000000\n",
        "\n",
        "def save_imgs(epoch):\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "    global save_name\n",
        "    save_name += 0.00000001\n",
        "    print(\"%.8f\" % save_name)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            # axs[i,j].imshow(gen_imgs[cnt])\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"generated_images/%.8f.png\" % save_name)\n",
        "    print('saved')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE57Lk5V0xs2"
      },
      "source": [
        "## **7) Training GAN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "egSJJvik00Iq",
        "outputId": "31c8c38f-dafd-41b3-e976-4f68473a9922"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "******* 0 [D loss: 0.011841, acc: 100.00%] [G loss: 4.001544]\n",
            "0.00000011\n",
            "saved\n",
            "******* 1 [D loss: 0.012337, acc: 100.00%] [G loss: 4.036134]\n",
            "******* 2 [D loss: 0.011586, acc: 100.00%] [G loss: 3.979584]\n",
            "******* 3 [D loss: 0.011705, acc: 100.00%] [G loss: 4.024421]\n",
            "******* 4 [D loss: 0.013163, acc: 100.00%] [G loss: 3.961791]\n",
            "******* 5 [D loss: 0.010247, acc: 100.00%] [G loss: 4.039857]\n",
            "******* 6 [D loss: 0.010104, acc: 100.00%] [G loss: 4.014960]\n",
            "******* 7 [D loss: 0.011210, acc: 100.00%] [G loss: 4.048666]\n",
            "******* 8 [D loss: 0.009536, acc: 100.00%] [G loss: 4.018991]\n",
            "******* 9 [D loss: 0.012399, acc: 100.00%] [G loss: 4.098874]\n",
            "******* 10 [D loss: 0.009998, acc: 100.00%] [G loss: 4.004060]\n",
            "******* 11 [D loss: 0.011881, acc: 100.00%] [G loss: 4.107146]\n",
            "******* 12 [D loss: 0.012763, acc: 100.00%] [G loss: 4.159957]\n",
            "******* 13 [D loss: 0.009360, acc: 100.00%] [G loss: 4.123012]\n",
            "******* 14 [D loss: 0.010094, acc: 100.00%] [G loss: 4.078710]\n",
            "******* 15 [D loss: 0.011355, acc: 100.00%] [G loss: 4.135493]\n",
            "******* 16 [D loss: 0.012273, acc: 100.00%] [G loss: 4.130708]\n",
            "******* 17 [D loss: 0.010577, acc: 100.00%] [G loss: 4.127978]\n",
            "******* 18 [D loss: 0.012839, acc: 100.00%] [G loss: 4.156993]\n",
            "******* 19 [D loss: 0.010111, acc: 100.00%] [G loss: 4.120181]\n",
            "******* 20 [D loss: 0.010238, acc: 100.00%] [G loss: 4.198695]\n",
            "******* 21 [D loss: 0.010923, acc: 100.00%] [G loss: 4.332286]\n",
            "******* 22 [D loss: 0.009358, acc: 100.00%] [G loss: 4.168168]\n",
            "******* 23 [D loss: 0.010284, acc: 100.00%] [G loss: 4.240449]\n",
            "******* 24 [D loss: 0.009922, acc: 100.00%] [G loss: 4.248011]\n",
            "******* 25 [D loss: 0.009988, acc: 100.00%] [G loss: 4.299167]\n",
            "******* 26 [D loss: 0.007217, acc: 100.00%] [G loss: 4.171338]\n",
            "******* 27 [D loss: 0.011420, acc: 100.00%] [G loss: 4.223832]\n",
            "******* 28 [D loss: 0.008172, acc: 100.00%] [G loss: 4.194816]\n",
            "******* 29 [D loss: 0.008304, acc: 100.00%] [G loss: 4.289361]\n",
            "******* 30 [D loss: 0.009009, acc: 100.00%] [G loss: 4.282949]\n",
            "******* 31 [D loss: 0.007933, acc: 100.00%] [G loss: 4.286782]\n",
            "******* 32 [D loss: 0.009767, acc: 100.00%] [G loss: 4.297042]\n",
            "******* 33 [D loss: 0.008765, acc: 100.00%] [G loss: 4.274705]\n",
            "******* 34 [D loss: 0.007231, acc: 100.00%] [G loss: 4.398163]\n",
            "******* 35 [D loss: 0.007755, acc: 100.00%] [G loss: 4.255712]\n",
            "******* 36 [D loss: 0.008033, acc: 100.00%] [G loss: 4.260770]\n",
            "******* 37 [D loss: 0.009107, acc: 100.00%] [G loss: 4.252779]\n",
            "******* 38 [D loss: 0.008533, acc: 100.00%] [G loss: 4.310709]\n",
            "******* 39 [D loss: 0.008893, acc: 100.00%] [G loss: 4.318809]\n",
            "******* 40 [D loss: 0.008815, acc: 100.00%] [G loss: 4.318511]\n",
            "******* 41 [D loss: 0.009499, acc: 100.00%] [G loss: 4.330019]\n",
            "******* 42 [D loss: 0.010993, acc: 100.00%] [G loss: 4.222171]\n",
            "******* 43 [D loss: 0.011184, acc: 100.00%] [G loss: 4.323704]\n",
            "******* 44 [D loss: 0.009793, acc: 100.00%] [G loss: 4.318305]\n",
            "******* 45 [D loss: 0.010708, acc: 100.00%] [G loss: 4.309078]\n",
            "******* 46 [D loss: 0.009320, acc: 100.00%] [G loss: 4.289899]\n",
            "******* 47 [D loss: 0.010018, acc: 100.00%] [G loss: 4.346095]\n",
            "******* 48 [D loss: 0.008441, acc: 100.00%] [G loss: 4.312838]\n",
            "******* 49 [D loss: 0.009377, acc: 100.00%] [G loss: 4.364261]\n",
            "******* 50 [D loss: 0.009556, acc: 100.00%] [G loss: 4.369876]\n",
            "******* 51 [D loss: 0.010031, acc: 100.00%] [G loss: 4.383403]\n",
            "******* 52 [D loss: 0.008611, acc: 100.00%] [G loss: 4.490389]\n",
            "******* 53 [D loss: 0.010024, acc: 100.00%] [G loss: 4.432400]\n",
            "******* 54 [D loss: 0.009481, acc: 100.00%] [G loss: 4.498687]\n",
            "******* 55 [D loss: 0.006779, acc: 100.00%] [G loss: 4.333529]\n",
            "******* 56 [D loss: 0.007224, acc: 100.00%] [G loss: 4.421918]\n",
            "******* 57 [D loss: 0.007730, acc: 100.00%] [G loss: 4.438324]\n",
            "******* 58 [D loss: 0.009246, acc: 100.00%] [G loss: 4.440432]\n",
            "******* 59 [D loss: 0.009075, acc: 100.00%] [G loss: 4.434519]\n",
            "******* 60 [D loss: 0.009165, acc: 100.00%] [G loss: 4.386464]\n",
            "******* 61 [D loss: 0.007792, acc: 100.00%] [G loss: 4.443556]\n",
            "******* 62 [D loss: 0.008907, acc: 100.00%] [G loss: 4.361959]\n",
            "******* 63 [D loss: 0.009412, acc: 100.00%] [G loss: 4.526476]\n",
            "******* 64 [D loss: 0.007485, acc: 100.00%] [G loss: 4.453586]\n",
            "******* 65 [D loss: 0.007450, acc: 100.00%] [G loss: 4.506795]\n",
            "******* 66 [D loss: 0.008127, acc: 100.00%] [G loss: 4.563111]\n",
            "******* 67 [D loss: 0.007396, acc: 100.00%] [G loss: 4.490235]\n",
            "******* 68 [D loss: 0.007262, acc: 100.00%] [G loss: 4.427119]\n",
            "******* 69 [D loss: 0.007959, acc: 100.00%] [G loss: 4.462831]\n",
            "******* 70 [D loss: 0.009934, acc: 100.00%] [G loss: 4.435618]\n",
            "******* 71 [D loss: 0.008840, acc: 100.00%] [G loss: 4.475294]\n",
            "******* 72 [D loss: 0.007473, acc: 100.00%] [G loss: 4.397548]\n",
            "******* 73 [D loss: 0.007698, acc: 100.00%] [G loss: 4.522687]\n",
            "******* 74 [D loss: 0.007684, acc: 100.00%] [G loss: 4.465107]\n",
            "******* 75 [D loss: 0.010332, acc: 100.00%] [G loss: 4.489720]\n",
            "******* 76 [D loss: 0.008474, acc: 100.00%] [G loss: 4.370665]\n",
            "******* 77 [D loss: 0.009045, acc: 100.00%] [G loss: 4.447644]\n",
            "******* 78 [D loss: 0.009084, acc: 100.00%] [G loss: 4.411147]\n",
            "******* 79 [D loss: 0.009123, acc: 100.00%] [G loss: 4.499304]\n",
            "******* 80 [D loss: 0.007183, acc: 100.00%] [G loss: 4.528506]\n",
            "******* 81 [D loss: 0.008979, acc: 100.00%] [G loss: 4.436069]\n",
            "******* 82 [D loss: 0.008470, acc: 100.00%] [G loss: 4.584544]\n",
            "******* 83 [D loss: 0.008990, acc: 100.00%] [G loss: 4.439322]\n",
            "******* 84 [D loss: 0.007840, acc: 100.00%] [G loss: 4.459397]\n",
            "******* 85 [D loss: 0.008243, acc: 100.00%] [G loss: 4.515450]\n",
            "******* 86 [D loss: 0.011245, acc: 100.00%] [G loss: 4.489272]\n",
            "******* 87 [D loss: 0.009163, acc: 100.00%] [G loss: 4.388073]\n",
            "******* 88 [D loss: 0.010130, acc: 100.00%] [G loss: 4.556498]\n",
            "******* 89 [D loss: 0.009848, acc: 100.00%] [G loss: 4.439362]\n",
            "******* 90 [D loss: 0.008953, acc: 100.00%] [G loss: 4.493249]\n",
            "******* 91 [D loss: 0.007520, acc: 100.00%] [G loss: 4.500772]\n",
            "******* 92 [D loss: 0.007934, acc: 100.00%] [G loss: 4.567938]\n",
            "******* 93 [D loss: 0.008521, acc: 100.00%] [G loss: 4.531096]\n",
            "******* 94 [D loss: 0.009575, acc: 100.00%] [G loss: 4.653886]\n",
            "******* 95 [D loss: 0.007384, acc: 100.00%] [G loss: 4.517232]\n",
            "******* 96 [D loss: 0.006972, acc: 100.00%] [G loss: 4.517258]\n",
            "******* 97 [D loss: 0.007715, acc: 100.00%] [G loss: 4.494250]\n",
            "******* 98 [D loss: 0.008366, acc: 100.00%] [G loss: 4.463204]\n",
            "******* 99 [D loss: 0.009322, acc: 100.00%] [G loss: 4.482577]\n",
            "******* 100 [D loss: 0.008804, acc: 100.00%] [G loss: 4.454595]\n",
            "0.00000012\n",
            "saved\n",
            "******* 101 [D loss: 0.007660, acc: 100.00%] [G loss: 4.526402]\n",
            "******* 102 [D loss: 0.010000, acc: 100.00%] [G loss: 4.513303]\n",
            "******* 103 [D loss: 0.009267, acc: 100.00%] [G loss: 4.545817]\n",
            "******* 104 [D loss: 0.008607, acc: 100.00%] [G loss: 4.451874]\n",
            "******* 105 [D loss: 0.007374, acc: 100.00%] [G loss: 4.496088]\n",
            "******* 106 [D loss: 0.009164, acc: 100.00%] [G loss: 4.581712]\n",
            "******* 107 [D loss: 0.006448, acc: 100.00%] [G loss: 4.499246]\n",
            "******* 108 [D loss: 0.009838, acc: 100.00%] [G loss: 4.527677]\n",
            "******* 109 [D loss: 0.009226, acc: 100.00%] [G loss: 4.561491]\n",
            "******* 110 [D loss: 0.011621, acc: 100.00%] [G loss: 4.566227]\n",
            "******* 111 [D loss: 0.008699, acc: 100.00%] [G loss: 4.538558]\n",
            "******* 112 [D loss: 0.008343, acc: 100.00%] [G loss: 4.516295]\n",
            "******* 113 [D loss: 0.011253, acc: 100.00%] [G loss: 4.441388]\n",
            "******* 114 [D loss: 0.009810, acc: 100.00%] [G loss: 4.496093]\n",
            "******* 115 [D loss: 0.008059, acc: 100.00%] [G loss: 4.601117]\n",
            "******* 116 [D loss: 0.009280, acc: 100.00%] [G loss: 4.554255]\n",
            "******* 117 [D loss: 0.010683, acc: 100.00%] [G loss: 4.538280]\n",
            "******* 118 [D loss: 0.008228, acc: 100.00%] [G loss: 4.539507]\n",
            "******* 119 [D loss: 0.009161, acc: 100.00%] [G loss: 4.494162]\n",
            "******* 120 [D loss: 0.009064, acc: 100.00%] [G loss: 4.516040]\n",
            "******* 121 [D loss: 0.010356, acc: 100.00%] [G loss: 4.487838]\n",
            "******* 122 [D loss: 0.008388, acc: 100.00%] [G loss: 4.558797]\n",
            "******* 123 [D loss: 0.008124, acc: 100.00%] [G loss: 4.551175]\n",
            "******* 124 [D loss: 0.010415, acc: 100.00%] [G loss: 4.457936]\n",
            "******* 125 [D loss: 0.009300, acc: 100.00%] [G loss: 4.499301]\n",
            "******* 126 [D loss: 0.010067, acc: 100.00%] [G loss: 4.511709]\n",
            "******* 127 [D loss: 0.009028, acc: 100.00%] [G loss: 4.533983]\n",
            "******* 128 [D loss: 0.009057, acc: 100.00%] [G loss: 4.425562]\n",
            "******* 129 [D loss: 0.009837, acc: 100.00%] [G loss: 4.483431]\n",
            "******* 130 [D loss: 0.015150, acc: 100.00%] [G loss: 4.484266]\n",
            "******* 131 [D loss: 0.010094, acc: 100.00%] [G loss: 4.533022]\n",
            "******* 132 [D loss: 0.011399, acc: 100.00%] [G loss: 4.487888]\n",
            "******* 133 [D loss: 0.012331, acc: 100.00%] [G loss: 4.527158]\n",
            "******* 134 [D loss: 0.011961, acc: 100.00%] [G loss: 4.452334]\n",
            "******* 135 [D loss: 0.010130, acc: 100.00%] [G loss: 4.495861]\n",
            "******* 136 [D loss: 0.011287, acc: 100.00%] [G loss: 4.522546]\n",
            "******* 137 [D loss: 0.012654, acc: 100.00%] [G loss: 4.502483]\n",
            "******* 138 [D loss: 0.012182, acc: 100.00%] [G loss: 4.419811]\n",
            "******* 139 [D loss: 0.010744, acc: 100.00%] [G loss: 4.461299]\n",
            "******* 140 [D loss: 0.012549, acc: 100.00%] [G loss: 4.463058]\n",
            "******* 141 [D loss: 0.012590, acc: 100.00%] [G loss: 4.552651]\n",
            "******* 142 [D loss: 0.010788, acc: 100.00%] [G loss: 4.481428]\n",
            "******* 143 [D loss: 0.009773, acc: 100.00%] [G loss: 4.598228]\n",
            "******* 144 [D loss: 0.011728, acc: 100.00%] [G loss: 4.464650]\n",
            "******* 145 [D loss: 0.012525, acc: 100.00%] [G loss: 4.458654]\n",
            "******* 146 [D loss: 0.014469, acc: 100.00%] [G loss: 4.467760]\n",
            "******* 147 [D loss: 0.013428, acc: 100.00%] [G loss: 4.566988]\n",
            "******* 148 [D loss: 0.011370, acc: 100.00%] [G loss: 4.494491]\n",
            "******* 149 [D loss: 0.010912, acc: 100.00%] [G loss: 4.479279]\n",
            "******* 150 [D loss: 0.012418, acc: 100.00%] [G loss: 4.645749]\n",
            "******* 151 [D loss: 0.013909, acc: 100.00%] [G loss: 4.420258]\n",
            "******* 152 [D loss: 0.010853, acc: 100.00%] [G loss: 4.497377]\n",
            "******* 153 [D loss: 0.015371, acc: 100.00%] [G loss: 4.526317]\n",
            "******* 154 [D loss: 0.013313, acc: 100.00%] [G loss: 4.490263]\n",
            "******* 155 [D loss: 0.012327, acc: 100.00%] [G loss: 4.362316]\n",
            "******* 156 [D loss: 0.014563, acc: 100.00%] [G loss: 4.357622]\n",
            "******* 157 [D loss: 0.015586, acc: 100.00%] [G loss: 4.380730]\n",
            "******* 158 [D loss: 0.014582, acc: 100.00%] [G loss: 4.462585]\n",
            "******* 159 [D loss: 0.014966, acc: 100.00%] [G loss: 4.401567]\n",
            "******* 160 [D loss: 0.014507, acc: 100.00%] [G loss: 4.444645]\n",
            "******* 161 [D loss: 0.014688, acc: 100.00%] [G loss: 4.400537]\n",
            "******* 162 [D loss: 0.016204, acc: 100.00%] [G loss: 4.463865]\n",
            "******* 163 [D loss: 0.019334, acc: 100.00%] [G loss: 4.455998]\n",
            "******* 164 [D loss: 0.011789, acc: 100.00%] [G loss: 4.461240]\n",
            "******* 165 [D loss: 0.014804, acc: 100.00%] [G loss: 4.437232]\n",
            "******* 166 [D loss: 0.013923, acc: 100.00%] [G loss: 4.419308]\n",
            "******* 167 [D loss: 0.017313, acc: 100.00%] [G loss: 4.549457]\n",
            "******* 168 [D loss: 0.015630, acc: 100.00%] [G loss: 4.397334]\n",
            "******* 169 [D loss: 0.014935, acc: 100.00%] [G loss: 4.326708]\n",
            "******* 170 [D loss: 0.012237, acc: 100.00%] [G loss: 4.373115]\n",
            "******* 171 [D loss: 0.018098, acc: 100.00%] [G loss: 4.450574]\n",
            "******* 172 [D loss: 0.020189, acc: 100.00%] [G loss: 4.341586]\n",
            "******* 173 [D loss: 0.014255, acc: 100.00%] [G loss: 4.505758]\n",
            "******* 174 [D loss: 0.020575, acc: 100.00%] [G loss: 4.440538]\n",
            "******* 175 [D loss: 0.018139, acc: 100.00%] [G loss: 4.446671]\n",
            "******* 176 [D loss: 0.018682, acc: 100.00%] [G loss: 4.494272]\n",
            "******* 177 [D loss: 0.017982, acc: 100.00%] [G loss: 4.492975]\n",
            "******* 178 [D loss: 0.015535, acc: 100.00%] [G loss: 4.363254]\n",
            "******* 179 [D loss: 0.017987, acc: 100.00%] [G loss: 4.371987]\n",
            "******* 180 [D loss: 0.013817, acc: 100.00%] [G loss: 4.333954]\n",
            "******* 181 [D loss: 0.017029, acc: 100.00%] [G loss: 4.331440]\n",
            "******* 182 [D loss: 0.020016, acc: 100.00%] [G loss: 4.287054]\n",
            "******* 183 [D loss: 0.019606, acc: 100.00%] [G loss: 4.284397]\n",
            "******* 184 [D loss: 0.020027, acc: 100.00%] [G loss: 4.292142]\n",
            "******* 185 [D loss: 0.025332, acc: 100.00%] [G loss: 4.254685]\n",
            "******* 186 [D loss: 0.022988, acc: 100.00%] [G loss: 4.118308]\n",
            "******* 187 [D loss: 0.018204, acc: 100.00%] [G loss: 4.187127]\n",
            "******* 188 [D loss: 0.020558, acc: 100.00%] [G loss: 4.155665]\n",
            "******* 189 [D loss: 0.026672, acc: 100.00%] [G loss: 4.242307]\n",
            "******* 190 [D loss: 0.019999, acc: 100.00%] [G loss: 4.176398]\n",
            "******* 191 [D loss: 0.028300, acc: 100.00%] [G loss: 4.106633]\n",
            "******* 192 [D loss: 0.028817, acc: 100.00%] [G loss: 4.146532]\n",
            "******* 193 [D loss: 0.017704, acc: 100.00%] [G loss: 4.206859]\n",
            "******* 194 [D loss: 0.030387, acc: 100.00%] [G loss: 4.187782]\n",
            "******* 195 [D loss: 0.027100, acc: 100.00%] [G loss: 4.157413]\n",
            "******* 196 [D loss: 0.027245, acc: 100.00%] [G loss: 4.061462]\n",
            "******* 197 [D loss: 0.025194, acc: 100.00%] [G loss: 4.174977]\n",
            "******* 198 [D loss: 0.021025, acc: 100.00%] [G loss: 4.002102]\n",
            "******* 199 [D loss: 0.018906, acc: 100.00%] [G loss: 4.072466]\n",
            "******* 200 [D loss: 0.031907, acc: 100.00%] [G loss: 4.034070]\n",
            "0.00000013\n",
            "saved\n",
            "******* 201 [D loss: 0.023219, acc: 100.00%] [G loss: 4.154929]\n",
            "******* 202 [D loss: 0.021827, acc: 100.00%] [G loss: 4.135475]\n",
            "******* 203 [D loss: 0.030412, acc: 100.00%] [G loss: 4.097822]\n",
            "******* 204 [D loss: 0.043171, acc: 99.22%] [G loss: 3.860161]\n",
            "******* 205 [D loss: 0.035623, acc: 100.00%] [G loss: 4.014563]\n",
            "******* 206 [D loss: 0.035119, acc: 99.22%] [G loss: 3.758344]\n",
            "******* 207 [D loss: 0.031713, acc: 100.00%] [G loss: 3.879293]\n",
            "******* 208 [D loss: 0.027131, acc: 100.00%] [G loss: 4.030305]\n",
            "******* 209 [D loss: 0.026498, acc: 100.00%] [G loss: 4.025587]\n",
            "******* 210 [D loss: 0.035222, acc: 100.00%] [G loss: 4.122914]\n",
            "******* 211 [D loss: 0.046333, acc: 100.00%] [G loss: 3.968375]\n",
            "******* 212 [D loss: 0.028464, acc: 100.00%] [G loss: 4.006289]\n",
            "******* 213 [D loss: 0.027285, acc: 100.00%] [G loss: 4.016934]\n",
            "******* 214 [D loss: 0.035027, acc: 99.22%] [G loss: 3.981992]\n",
            "******* 215 [D loss: 0.050961, acc: 98.44%] [G loss: 4.114560]\n",
            "******* 216 [D loss: 0.029276, acc: 100.00%] [G loss: 3.925408]\n",
            "******* 217 [D loss: 0.029606, acc: 100.00%] [G loss: 3.804648]\n",
            "******* 218 [D loss: 0.031940, acc: 100.00%] [G loss: 3.830742]\n",
            "******* 219 [D loss: 0.026894, acc: 100.00%] [G loss: 3.822336]\n",
            "******* 220 [D loss: 0.049707, acc: 99.22%] [G loss: 3.884508]\n",
            "******* 221 [D loss: 0.040446, acc: 100.00%] [G loss: 3.739069]\n",
            "******* 222 [D loss: 0.061535, acc: 99.22%] [G loss: 3.717521]\n",
            "******* 223 [D loss: 0.030520, acc: 100.00%] [G loss: 3.581511]\n",
            "******* 224 [D loss: 0.040457, acc: 100.00%] [G loss: 3.606329]\n",
            "******* 225 [D loss: 0.031568, acc: 100.00%] [G loss: 3.705970]\n",
            "******* 226 [D loss: 0.058742, acc: 99.22%] [G loss: 3.624743]\n",
            "******* 227 [D loss: 0.069087, acc: 96.88%] [G loss: 3.397411]\n",
            "******* 228 [D loss: 0.050182, acc: 100.00%] [G loss: 3.702609]\n",
            "******* 229 [D loss: 0.032662, acc: 100.00%] [G loss: 3.720977]\n",
            "******* 230 [D loss: 0.045637, acc: 100.00%] [G loss: 3.639207]\n",
            "******* 231 [D loss: 0.047761, acc: 99.22%] [G loss: 3.582117]\n",
            "******* 232 [D loss: 0.043722, acc: 100.00%] [G loss: 3.495820]\n",
            "******* 233 [D loss: 0.035011, acc: 100.00%] [G loss: 3.637794]\n",
            "******* 234 [D loss: 0.035979, acc: 100.00%] [G loss: 3.622926]\n",
            "******* 235 [D loss: 0.038058, acc: 100.00%] [G loss: 3.720262]\n",
            "******* 236 [D loss: 0.039932, acc: 100.00%] [G loss: 3.777061]\n",
            "******* 237 [D loss: 0.041034, acc: 100.00%] [G loss: 3.687749]\n",
            "******* 238 [D loss: 0.042116, acc: 99.22%] [G loss: 3.674531]\n",
            "******* 239 [D loss: 0.049817, acc: 100.00%] [G loss: 3.821708]\n",
            "******* 240 [D loss: 0.082409, acc: 96.88%] [G loss: 3.339980]\n",
            "******* 241 [D loss: 0.059282, acc: 99.22%] [G loss: 3.063647]\n",
            "******* 242 [D loss: 0.073761, acc: 97.66%] [G loss: 3.325940]\n",
            "******* 243 [D loss: 0.049789, acc: 99.22%] [G loss: 3.908546]\n",
            "******* 244 [D loss: 0.043987, acc: 100.00%] [G loss: 3.886015]\n",
            "******* 245 [D loss: 0.042973, acc: 100.00%] [G loss: 3.725913]\n",
            "******* 246 [D loss: 0.085846, acc: 97.66%] [G loss: 3.318372]\n",
            "******* 247 [D loss: 0.051605, acc: 98.44%] [G loss: 3.254590]\n",
            "******* 248 [D loss: 0.054795, acc: 100.00%] [G loss: 3.567793]\n",
            "******* 249 [D loss: 0.032674, acc: 100.00%] [G loss: 3.660369]\n",
            "******* 250 [D loss: 0.045808, acc: 99.22%] [G loss: 4.147536]\n",
            "******* 251 [D loss: 0.041820, acc: 100.00%] [G loss: 3.669899]\n",
            "******* 252 [D loss: 0.054731, acc: 100.00%] [G loss: 3.635321]\n",
            "******* 253 [D loss: 0.051925, acc: 99.22%] [G loss: 3.710111]\n",
            "******* 254 [D loss: 0.062322, acc: 97.66%] [G loss: 3.525239]\n",
            "******* 255 [D loss: 0.039912, acc: 100.00%] [G loss: 3.953642]\n",
            "******* 256 [D loss: 0.032005, acc: 100.00%] [G loss: 3.949395]\n",
            "******* 257 [D loss: 0.064823, acc: 98.44%] [G loss: 3.659393]\n",
            "******* 258 [D loss: 0.077641, acc: 97.66%] [G loss: 3.356377]\n",
            "******* 259 [D loss: 0.060762, acc: 100.00%] [G loss: 3.398682]\n",
            "******* 260 [D loss: 0.034939, acc: 100.00%] [G loss: 3.645518]\n",
            "******* 261 [D loss: 0.042512, acc: 100.00%] [G loss: 3.903091]\n",
            "******* 262 [D loss: 0.040606, acc: 100.00%] [G loss: 3.917298]\n",
            "******* 263 [D loss: 0.080606, acc: 98.44%] [G loss: 3.905794]\n",
            "******* 264 [D loss: 0.053915, acc: 100.00%] [G loss: 3.465722]\n",
            "******* 265 [D loss: 0.063193, acc: 99.22%] [G loss: 3.497899]\n",
            "******* 266 [D loss: 0.036786, acc: 99.22%] [G loss: 3.589260]\n",
            "******* 267 [D loss: 0.038611, acc: 100.00%] [G loss: 3.921337]\n",
            "******* 268 [D loss: 0.074713, acc: 98.44%] [G loss: 3.485768]\n",
            "******* 269 [D loss: 0.052329, acc: 99.22%] [G loss: 3.428097]\n",
            "******* 270 [D loss: 0.049177, acc: 100.00%] [G loss: 3.482230]\n",
            "******* 271 [D loss: 0.049986, acc: 100.00%] [G loss: 3.508360]\n",
            "******* 272 [D loss: 0.068231, acc: 98.44%] [G loss: 3.705496]\n",
            "******* 273 [D loss: 0.089968, acc: 97.66%] [G loss: 3.574234]\n",
            "******* 274 [D loss: 0.041590, acc: 100.00%] [G loss: 3.626964]\n",
            "******* 275 [D loss: 0.118183, acc: 97.66%] [G loss: 3.111670]\n",
            "******* 276 [D loss: 0.070643, acc: 98.44%] [G loss: 3.246699]\n",
            "******* 277 [D loss: 0.059487, acc: 99.22%] [G loss: 3.525354]\n",
            "******* 278 [D loss: 0.047961, acc: 100.00%] [G loss: 3.944645]\n",
            "******* 279 [D loss: 0.054992, acc: 99.22%] [G loss: 3.633426]\n",
            "******* 280 [D loss: 0.112702, acc: 96.88%] [G loss: 3.086694]\n",
            "******* 281 [D loss: 0.090733, acc: 99.22%] [G loss: 2.751018]\n",
            "******* 282 [D loss: 0.116077, acc: 94.53%] [G loss: 3.236512]\n",
            "******* 283 [D loss: 0.045526, acc: 100.00%] [G loss: 3.741994]\n",
            "******* 284 [D loss: 0.036346, acc: 100.00%] [G loss: 4.123039]\n",
            "******* 285 [D loss: 0.056004, acc: 99.22%] [G loss: 4.171796]\n",
            "******* 286 [D loss: 0.055402, acc: 99.22%] [G loss: 3.681186]\n",
            "******* 287 [D loss: 0.036718, acc: 100.00%] [G loss: 3.356235]\n",
            "******* 288 [D loss: 0.062517, acc: 100.00%] [G loss: 3.185724]\n",
            "******* 289 [D loss: 0.052212, acc: 100.00%] [G loss: 3.549747]\n",
            "******* 290 [D loss: 0.045985, acc: 100.00%] [G loss: 3.998889]\n",
            "******* 291 [D loss: 0.088957, acc: 96.88%] [G loss: 3.530305]\n",
            "******* 292 [D loss: 0.046051, acc: 99.22%] [G loss: 3.455017]\n",
            "******* 293 [D loss: 0.077252, acc: 99.22%] [G loss: 3.418571]\n",
            "******* 294 [D loss: 0.049085, acc: 100.00%] [G loss: 3.969641]\n",
            "******* 295 [D loss: 0.033589, acc: 100.00%] [G loss: 4.119432]\n",
            "******* 296 [D loss: 0.103554, acc: 96.88%] [G loss: 3.686116]\n",
            "******* 297 [D loss: 0.084426, acc: 97.66%] [G loss: 3.082952]\n",
            "******* 298 [D loss: 0.084140, acc: 96.88%] [G loss: 3.133068]\n",
            "******* 299 [D loss: 0.052289, acc: 99.22%] [G loss: 3.838853]\n",
            "******* 300 [D loss: 0.025672, acc: 100.00%] [G loss: 4.416451]\n",
            "0.00000014\n",
            "saved\n",
            "******* 301 [D loss: 0.046195, acc: 99.22%] [G loss: 4.301845]\n",
            "******* 302 [D loss: 0.072369, acc: 98.44%] [G loss: 4.005923]\n",
            "******* 303 [D loss: 0.048223, acc: 100.00%] [G loss: 3.230310]\n",
            "******* 304 [D loss: 0.075411, acc: 98.44%] [G loss: 3.171371]\n",
            "******* 305 [D loss: 0.047025, acc: 100.00%] [G loss: 3.379663]\n",
            "******* 306 [D loss: 0.040217, acc: 99.22%] [G loss: 3.943791]\n",
            "******* 307 [D loss: 0.028542, acc: 100.00%] [G loss: 4.423566]\n",
            "******* 308 [D loss: 0.044990, acc: 100.00%] [G loss: 4.369890]\n",
            "******* 309 [D loss: 0.041702, acc: 100.00%] [G loss: 4.410347]\n",
            "******* 310 [D loss: 0.042342, acc: 99.22%] [G loss: 3.944403]\n",
            "******* 311 [D loss: 0.057851, acc: 99.22%] [G loss: 3.840909]\n",
            "******* 312 [D loss: 0.037131, acc: 100.00%] [G loss: 3.898599]\n",
            "******* 313 [D loss: 0.077986, acc: 98.44%] [G loss: 4.104691]\n",
            "******* 314 [D loss: 0.049146, acc: 99.22%] [G loss: 3.878645]\n",
            "******* 315 [D loss: 0.047435, acc: 99.22%] [G loss: 4.277225]\n",
            "******* 316 [D loss: 0.027593, acc: 100.00%] [G loss: 4.285131]\n",
            "******* 317 [D loss: 0.040454, acc: 100.00%] [G loss: 4.062377]\n",
            "******* 318 [D loss: 0.047653, acc: 99.22%] [G loss: 3.935716]\n",
            "******* 319 [D loss: 0.030857, acc: 100.00%] [G loss: 4.034339]\n",
            "******* 320 [D loss: 0.046134, acc: 99.22%] [G loss: 3.981710]\n",
            "******* 321 [D loss: 0.059922, acc: 99.22%] [G loss: 3.939313]\n",
            "******* 322 [D loss: 0.049940, acc: 100.00%] [G loss: 3.860002]\n",
            "******* 323 [D loss: 0.034804, acc: 99.22%] [G loss: 3.990781]\n",
            "******* 324 [D loss: 0.047402, acc: 100.00%] [G loss: 4.357414]\n",
            "******* 325 [D loss: 0.019520, acc: 100.00%] [G loss: 4.235065]\n",
            "******* 326 [D loss: 0.030064, acc: 100.00%] [G loss: 4.159125]\n",
            "******* 327 [D loss: 0.024778, acc: 100.00%] [G loss: 4.316763]\n",
            "******* 328 [D loss: 0.039198, acc: 99.22%] [G loss: 4.013517]\n",
            "******* 329 [D loss: 0.035164, acc: 99.22%] [G loss: 4.175590]\n",
            "******* 330 [D loss: 0.054395, acc: 99.22%] [G loss: 3.970443]\n",
            "******* 331 [D loss: 0.033444, acc: 100.00%] [G loss: 4.288744]\n",
            "******* 332 [D loss: 0.036047, acc: 99.22%] [G loss: 4.226417]\n",
            "******* 333 [D loss: 0.044896, acc: 98.44%] [G loss: 4.013021]\n",
            "******* 334 [D loss: 0.044168, acc: 100.00%] [G loss: 4.005997]\n",
            "******* 335 [D loss: 0.022562, acc: 100.00%] [G loss: 3.892874]\n",
            "******* 336 [D loss: 0.023415, acc: 100.00%] [G loss: 4.035171]\n",
            "******* 337 [D loss: 0.025993, acc: 100.00%] [G loss: 4.081787]\n",
            "******* 338 [D loss: 0.033954, acc: 100.00%] [G loss: 4.269502]\n",
            "******* 339 [D loss: 0.045699, acc: 99.22%] [G loss: 4.062949]\n",
            "******* 340 [D loss: 0.031367, acc: 100.00%] [G loss: 3.953688]\n",
            "******* 341 [D loss: 0.030530, acc: 100.00%] [G loss: 4.017457]\n",
            "******* 342 [D loss: 0.040656, acc: 100.00%] [G loss: 4.122766]\n",
            "******* 343 [D loss: 0.026576, acc: 100.00%] [G loss: 3.929368]\n",
            "******* 344 [D loss: 0.024827, acc: 100.00%] [G loss: 4.431710]\n",
            "******* 345 [D loss: 0.043835, acc: 99.22%] [G loss: 4.052885]\n",
            "******* 346 [D loss: 0.034592, acc: 100.00%] [G loss: 4.305009]\n",
            "******* 347 [D loss: 0.027473, acc: 100.00%] [G loss: 4.443685]\n",
            "******* 348 [D loss: 0.018407, acc: 100.00%] [G loss: 4.556403]\n",
            "******* 349 [D loss: 0.047424, acc: 99.22%] [G loss: 4.398082]\n",
            "******* 350 [D loss: 0.022059, acc: 100.00%] [G loss: 4.325956]\n",
            "******* 351 [D loss: 0.032542, acc: 99.22%] [G loss: 4.320728]\n",
            "******* 352 [D loss: 0.034940, acc: 100.00%] [G loss: 4.489381]\n",
            "******* 353 [D loss: 0.026582, acc: 99.22%] [G loss: 4.474100]\n",
            "******* 354 [D loss: 0.020495, acc: 100.00%] [G loss: 4.242880]\n",
            "******* 355 [D loss: 0.022795, acc: 100.00%] [G loss: 4.446663]\n",
            "******* 356 [D loss: 0.024001, acc: 100.00%] [G loss: 4.626894]\n",
            "******* 357 [D loss: 0.022524, acc: 100.00%] [G loss: 4.583196]\n",
            "******* 358 [D loss: 0.027790, acc: 100.00%] [G loss: 4.314053]\n",
            "******* 359 [D loss: 0.019849, acc: 100.00%] [G loss: 4.725298]\n",
            "******* 360 [D loss: 0.020926, acc: 100.00%] [G loss: 4.547339]\n",
            "******* 361 [D loss: 0.065430, acc: 97.66%] [G loss: 4.050092]\n",
            "******* 362 [D loss: 0.036377, acc: 100.00%] [G loss: 3.901723]\n",
            "******* 363 [D loss: 0.021029, acc: 100.00%] [G loss: 4.389702]\n",
            "******* 364 [D loss: 0.027891, acc: 100.00%] [G loss: 4.675225]\n",
            "******* 365 [D loss: 0.016886, acc: 100.00%] [G loss: 5.014174]\n",
            "******* 366 [D loss: 0.025436, acc: 99.22%] [G loss: 4.668122]\n",
            "******* 367 [D loss: 0.025130, acc: 99.22%] [G loss: 4.800042]\n",
            "******* 368 [D loss: 0.039587, acc: 99.22%] [G loss: 4.069426]\n",
            "******* 369 [D loss: 0.031939, acc: 100.00%] [G loss: 3.727428]\n",
            "******* 370 [D loss: 0.023236, acc: 100.00%] [G loss: 4.532624]\n",
            "******* 371 [D loss: 0.021725, acc: 100.00%] [G loss: 4.689343]\n",
            "******* 372 [D loss: 0.019810, acc: 100.00%] [G loss: 4.896298]\n",
            "******* 373 [D loss: 0.039793, acc: 99.22%] [G loss: 4.729945]\n",
            "******* 374 [D loss: 0.040446, acc: 99.22%] [G loss: 4.494358]\n",
            "******* 375 [D loss: 0.029139, acc: 100.00%] [G loss: 4.206130]\n",
            "******* 376 [D loss: 0.026373, acc: 100.00%] [G loss: 4.433503]\n",
            "******* 377 [D loss: 0.023284, acc: 100.00%] [G loss: 4.645727]\n",
            "******* 378 [D loss: 0.024550, acc: 100.00%] [G loss: 4.546790]\n",
            "******* 379 [D loss: 0.021140, acc: 100.00%] [G loss: 4.907203]\n",
            "******* 380 [D loss: 0.028010, acc: 99.22%] [G loss: 4.242650]\n",
            "******* 381 [D loss: 0.027405, acc: 99.22%] [G loss: 4.024393]\n",
            "******* 382 [D loss: 0.036670, acc: 100.00%] [G loss: 4.192483]\n",
            "******* 383 [D loss: 0.036459, acc: 100.00%] [G loss: 4.463545]\n",
            "******* 384 [D loss: 0.020815, acc: 100.00%] [G loss: 5.152524]\n",
            "******* 385 [D loss: 0.026732, acc: 99.22%] [G loss: 5.137819]\n",
            "******* 386 [D loss: 0.027491, acc: 100.00%] [G loss: 4.680909]\n",
            "******* 387 [D loss: 0.035092, acc: 99.22%] [G loss: 3.744352]\n",
            "******* 388 [D loss: 0.030789, acc: 100.00%] [G loss: 4.315207]\n",
            "******* 389 [D loss: 0.020712, acc: 100.00%] [G loss: 4.959607]\n",
            "******* 390 [D loss: 0.011452, acc: 100.00%] [G loss: 5.420987]\n",
            "******* 391 [D loss: 0.015546, acc: 100.00%] [G loss: 5.702477]\n",
            "******* 392 [D loss: 0.009132, acc: 100.00%] [G loss: 5.541414]\n",
            "******* 393 [D loss: 0.019602, acc: 100.00%] [G loss: 4.870372]\n",
            "******* 394 [D loss: 0.015070, acc: 100.00%] [G loss: 4.941993]\n",
            "******* 395 [D loss: 0.014907, acc: 100.00%] [G loss: 4.512188]\n",
            "******* 396 [D loss: 0.023203, acc: 100.00%] [G loss: 4.428527]\n",
            "******* 397 [D loss: 0.025767, acc: 100.00%] [G loss: 4.446458]\n",
            "******* 398 [D loss: 0.013305, acc: 100.00%] [G loss: 4.778904]\n",
            "******* 399 [D loss: 0.035784, acc: 99.22%] [G loss: 4.179931]\n",
            "******* 400 [D loss: 0.020098, acc: 100.00%] [G loss: 4.199291]\n",
            "0.00000015\n",
            "saved\n",
            "******* 401 [D loss: 0.025825, acc: 100.00%] [G loss: 4.641763]\n",
            "******* 402 [D loss: 0.014655, acc: 100.00%] [G loss: 4.978851]\n",
            "******* 403 [D loss: 0.065860, acc: 98.44%] [G loss: 3.954771]\n",
            "******* 404 [D loss: 0.030983, acc: 100.00%] [G loss: 4.097256]\n",
            "******* 405 [D loss: 0.029632, acc: 100.00%] [G loss: 3.998979]\n",
            "******* 406 [D loss: 0.024594, acc: 100.00%] [G loss: 5.043188]\n",
            "******* 407 [D loss: 0.013514, acc: 100.00%] [G loss: 5.242399]\n",
            "******* 408 [D loss: 0.009272, acc: 100.00%] [G loss: 5.682310]\n",
            "******* 409 [D loss: 0.020346, acc: 100.00%] [G loss: 4.959332]\n",
            "******* 410 [D loss: 0.016235, acc: 100.00%] [G loss: 4.606887]\n",
            "******* 411 [D loss: 0.035117, acc: 99.22%] [G loss: 3.835817]\n",
            "******* 412 [D loss: 0.029520, acc: 100.00%] [G loss: 4.002547]\n",
            "******* 413 [D loss: 0.015381, acc: 100.00%] [G loss: 4.503901]\n",
            "******* 414 [D loss: 0.013323, acc: 100.00%] [G loss: 5.175214]\n",
            "******* 415 [D loss: 0.011778, acc: 100.00%] [G loss: 5.118395]\n",
            "******* 416 [D loss: 0.019797, acc: 100.00%] [G loss: 4.939317]\n",
            "******* 417 [D loss: 0.019737, acc: 100.00%] [G loss: 4.403518]\n",
            "******* 418 [D loss: 0.023630, acc: 100.00%] [G loss: 4.264872]\n",
            "******* 419 [D loss: 0.024685, acc: 100.00%] [G loss: 4.519691]\n",
            "******* 420 [D loss: 0.023465, acc: 100.00%] [G loss: 4.507417]\n",
            "******* 421 [D loss: 0.013591, acc: 100.00%] [G loss: 4.617816]\n",
            "******* 422 [D loss: 0.014690, acc: 100.00%] [G loss: 5.334825]\n",
            "******* 423 [D loss: 0.020050, acc: 99.22%] [G loss: 5.027827]\n",
            "******* 424 [D loss: 0.032077, acc: 100.00%] [G loss: 4.126957]\n",
            "******* 425 [D loss: 0.026086, acc: 100.00%] [G loss: 3.748266]\n",
            "******* 426 [D loss: 0.022034, acc: 100.00%] [G loss: 4.740966]\n",
            "******* 427 [D loss: 0.013537, acc: 100.00%] [G loss: 5.004742]\n",
            "******* 428 [D loss: 0.041026, acc: 99.22%] [G loss: 4.337957]\n",
            "******* 429 [D loss: 0.019199, acc: 100.00%] [G loss: 4.501211]\n",
            "******* 430 [D loss: 0.019851, acc: 100.00%] [G loss: 4.478508]\n",
            "******* 431 [D loss: 0.015639, acc: 100.00%] [G loss: 4.520634]\n",
            "******* 432 [D loss: 0.026003, acc: 100.00%] [G loss: 4.379743]\n",
            "******* 433 [D loss: 0.022817, acc: 100.00%] [G loss: 4.241066]\n",
            "******* 434 [D loss: 0.019478, acc: 100.00%] [G loss: 3.997788]\n",
            "******* 435 [D loss: 0.018265, acc: 100.00%] [G loss: 4.438999]\n",
            "******* 436 [D loss: 0.015748, acc: 100.00%] [G loss: 4.347097]\n",
            "******* 437 [D loss: 0.023034, acc: 100.00%] [G loss: 4.175397]\n",
            "******* 438 [D loss: 0.016495, acc: 100.00%] [G loss: 4.661989]\n",
            "******* 439 [D loss: 0.017949, acc: 100.00%] [G loss: 4.342699]\n",
            "******* 440 [D loss: 0.032103, acc: 100.00%] [G loss: 4.215869]\n",
            "******* 441 [D loss: 0.037288, acc: 99.22%] [G loss: 4.225758]\n",
            "******* 442 [D loss: 0.034002, acc: 99.22%] [G loss: 4.176451]\n",
            "******* 443 [D loss: 0.031985, acc: 99.22%] [G loss: 4.784617]\n",
            "******* 444 [D loss: 0.011639, acc: 100.00%] [G loss: 5.226919]\n",
            "******* 445 [D loss: 0.034715, acc: 100.00%] [G loss: 4.568304]\n",
            "******* 446 [D loss: 0.019932, acc: 100.00%] [G loss: 4.004580]\n",
            "******* 447 [D loss: 0.017412, acc: 100.00%] [G loss: 4.257635]\n",
            "******* 448 [D loss: 0.035474, acc: 99.22%] [G loss: 4.152376]\n",
            "******* 449 [D loss: 0.022090, acc: 100.00%] [G loss: 4.786864]\n",
            "******* 450 [D loss: 0.020507, acc: 100.00%] [G loss: 4.647372]\n",
            "******* 451 [D loss: 0.018842, acc: 100.00%] [G loss: 5.082985]\n",
            "******* 452 [D loss: 0.037847, acc: 99.22%] [G loss: 3.877945]\n",
            "******* 453 [D loss: 0.038794, acc: 100.00%] [G loss: 3.718858]\n",
            "******* 454 [D loss: 0.022008, acc: 100.00%] [G loss: 4.242499]\n",
            "******* 455 [D loss: 0.026844, acc: 99.22%] [G loss: 5.335139]\n",
            "******* 456 [D loss: 0.024704, acc: 100.00%] [G loss: 5.108117]\n",
            "******* 457 [D loss: 0.048204, acc: 97.66%] [G loss: 4.423808]\n",
            "******* 458 [D loss: 0.024802, acc: 100.00%] [G loss: 4.149933]\n",
            "******* 459 [D loss: 0.017411, acc: 100.00%] [G loss: 4.574219]\n",
            "******* 460 [D loss: 0.015396, acc: 100.00%] [G loss: 4.858422]\n",
            "******* 461 [D loss: 0.039822, acc: 98.44%] [G loss: 4.284659]\n",
            "******* 462 [D loss: 0.021491, acc: 100.00%] [G loss: 4.539996]\n",
            "******* 463 [D loss: 0.020207, acc: 100.00%] [G loss: 4.358617]\n",
            "******* 464 [D loss: 0.037240, acc: 99.22%] [G loss: 4.328102]\n",
            "******* 465 [D loss: 0.029949, acc: 100.00%] [G loss: 4.230600]\n",
            "******* 466 [D loss: 0.041961, acc: 99.22%] [G loss: 4.475431]\n",
            "******* 467 [D loss: 0.052661, acc: 98.44%] [G loss: 4.150747]\n",
            "******* 468 [D loss: 0.069255, acc: 100.00%] [G loss: 4.758111]\n",
            "******* 469 [D loss: 0.026830, acc: 100.00%] [G loss: 5.085604]\n",
            "******* 470 [D loss: 0.039545, acc: 99.22%] [G loss: 4.449541]\n",
            "******* 471 [D loss: 0.028021, acc: 100.00%] [G loss: 4.261764]\n",
            "******* 472 [D loss: 0.023195, acc: 100.00%] [G loss: 4.539424]\n",
            "******* 473 [D loss: 0.019912, acc: 100.00%] [G loss: 4.528412]\n",
            "******* 474 [D loss: 0.050573, acc: 97.66%] [G loss: 4.180483]\n",
            "******* 475 [D loss: 0.042180, acc: 99.22%] [G loss: 3.803923]\n",
            "******* 476 [D loss: 0.024527, acc: 100.00%] [G loss: 4.477399]\n",
            "******* 477 [D loss: 0.030293, acc: 99.22%] [G loss: 5.067654]\n",
            "******* 478 [D loss: 0.064028, acc: 99.22%] [G loss: 4.261226]\n",
            "******* 479 [D loss: 0.028379, acc: 100.00%] [G loss: 4.586930]\n",
            "******* 480 [D loss: 0.018752, acc: 100.00%] [G loss: 5.130015]\n",
            "******* 481 [D loss: 0.033295, acc: 99.22%] [G loss: 5.290791]\n",
            "******* 482 [D loss: 0.026669, acc: 100.00%] [G loss: 4.732555]\n",
            "******* 483 [D loss: 0.039584, acc: 100.00%] [G loss: 4.412268]\n",
            "******* 484 [D loss: 0.031052, acc: 100.00%] [G loss: 4.112629]\n",
            "******* 485 [D loss: 0.031379, acc: 100.00%] [G loss: 4.346519]\n",
            "******* 486 [D loss: 0.026639, acc: 100.00%] [G loss: 4.611209]\n",
            "******* 487 [D loss: 0.065868, acc: 98.44%] [G loss: 3.602991]\n",
            "******* 488 [D loss: 0.047286, acc: 99.22%] [G loss: 4.071547]\n",
            "******* 489 [D loss: 0.031420, acc: 99.22%] [G loss: 4.866389]\n",
            "******* 490 [D loss: 0.043563, acc: 99.22%] [G loss: 4.684538]\n",
            "******* 491 [D loss: 0.030619, acc: 100.00%] [G loss: 4.244005]\n",
            "******* 492 [D loss: 0.035656, acc: 100.00%] [G loss: 4.320095]\n",
            "******* 493 [D loss: 0.032228, acc: 100.00%] [G loss: 4.689522]\n",
            "******* 494 [D loss: 0.032621, acc: 100.00%] [G loss: 4.641454]\n",
            "******* 495 [D loss: 0.043769, acc: 99.22%] [G loss: 4.556448]\n",
            "******* 496 [D loss: 0.047046, acc: 99.22%] [G loss: 3.992563]\n",
            "******* 497 [D loss: 0.046374, acc: 99.22%] [G loss: 4.699864]\n",
            "******* 498 [D loss: 0.019712, acc: 100.00%] [G loss: 5.541111]\n",
            "******* 499 [D loss: 0.020117, acc: 100.00%] [G loss: 6.098006]\n",
            "******* 500 [D loss: 0.045182, acc: 99.22%] [G loss: 4.536022]\n",
            "0.00000016\n",
            "saved\n",
            "******* 501 [D loss: 0.057264, acc: 99.22%] [G loss: 3.449644]\n",
            "******* 502 [D loss: 0.055678, acc: 99.22%] [G loss: 3.970713]\n",
            "******* 503 [D loss: 0.035203, acc: 100.00%] [G loss: 5.565690]\n",
            "******* 504 [D loss: 0.052603, acc: 98.44%] [G loss: 5.847563]\n",
            "******* 505 [D loss: 0.085910, acc: 96.88%] [G loss: 3.487298]\n",
            "******* 506 [D loss: 0.098108, acc: 96.88%] [G loss: 3.942878]\n",
            "******* 507 [D loss: 0.020103, acc: 100.00%] [G loss: 5.473543]\n",
            "******* 508 [D loss: 0.018027, acc: 99.22%] [G loss: 6.119548]\n",
            "******* 509 [D loss: 0.092529, acc: 96.09%] [G loss: 4.487971]\n",
            "******* 510 [D loss: 0.069069, acc: 99.22%] [G loss: 3.050379]\n",
            "******* 511 [D loss: 0.045503, acc: 100.00%] [G loss: 4.269262]\n",
            "******* 512 [D loss: 0.033751, acc: 99.22%] [G loss: 5.562343]\n",
            "******* 513 [D loss: 0.013801, acc: 100.00%] [G loss: 6.362242]\n",
            "******* 514 [D loss: 0.020942, acc: 100.00%] [G loss: 6.567294]\n",
            "******* 515 [D loss: 0.026727, acc: 100.00%] [G loss: 5.422976]\n",
            "******* 516 [D loss: 0.038956, acc: 99.22%] [G loss: 4.817456]\n",
            "******* 517 [D loss: 0.030306, acc: 99.22%] [G loss: 4.925870]\n",
            "******* 518 [D loss: 0.015380, acc: 100.00%] [G loss: 5.516791]\n",
            "******* 519 [D loss: 0.051422, acc: 98.44%] [G loss: 4.347319]\n",
            "******* 520 [D loss: 0.074798, acc: 97.66%] [G loss: 3.475286]\n",
            "******* 521 [D loss: 0.070659, acc: 100.00%] [G loss: 3.984979]\n",
            "******* 522 [D loss: 0.017614, acc: 100.00%] [G loss: 5.468783]\n",
            "******* 523 [D loss: 0.083242, acc: 97.66%] [G loss: 4.863284]\n",
            "******* 524 [D loss: 0.072469, acc: 97.66%] [G loss: 3.076912]\n",
            "******* 525 [D loss: 0.081269, acc: 99.22%] [G loss: 3.558619]\n",
            "******* 526 [D loss: 0.038873, acc: 99.22%] [G loss: 5.356860]\n",
            "******* 527 [D loss: 0.057413, acc: 98.44%] [G loss: 5.626712]\n",
            "******* 528 [D loss: 0.030614, acc: 100.00%] [G loss: 4.667699]\n",
            "******* 529 [D loss: 0.086068, acc: 97.66%] [G loss: 4.492285]\n",
            "******* 530 [D loss: 0.042507, acc: 100.00%] [G loss: 4.542287]\n",
            "******* 531 [D loss: 0.026402, acc: 100.00%] [G loss: 5.648780]\n",
            "******* 532 [D loss: 0.031460, acc: 99.22%] [G loss: 5.546020]\n",
            "******* 533 [D loss: 0.018854, acc: 100.00%] [G loss: 4.724018]\n",
            "******* 534 [D loss: 0.030502, acc: 100.00%] [G loss: 4.418670]\n",
            "******* 535 [D loss: 0.118924, acc: 98.44%] [G loss: 4.071450]\n",
            "******* 536 [D loss: 0.051005, acc: 100.00%] [G loss: 4.163622]\n",
            "******* 537 [D loss: 0.051876, acc: 98.44%] [G loss: 4.758241]\n",
            "******* 538 [D loss: 0.025291, acc: 100.00%] [G loss: 5.289017]\n",
            "******* 539 [D loss: 0.069739, acc: 98.44%] [G loss: 4.213223]\n",
            "******* 540 [D loss: 0.059757, acc: 100.00%] [G loss: 4.108469]\n",
            "******* 541 [D loss: 0.030282, acc: 100.00%] [G loss: 5.247144]\n",
            "******* 542 [D loss: 0.054682, acc: 97.66%] [G loss: 4.122629]\n",
            "******* 543 [D loss: 0.037846, acc: 100.00%] [G loss: 4.129679]\n",
            "******* 544 [D loss: 0.045505, acc: 98.44%] [G loss: 4.729633]\n",
            "******* 545 [D loss: 0.034738, acc: 100.00%] [G loss: 4.677315]\n",
            "******* 546 [D loss: 0.193266, acc: 95.31%] [G loss: 3.076994]\n",
            "******* 547 [D loss: 0.065817, acc: 99.22%] [G loss: 4.611391]\n",
            "******* 548 [D loss: 0.063658, acc: 99.22%] [G loss: 5.950122]\n",
            "******* 549 [D loss: 0.045342, acc: 97.66%] [G loss: 5.088286]\n",
            "******* 550 [D loss: 0.037920, acc: 99.22%] [G loss: 4.892310]\n",
            "******* 551 [D loss: 0.042560, acc: 100.00%] [G loss: 4.144069]\n",
            "******* 552 [D loss: 0.023290, acc: 100.00%] [G loss: 4.677238]\n",
            "******* 553 [D loss: 0.037847, acc: 99.22%] [G loss: 4.815363]\n",
            "******* 554 [D loss: 0.022550, acc: 100.00%] [G loss: 4.858276]\n",
            "******* 555 [D loss: 0.040223, acc: 100.00%] [G loss: 4.303815]\n",
            "******* 556 [D loss: 0.085005, acc: 99.22%] [G loss: 4.596040]\n",
            "******* 557 [D loss: 0.056190, acc: 99.22%] [G loss: 3.940713]\n",
            "******* 558 [D loss: 0.047756, acc: 99.22%] [G loss: 4.878401]\n",
            "******* 559 [D loss: 0.101707, acc: 97.66%] [G loss: 3.437444]\n",
            "******* 560 [D loss: 0.069043, acc: 100.00%] [G loss: 3.699883]\n",
            "******* 561 [D loss: 0.043656, acc: 98.44%] [G loss: 4.986284]\n",
            "******* 562 [D loss: 0.043362, acc: 99.22%] [G loss: 5.016245]\n",
            "******* 563 [D loss: 0.075017, acc: 98.44%] [G loss: 4.012454]\n",
            "******* 564 [D loss: 0.057533, acc: 100.00%] [G loss: 4.096241]\n",
            "******* 565 [D loss: 0.030268, acc: 100.00%] [G loss: 5.039518]\n",
            "******* 566 [D loss: 0.047669, acc: 99.22%] [G loss: 5.376514]\n",
            "******* 567 [D loss: 0.068675, acc: 98.44%] [G loss: 4.093478]\n",
            "******* 568 [D loss: 0.052610, acc: 100.00%] [G loss: 4.023688]\n",
            "******* 569 [D loss: 0.037376, acc: 98.44%] [G loss: 4.506526]\n",
            "******* 570 [D loss: 0.023403, acc: 100.00%] [G loss: 5.058908]\n",
            "******* 571 [D loss: 0.051769, acc: 99.22%] [G loss: 4.394462]\n",
            "******* 572 [D loss: 0.048904, acc: 100.00%] [G loss: 4.017501]\n",
            "******* 573 [D loss: 0.040140, acc: 99.22%] [G loss: 4.078669]\n",
            "******* 574 [D loss: 0.077589, acc: 98.44%] [G loss: 4.513799]\n",
            "******* 575 [D loss: 0.038747, acc: 100.00%] [G loss: 5.045228]\n",
            "******* 576 [D loss: 0.064395, acc: 99.22%] [G loss: 4.735925]\n",
            "******* 577 [D loss: 0.041662, acc: 99.22%] [G loss: 4.677979]\n",
            "******* 578 [D loss: 0.045443, acc: 99.22%] [G loss: 4.566730]\n",
            "******* 579 [D loss: 0.027635, acc: 100.00%] [G loss: 5.740537]\n",
            "******* 580 [D loss: 0.078800, acc: 98.44%] [G loss: 4.494666]\n",
            "******* 581 [D loss: 0.030852, acc: 100.00%] [G loss: 4.521944]\n",
            "******* 582 [D loss: 0.083469, acc: 98.44%] [G loss: 3.861294]\n",
            "******* 583 [D loss: 0.103165, acc: 99.22%] [G loss: 4.213304]\n",
            "******* 584 [D loss: 0.046326, acc: 99.22%] [G loss: 5.044116]\n",
            "******* 585 [D loss: 0.126997, acc: 96.09%] [G loss: 3.253498]\n",
            "******* 586 [D loss: 0.080229, acc: 98.44%] [G loss: 4.181846]\n",
            "******* 587 [D loss: 0.022765, acc: 100.00%] [G loss: 5.677474]\n",
            "******* 588 [D loss: 0.099767, acc: 97.66%] [G loss: 3.769535]\n",
            "******* 589 [D loss: 0.096494, acc: 98.44%] [G loss: 3.476371]\n",
            "******* 590 [D loss: 0.073570, acc: 97.66%] [G loss: 5.394191]\n",
            "******* 591 [D loss: 0.019445, acc: 100.00%] [G loss: 6.316241]\n",
            "******* 592 [D loss: 0.072591, acc: 98.44%] [G loss: 4.509770]\n",
            "******* 593 [D loss: 0.064746, acc: 99.22%] [G loss: 3.994049]\n",
            "******* 594 [D loss: 0.079841, acc: 99.22%] [G loss: 4.258891]\n",
            "******* 595 [D loss: 0.026257, acc: 100.00%] [G loss: 5.390078]\n",
            "******* 596 [D loss: 0.048236, acc: 98.44%] [G loss: 5.077689]\n",
            "******* 597 [D loss: 0.062965, acc: 99.22%] [G loss: 3.821561]\n",
            "******* 598 [D loss: 0.047032, acc: 100.00%] [G loss: 4.173486]\n",
            "******* 599 [D loss: 0.066220, acc: 98.44%] [G loss: 4.548773]\n",
            "******* 600 [D loss: 0.044215, acc: 99.22%] [G loss: 4.923948]\n",
            "0.00000017\n",
            "saved\n",
            "******* 601 [D loss: 0.078147, acc: 96.88%] [G loss: 3.780761]\n",
            "******* 602 [D loss: 0.100140, acc: 96.88%] [G loss: 3.691295]\n",
            "******* 603 [D loss: 0.022810, acc: 100.00%] [G loss: 5.539266]\n",
            "******* 604 [D loss: 0.231301, acc: 92.97%] [G loss: 3.906603]\n",
            "******* 605 [D loss: 0.201475, acc: 91.41%] [G loss: 3.353284]\n",
            "******* 606 [D loss: 0.060995, acc: 99.22%] [G loss: 5.358067]\n",
            "******* 607 [D loss: 0.060376, acc: 98.44%] [G loss: 6.102304]\n",
            "******* 608 [D loss: 0.108505, acc: 96.88%] [G loss: 4.219881]\n",
            "******* 609 [D loss: 0.109555, acc: 96.09%] [G loss: 4.770737]\n",
            "******* 610 [D loss: 0.024205, acc: 100.00%] [G loss: 5.360915]\n",
            "******* 611 [D loss: 0.037401, acc: 99.22%] [G loss: 6.232166]\n",
            "******* 612 [D loss: 0.166156, acc: 96.09%] [G loss: 4.020412]\n",
            "******* 613 [D loss: 0.072063, acc: 98.44%] [G loss: 3.520613]\n",
            "******* 614 [D loss: 0.032493, acc: 100.00%] [G loss: 4.549893]\n",
            "******* 615 [D loss: 0.018302, acc: 100.00%] [G loss: 6.124436]\n",
            "******* 616 [D loss: 0.101724, acc: 96.88%] [G loss: 3.740115]\n",
            "******* 617 [D loss: 0.136004, acc: 95.31%] [G loss: 4.154364]\n",
            "******* 618 [D loss: 0.022948, acc: 100.00%] [G loss: 5.221016]\n",
            "******* 619 [D loss: 0.095851, acc: 97.66%] [G loss: 5.120891]\n",
            "******* 620 [D loss: 0.041551, acc: 99.22%] [G loss: 4.677682]\n",
            "******* 621 [D loss: 0.092130, acc: 98.44%] [G loss: 4.268470]\n",
            "******* 622 [D loss: 0.038972, acc: 100.00%] [G loss: 4.960446]\n",
            "******* 623 [D loss: 0.080721, acc: 96.88%] [G loss: 4.028819]\n",
            "******* 624 [D loss: 0.065174, acc: 98.44%] [G loss: 4.693479]\n",
            "******* 625 [D loss: 0.073511, acc: 98.44%] [G loss: 4.145520]\n",
            "******* 626 [D loss: 0.039377, acc: 99.22%] [G loss: 4.785964]\n",
            "******* 627 [D loss: 0.113416, acc: 96.88%] [G loss: 3.629532]\n",
            "******* 628 [D loss: 0.076537, acc: 97.66%] [G loss: 3.881298]\n",
            "******* 629 [D loss: 0.107530, acc: 97.66%] [G loss: 4.539539]\n",
            "******* 630 [D loss: 0.075830, acc: 98.44%] [G loss: 4.607470]\n",
            "******* 631 [D loss: 0.126963, acc: 96.09%] [G loss: 3.759089]\n",
            "******* 632 [D loss: 0.046024, acc: 100.00%] [G loss: 4.662754]\n",
            "******* 633 [D loss: 0.129881, acc: 96.09%] [G loss: 3.501155]\n",
            "******* 634 [D loss: 0.109709, acc: 97.66%] [G loss: 3.389774]\n",
            "******* 635 [D loss: 0.092031, acc: 98.44%] [G loss: 4.048955]\n",
            "******* 636 [D loss: 0.047019, acc: 99.22%] [G loss: 4.986253]\n",
            "******* 637 [D loss: 0.134559, acc: 97.66%] [G loss: 2.573555]\n",
            "******* 638 [D loss: 0.204953, acc: 92.19%] [G loss: 3.707912]\n",
            "******* 639 [D loss: 0.023273, acc: 100.00%] [G loss: 6.635398]\n",
            "******* 640 [D loss: 0.319505, acc: 89.06%] [G loss: 1.998285]\n",
            "******* 641 [D loss: 0.617638, acc: 65.62%] [G loss: 3.611652]\n",
            "******* 642 [D loss: 0.006016, acc: 100.00%] [G loss: 10.356928]\n",
            "******* 643 [D loss: 0.969287, acc: 67.97%] [G loss: 2.699814]\n",
            "******* 644 [D loss: 0.953986, acc: 60.94%] [G loss: 1.981962]\n",
            "******* 645 [D loss: 0.202626, acc: 89.84%] [G loss: 5.025194]\n",
            "******* 646 [D loss: 0.008982, acc: 100.00%] [G loss: 9.276760]\n",
            "******* 647 [D loss: 0.200649, acc: 93.75%] [G loss: 10.896674]\n",
            "******* 648 [D loss: 0.339574, acc: 88.28%] [G loss: 8.066650]\n",
            "******* 649 [D loss: 0.065674, acc: 98.44%] [G loss: 4.751649]\n",
            "******* 650 [D loss: 0.119394, acc: 96.09%] [G loss: 3.140373]\n",
            "******* 651 [D loss: 0.135712, acc: 94.53%] [G loss: 3.778441]\n",
            "******* 652 [D loss: 0.044210, acc: 98.44%] [G loss: 5.030845]\n",
            "******* 653 [D loss: 0.033325, acc: 98.44%] [G loss: 6.192180]\n",
            "******* 654 [D loss: 0.096935, acc: 96.88%] [G loss: 5.133317]\n",
            "******* 655 [D loss: 0.100605, acc: 96.09%] [G loss: 4.233507]\n",
            "******* 656 [D loss: 0.074516, acc: 99.22%] [G loss: 3.952109]\n",
            "******* 657 [D loss: 0.085653, acc: 97.66%] [G loss: 4.504863]\n",
            "******* 658 [D loss: 0.103011, acc: 96.09%] [G loss: 4.376445]\n",
            "******* 659 [D loss: 0.217870, acc: 92.97%] [G loss: 3.690588]\n",
            "******* 660 [D loss: 0.152011, acc: 91.41%] [G loss: 4.060369]\n",
            "******* 661 [D loss: 0.131265, acc: 94.53%] [G loss: 3.529840]\n",
            "******* 662 [D loss: 0.243965, acc: 93.75%] [G loss: 2.919950]\n",
            "******* 663 [D loss: 0.259229, acc: 91.41%] [G loss: 3.225781]\n",
            "******* 664 [D loss: 0.229701, acc: 93.75%] [G loss: 4.304124]\n",
            "******* 665 [D loss: 0.219642, acc: 92.97%] [G loss: 3.497940]\n",
            "******* 666 [D loss: 0.206943, acc: 95.31%] [G loss: 3.823308]\n",
            "******* 667 [D loss: 0.209975, acc: 94.53%] [G loss: 4.129183]\n",
            "******* 668 [D loss: 0.127833, acc: 96.88%] [G loss: 4.041013]\n",
            "******* 669 [D loss: 0.242655, acc: 92.97%] [G loss: 2.657019]\n",
            "******* 670 [D loss: 0.090455, acc: 98.44%] [G loss: 3.863141]\n",
            "******* 671 [D loss: 0.141928, acc: 97.66%] [G loss: 3.793527]\n",
            "******* 672 [D loss: 0.152761, acc: 96.09%] [G loss: 3.707561]\n",
            "******* 673 [D loss: 0.154935, acc: 96.09%] [G loss: 2.798462]\n",
            "******* 674 [D loss: 0.234779, acc: 91.41%] [G loss: 3.050139]\n",
            "******* 675 [D loss: 0.128666, acc: 96.88%] [G loss: 4.141310]\n",
            "******* 676 [D loss: 0.060442, acc: 98.44%] [G loss: 4.576337]\n",
            "******* 677 [D loss: 0.186483, acc: 92.19%] [G loss: 3.099424]\n",
            "******* 678 [D loss: 0.141223, acc: 96.09%] [G loss: 3.490876]\n",
            "******* 679 [D loss: 0.123689, acc: 98.44%] [G loss: 4.457063]\n",
            "******* 680 [D loss: 0.103187, acc: 96.09%] [G loss: 4.348041]\n",
            "******* 681 [D loss: 0.144967, acc: 94.53%] [G loss: 3.786833]\n",
            "******* 682 [D loss: 0.138150, acc: 97.66%] [G loss: 3.948657]\n",
            "******* 683 [D loss: 0.171656, acc: 94.53%] [G loss: 2.986926]\n",
            "******* 684 [D loss: 0.100306, acc: 98.44%] [G loss: 4.292954]\n",
            "******* 685 [D loss: 0.059729, acc: 97.66%] [G loss: 5.216756]\n",
            "******* 686 [D loss: 0.136807, acc: 94.53%] [G loss: 3.181046]\n",
            "******* 687 [D loss: 0.191545, acc: 90.62%] [G loss: 3.306861]\n",
            "******* 688 [D loss: 0.053386, acc: 100.00%] [G loss: 4.379621]\n",
            "******* 689 [D loss: 0.105711, acc: 96.88%] [G loss: 4.199735]\n",
            "******* 690 [D loss: 0.136825, acc: 96.88%] [G loss: 3.000003]\n",
            "******* 691 [D loss: 0.188867, acc: 90.62%] [G loss: 4.223307]\n",
            "******* 692 [D loss: 0.133561, acc: 95.31%] [G loss: 4.784588]\n",
            "******* 693 [D loss: 0.188355, acc: 93.75%] [G loss: 3.455616]\n",
            "******* 694 [D loss: 0.096013, acc: 97.66%] [G loss: 3.813093]\n",
            "******* 695 [D loss: 0.118297, acc: 97.66%] [G loss: 4.442993]\n",
            "******* 696 [D loss: 0.129354, acc: 96.09%] [G loss: 3.508093]\n",
            "******* 697 [D loss: 0.147548, acc: 95.31%] [G loss: 3.510231]\n",
            "******* 698 [D loss: 0.107387, acc: 97.66%] [G loss: 4.022636]\n",
            "******* 699 [D loss: 0.082805, acc: 98.44%] [G loss: 4.142114]\n",
            "******* 700 [D loss: 0.117078, acc: 96.09%] [G loss: 3.245669]\n",
            "0.00000018\n",
            "saved\n",
            "******* 701 [D loss: 0.256241, acc: 87.50%] [G loss: 3.245879]\n",
            "******* 702 [D loss: 0.143343, acc: 96.88%] [G loss: 4.398861]\n",
            "******* 703 [D loss: 0.109556, acc: 96.09%] [G loss: 3.987847]\n",
            "******* 704 [D loss: 0.299606, acc: 91.41%] [G loss: 2.064166]\n",
            "******* 705 [D loss: 0.183565, acc: 92.19%] [G loss: 3.862914]\n",
            "******* 706 [D loss: 0.058527, acc: 97.66%] [G loss: 6.109701]\n",
            "******* 707 [D loss: 0.538826, acc: 82.03%] [G loss: 1.539806]\n",
            "******* 708 [D loss: 0.781989, acc: 57.81%] [G loss: 2.597073]\n",
            "******* 709 [D loss: 0.049495, acc: 98.44%] [G loss: 7.660360]\n",
            "******* 710 [D loss: 0.773181, acc: 75.00%] [G loss: 2.931749]\n",
            "******* 711 [D loss: 0.539512, acc: 68.75%] [G loss: 1.764819]\n",
            "******* 712 [D loss: 0.123912, acc: 95.31%] [G loss: 4.171480]\n",
            "******* 713 [D loss: 0.050334, acc: 98.44%] [G loss: 6.966633]\n",
            "******* 714 [D loss: 0.262532, acc: 90.62%] [G loss: 5.575568]\n",
            "******* 715 [D loss: 0.227574, acc: 93.75%] [G loss: 2.988441]\n",
            "******* 716 [D loss: 0.184743, acc: 93.75%] [G loss: 2.936607]\n",
            "******* 717 [D loss: 0.199107, acc: 89.06%] [G loss: 3.278357]\n",
            "******* 718 [D loss: 0.045163, acc: 100.00%] [G loss: 5.343540]\n",
            "******* 719 [D loss: 0.167261, acc: 94.53%] [G loss: 4.690686]\n",
            "******* 720 [D loss: 0.151416, acc: 95.31%] [G loss: 3.360388]\n",
            "******* 721 [D loss: 0.281793, acc: 90.62%] [G loss: 2.602226]\n",
            "******* 722 [D loss: 0.122244, acc: 97.66%] [G loss: 3.818090]\n",
            "******* 723 [D loss: 0.139612, acc: 96.09%] [G loss: 4.459016]\n",
            "******* 724 [D loss: 0.311219, acc: 91.41%] [G loss: 2.861437]\n",
            "******* 725 [D loss: 0.243508, acc: 89.84%] [G loss: 2.728455]\n",
            "******* 726 [D loss: 0.094379, acc: 98.44%] [G loss: 4.022585]\n",
            "******* 727 [D loss: 0.242859, acc: 92.97%] [G loss: 2.993452]\n",
            "******* 728 [D loss: 0.236653, acc: 91.41%] [G loss: 2.821182]\n",
            "******* 729 [D loss: 0.129807, acc: 97.66%] [G loss: 3.643897]\n",
            "******* 730 [D loss: 0.177058, acc: 94.53%] [G loss: 3.500150]\n",
            "******* 731 [D loss: 0.313532, acc: 89.84%] [G loss: 2.738730]\n",
            "******* 732 [D loss: 0.337636, acc: 85.16%] [G loss: 3.190174]\n",
            "******* 733 [D loss: 0.071535, acc: 99.22%] [G loss: 4.715069]\n",
            "******* 734 [D loss: 0.347577, acc: 89.84%] [G loss: 3.302517]\n",
            "******* 735 [D loss: 0.353156, acc: 90.62%] [G loss: 2.487519]\n",
            "******* 736 [D loss: 0.227103, acc: 92.19%] [G loss: 3.329735]\n",
            "******* 737 [D loss: 0.226714, acc: 92.19%] [G loss: 3.871060]\n",
            "******* 738 [D loss: 0.291824, acc: 92.19%] [G loss: 3.144613]\n",
            "******* 739 [D loss: 0.315623, acc: 85.16%] [G loss: 2.402799]\n",
            "******* 740 [D loss: 0.184990, acc: 95.31%] [G loss: 3.581444]\n",
            "******* 741 [D loss: 0.168029, acc: 96.09%] [G loss: 3.992038]\n",
            "******* 742 [D loss: 0.174802, acc: 95.31%] [G loss: 3.396574]\n",
            "******* 743 [D loss: 0.218788, acc: 94.53%] [G loss: 2.799652]\n",
            "******* 744 [D loss: 0.201965, acc: 93.75%] [G loss: 2.857670]\n",
            "******* 745 [D loss: 0.172455, acc: 96.09%] [G loss: 3.543581]\n",
            "******* 746 [D loss: 0.163747, acc: 95.31%] [G loss: 3.637807]\n",
            "******* 747 [D loss: 0.311242, acc: 90.62%] [G loss: 2.653251]\n",
            "******* 748 [D loss: 0.290304, acc: 87.50%] [G loss: 3.121736]\n",
            "******* 749 [D loss: 0.098770, acc: 96.88%] [G loss: 4.324488]\n",
            "******* 750 [D loss: 0.332004, acc: 85.94%] [G loss: 2.468825]\n",
            "******* 751 [D loss: 0.265452, acc: 89.06%] [G loss: 2.739221]\n",
            "******* 752 [D loss: 0.118729, acc: 97.66%] [G loss: 4.146948]\n",
            "******* 753 [D loss: 0.148012, acc: 96.09%] [G loss: 4.570122]\n",
            "******* 754 [D loss: 0.201048, acc: 93.75%] [G loss: 2.687226]\n",
            "******* 755 [D loss: 0.208145, acc: 94.53%] [G loss: 2.768952]\n",
            "******* 756 [D loss: 0.152156, acc: 95.31%] [G loss: 3.786234]\n",
            "******* 757 [D loss: 0.150276, acc: 94.53%] [G loss: 3.644323]\n",
            "******* 758 [D loss: 0.368429, acc: 86.72%] [G loss: 2.310444]\n",
            "******* 759 [D loss: 0.286187, acc: 85.16%] [G loss: 3.228955]\n",
            "******* 760 [D loss: 0.076212, acc: 98.44%] [G loss: 4.921267]\n",
            "******* 761 [D loss: 0.411413, acc: 87.50%] [G loss: 2.898989]\n",
            "******* 762 [D loss: 0.440716, acc: 75.78%] [G loss: 2.391207]\n",
            "******* 763 [D loss: 0.171434, acc: 95.31%] [G loss: 4.285655]\n",
            "******* 764 [D loss: 0.179511, acc: 93.75%] [G loss: 4.187489]\n",
            "******* 765 [D loss: 0.289448, acc: 91.41%] [G loss: 2.382165]\n",
            "******* 766 [D loss: 0.436598, acc: 75.00%] [G loss: 2.481077]\n",
            "******* 767 [D loss: 0.122338, acc: 97.66%] [G loss: 4.556118]\n",
            "******* 768 [D loss: 0.253739, acc: 89.06%] [G loss: 3.963205]\n",
            "******* 769 [D loss: 0.209171, acc: 92.19%] [G loss: 2.627193]\n",
            "******* 770 [D loss: 0.235965, acc: 90.62%] [G loss: 2.877633]\n",
            "******* 771 [D loss: 0.147208, acc: 96.09%] [G loss: 3.867093]\n",
            "******* 772 [D loss: 0.159727, acc: 93.75%] [G loss: 4.158067]\n",
            "******* 773 [D loss: 0.394679, acc: 87.50%] [G loss: 2.090219]\n",
            "******* 774 [D loss: 0.301763, acc: 82.03%] [G loss: 2.742579]\n",
            "******* 775 [D loss: 0.173084, acc: 96.88%] [G loss: 4.285534]\n",
            "******* 776 [D loss: 0.302644, acc: 87.50%] [G loss: 2.965219]\n",
            "******* 777 [D loss: 0.209064, acc: 94.53%] [G loss: 2.930543]\n",
            "******* 778 [D loss: 0.127706, acc: 97.66%] [G loss: 3.331828]\n",
            "******* 779 [D loss: 0.285004, acc: 89.84%] [G loss: 3.119541]\n",
            "******* 780 [D loss: 0.270165, acc: 90.62%] [G loss: 2.707505]\n",
            "******* 781 [D loss: 0.132933, acc: 96.09%] [G loss: 3.540949]\n",
            "******* 782 [D loss: 0.121334, acc: 95.31%] [G loss: 4.560867]\n",
            "******* 783 [D loss: 0.308968, acc: 89.84%] [G loss: 2.636031]\n",
            "******* 784 [D loss: 0.234513, acc: 90.62%] [G loss: 2.815964]\n",
            "******* 785 [D loss: 0.142020, acc: 97.66%] [G loss: 4.242348]\n",
            "******* 786 [D loss: 0.343509, acc: 88.28%] [G loss: 3.035180]\n",
            "******* 787 [D loss: 0.285214, acc: 88.28%] [G loss: 2.372725]\n",
            "******* 788 [D loss: 0.340576, acc: 85.16%] [G loss: 3.015912]\n",
            "******* 789 [D loss: 0.110725, acc: 97.66%] [G loss: 4.619559]\n",
            "******* 790 [D loss: 0.497208, acc: 83.59%] [G loss: 2.310660]\n",
            "******* 791 [D loss: 0.363090, acc: 78.91%] [G loss: 2.279748]\n",
            "******* 792 [D loss: 0.145063, acc: 96.88%] [G loss: 3.960654]\n",
            "******* 793 [D loss: 0.353886, acc: 91.41%] [G loss: 3.740808]\n",
            "******* 794 [D loss: 0.198365, acc: 92.19%] [G loss: 2.624066]\n",
            "******* 795 [D loss: 0.226952, acc: 91.41%] [G loss: 2.718995]\n",
            "******* 796 [D loss: 0.162405, acc: 95.31%] [G loss: 3.597367]\n",
            "******* 797 [D loss: 0.158262, acc: 93.75%] [G loss: 3.771615]\n",
            "******* 798 [D loss: 0.144790, acc: 94.53%] [G loss: 3.339513]\n",
            "******* 799 [D loss: 0.174285, acc: 96.88%] [G loss: 3.056759]\n",
            "******* 800 [D loss: 0.237158, acc: 95.31%] [G loss: 3.275435]\n",
            "0.00000019\n",
            "saved\n",
            "******* 801 [D loss: 0.228034, acc: 94.53%] [G loss: 3.334040]\n",
            "******* 802 [D loss: 0.218500, acc: 90.62%] [G loss: 2.922503]\n",
            "******* 803 [D loss: 0.303756, acc: 90.62%] [G loss: 2.649981]\n",
            "******* 804 [D loss: 0.256292, acc: 90.62%] [G loss: 2.903656]\n",
            "******* 805 [D loss: 0.155205, acc: 96.09%] [G loss: 3.373795]\n",
            "******* 806 [D loss: 0.216282, acc: 89.84%] [G loss: 3.274431]\n",
            "******* 807 [D loss: 0.188077, acc: 96.09%] [G loss: 3.034662]\n",
            "******* 808 [D loss: 0.364788, acc: 82.03%] [G loss: 2.717120]\n",
            "******* 809 [D loss: 0.195272, acc: 93.75%] [G loss: 3.712315]\n",
            "******* 810 [D loss: 0.171229, acc: 95.31%] [G loss: 4.027826]\n",
            "******* 811 [D loss: 0.325611, acc: 86.72%] [G loss: 2.210053]\n",
            "******* 812 [D loss: 0.270252, acc: 88.28%] [G loss: 2.973261]\n",
            "******* 813 [D loss: 0.236296, acc: 91.41%] [G loss: 3.403491]\n",
            "******* 814 [D loss: 0.229451, acc: 91.41%] [G loss: 2.874507]\n",
            "******* 815 [D loss: 0.207016, acc: 93.75%] [G loss: 3.469208]\n",
            "******* 816 [D loss: 0.116623, acc: 96.88%] [G loss: 3.721686]\n",
            "******* 817 [D loss: 0.338218, acc: 86.72%] [G loss: 2.521634]\n",
            "******* 818 [D loss: 0.237763, acc: 89.06%] [G loss: 2.522606]\n",
            "******* 819 [D loss: 0.188732, acc: 94.53%] [G loss: 3.627511]\n",
            "******* 820 [D loss: 0.185702, acc: 92.97%] [G loss: 3.658446]\n",
            "******* 821 [D loss: 0.108600, acc: 96.88%] [G loss: 3.276124]\n",
            "******* 822 [D loss: 0.175233, acc: 96.09%] [G loss: 2.732784]\n",
            "******* 823 [D loss: 0.242754, acc: 92.97%] [G loss: 3.010565]\n",
            "******* 824 [D loss: 0.182713, acc: 95.31%] [G loss: 3.962757]\n",
            "******* 825 [D loss: 0.297968, acc: 91.41%] [G loss: 3.421077]\n",
            "******* 826 [D loss: 0.244528, acc: 92.19%] [G loss: 2.730154]\n",
            "******* 827 [D loss: 0.218783, acc: 95.31%] [G loss: 3.324986]\n",
            "******* 828 [D loss: 0.134251, acc: 97.66%] [G loss: 3.693733]\n",
            "******* 829 [D loss: 0.216876, acc: 92.19%] [G loss: 2.994274]\n",
            "******* 830 [D loss: 0.283414, acc: 88.28%] [G loss: 2.790592]\n",
            "******* 831 [D loss: 0.242936, acc: 94.53%] [G loss: 3.051540]\n",
            "******* 832 [D loss: 0.121494, acc: 96.88%] [G loss: 3.597240]\n",
            "******* 833 [D loss: 0.231475, acc: 89.06%] [G loss: 2.985499]\n",
            "******* 834 [D loss: 0.202829, acc: 93.75%] [G loss: 3.053603]\n",
            "******* 835 [D loss: 0.126757, acc: 97.66%] [G loss: 3.795660]\n",
            "******* 836 [D loss: 0.286806, acc: 90.62%] [G loss: 3.224710]\n",
            "******* 837 [D loss: 0.179170, acc: 96.09%] [G loss: 3.085547]\n",
            "******* 838 [D loss: 0.120293, acc: 97.66%] [G loss: 3.931612]\n",
            "******* 839 [D loss: 0.286934, acc: 90.62%] [G loss: 2.937568]\n",
            "******* 840 [D loss: 0.309577, acc: 89.84%] [G loss: 2.523646]\n",
            "******* 841 [D loss: 0.169027, acc: 97.66%] [G loss: 3.338901]\n",
            "******* 842 [D loss: 0.158431, acc: 96.88%] [G loss: 3.761727]\n",
            "******* 843 [D loss: 0.251086, acc: 92.97%] [G loss: 3.163400]\n",
            "******* 844 [D loss: 0.246887, acc: 93.75%] [G loss: 3.006267]\n",
            "******* 845 [D loss: 0.178092, acc: 94.53%] [G loss: 3.209515]\n",
            "******* 846 [D loss: 0.145347, acc: 96.09%] [G loss: 3.566777]\n",
            "******* 847 [D loss: 0.211593, acc: 93.75%] [G loss: 2.769582]\n",
            "******* 848 [D loss: 0.250430, acc: 92.97%] [G loss: 2.650089]\n",
            "******* 849 [D loss: 0.187132, acc: 94.53%] [G loss: 3.285210]\n",
            "******* 850 [D loss: 0.117909, acc: 98.44%] [G loss: 3.555027]\n",
            "******* 851 [D loss: 0.161692, acc: 96.09%] [G loss: 3.212115]\n",
            "******* 852 [D loss: 0.324614, acc: 87.50%] [G loss: 2.652115]\n",
            "******* 853 [D loss: 0.242797, acc: 92.19%] [G loss: 3.361541]\n",
            "******* 854 [D loss: 0.233226, acc: 92.97%] [G loss: 2.960674]\n",
            "******* 855 [D loss: 0.283907, acc: 89.84%] [G loss: 2.400381]\n",
            "******* 856 [D loss: 0.198801, acc: 90.62%] [G loss: 3.325706]\n",
            "******* 857 [D loss: 0.097361, acc: 98.44%] [G loss: 4.241765]\n",
            "******* 858 [D loss: 0.317780, acc: 89.84%] [G loss: 2.988950]\n",
            "******* 859 [D loss: 0.410501, acc: 79.69%] [G loss: 2.091682]\n",
            "******* 860 [D loss: 0.251973, acc: 92.97%] [G loss: 3.142873]\n",
            "******* 861 [D loss: 0.153699, acc: 96.09%] [G loss: 4.155059]\n",
            "******* 862 [D loss: 0.241755, acc: 91.41%] [G loss: 3.439992]\n",
            "******* 863 [D loss: 0.241801, acc: 93.75%] [G loss: 2.580787]\n",
            "******* 864 [D loss: 0.244837, acc: 91.41%] [G loss: 3.252002]\n",
            "******* 865 [D loss: 0.086630, acc: 98.44%] [G loss: 4.347330]\n",
            "******* 866 [D loss: 0.264378, acc: 87.50%] [G loss: 3.155698]\n",
            "******* 867 [D loss: 0.217297, acc: 95.31%] [G loss: 2.520329]\n",
            "******* 868 [D loss: 0.223450, acc: 92.97%] [G loss: 3.083410]\n",
            "******* 869 [D loss: 0.198644, acc: 92.97%] [G loss: 3.920861]\n",
            "******* 870 [D loss: 0.194560, acc: 94.53%] [G loss: 3.929802]\n",
            "******* 871 [D loss: 0.382272, acc: 83.59%] [G loss: 2.263946]\n",
            "******* 872 [D loss: 0.341637, acc: 84.38%] [G loss: 2.563257]\n",
            "******* 873 [D loss: 0.095075, acc: 98.44%] [G loss: 4.242476]\n",
            "******* 874 [D loss: 0.254070, acc: 92.19%] [G loss: 3.755653]\n",
            "******* 875 [D loss: 0.199378, acc: 95.31%] [G loss: 2.680315]\n",
            "******* 876 [D loss: 0.244135, acc: 91.41%] [G loss: 2.679288]\n",
            "******* 877 [D loss: 0.115198, acc: 97.66%] [G loss: 3.464039]\n",
            "******* 878 [D loss: 0.149217, acc: 94.53%] [G loss: 3.631036]\n",
            "******* 879 [D loss: 0.285654, acc: 90.62%] [G loss: 2.627067]\n",
            "******* 880 [D loss: 0.197760, acc: 96.88%] [G loss: 2.749250]\n",
            "******* 881 [D loss: 0.060868, acc: 99.22%] [G loss: 4.329830]\n",
            "******* 882 [D loss: 0.217577, acc: 94.53%] [G loss: 3.307142]\n",
            "******* 883 [D loss: 0.307696, acc: 89.06%] [G loss: 2.300498]\n",
            "******* 884 [D loss: 0.286467, acc: 89.06%] [G loss: 2.519716]\n",
            "******* 885 [D loss: 0.107617, acc: 96.88%] [G loss: 3.758937]\n",
            "******* 886 [D loss: 0.305641, acc: 89.84%] [G loss: 3.222127]\n",
            "******* 887 [D loss: 0.185302, acc: 97.66%] [G loss: 2.694445]\n",
            "******* 888 [D loss: 0.232471, acc: 92.19%] [G loss: 2.783876]\n",
            "******* 889 [D loss: 0.256633, acc: 91.41%] [G loss: 3.216411]\n",
            "******* 890 [D loss: 0.207543, acc: 92.19%] [G loss: 3.154475]\n",
            "******* 891 [D loss: 0.320321, acc: 92.19%] [G loss: 2.980042]\n",
            "******* 892 [D loss: 0.149187, acc: 97.66%] [G loss: 3.154710]\n",
            "******* 893 [D loss: 0.187294, acc: 96.09%] [G loss: 3.147303]\n",
            "******* 894 [D loss: 0.219299, acc: 93.75%] [G loss: 3.041649]\n",
            "******* 895 [D loss: 0.338437, acc: 85.16%] [G loss: 2.453893]\n",
            "******* 896 [D loss: 0.179086, acc: 96.09%] [G loss: 3.205233]\n",
            "******* 897 [D loss: 0.159228, acc: 96.88%] [G loss: 3.268651]\n",
            "******* 898 [D loss: 0.253700, acc: 93.75%] [G loss: 2.802863]\n",
            "******* 899 [D loss: 0.196708, acc: 94.53%] [G loss: 2.810223]\n",
            "******* 900 [D loss: 0.193957, acc: 95.31%] [G loss: 3.131044]\n",
            "0.00000020\n",
            "saved\n",
            "******* 901 [D loss: 0.140029, acc: 97.66%] [G loss: 3.656699]\n",
            "******* 902 [D loss: 0.194503, acc: 96.09%] [G loss: 3.253729]\n",
            "******* 903 [D loss: 0.224486, acc: 92.97%] [G loss: 2.636538]\n",
            "******* 904 [D loss: 0.255764, acc: 92.97%] [G loss: 3.032423]\n",
            "******* 905 [D loss: 0.137860, acc: 96.88%] [G loss: 3.708622]\n",
            "******* 906 [D loss: 0.268863, acc: 88.28%] [G loss: 3.057502]\n",
            "******* 907 [D loss: 0.226743, acc: 94.53%] [G loss: 2.685220]\n",
            "******* 908 [D loss: 0.153465, acc: 96.88%] [G loss: 3.340102]\n",
            "******* 909 [D loss: 0.226378, acc: 92.19%] [G loss: 2.977834]\n",
            "******* 910 [D loss: 0.111359, acc: 99.22%] [G loss: 2.672660]\n",
            "******* 911 [D loss: 0.167592, acc: 96.09%] [G loss: 3.310507]\n",
            "******* 912 [D loss: 0.173298, acc: 96.09%] [G loss: 3.176206]\n",
            "******* 913 [D loss: 0.241381, acc: 89.84%] [G loss: 2.847354]\n",
            "******* 914 [D loss: 0.241929, acc: 92.19%] [G loss: 2.718151]\n",
            "******* 915 [D loss: 0.266639, acc: 89.84%] [G loss: 3.548112]\n",
            "******* 916 [D loss: 0.240083, acc: 90.62%] [G loss: 3.264469]\n",
            "******* 917 [D loss: 0.243650, acc: 89.84%] [G loss: 2.515648]\n",
            "******* 918 [D loss: 0.248789, acc: 92.97%] [G loss: 2.833761]\n",
            "******* 919 [D loss: 0.256098, acc: 89.84%] [G loss: 2.925604]\n",
            "******* 920 [D loss: 0.176005, acc: 92.97%] [G loss: 3.470799]\n",
            "******* 921 [D loss: 0.148872, acc: 96.88%] [G loss: 3.453724]\n",
            "******* 922 [D loss: 0.175633, acc: 96.09%] [G loss: 3.277854]\n",
            "******* 923 [D loss: 0.224005, acc: 90.62%] [G loss: 3.065800]\n",
            "******* 924 [D loss: 0.155514, acc: 96.09%] [G loss: 2.981040]\n",
            "******* 925 [D loss: 0.133370, acc: 97.66%] [G loss: 3.586317]\n",
            "******* 926 [D loss: 0.302457, acc: 89.84%] [G loss: 2.783555]\n",
            "******* 927 [D loss: 0.255764, acc: 89.84%] [G loss: 2.629328]\n",
            "******* 928 [D loss: 0.347962, acc: 87.50%] [G loss: 3.048801]\n",
            "******* 929 [D loss: 0.260227, acc: 92.19%] [G loss: 3.026711]\n",
            "******* 930 [D loss: 0.207350, acc: 93.75%] [G loss: 2.594748]\n",
            "******* 931 [D loss: 0.213227, acc: 91.41%] [G loss: 2.939701]\n",
            "******* 932 [D loss: 0.233692, acc: 90.62%] [G loss: 3.002820]\n",
            "******* 933 [D loss: 0.356938, acc: 89.84%] [G loss: 2.757588]\n",
            "******* 934 [D loss: 0.240256, acc: 93.75%] [G loss: 2.694120]\n",
            "******* 935 [D loss: 0.250717, acc: 90.62%] [G loss: 2.530894]\n",
            "******* 936 [D loss: 0.205405, acc: 94.53%] [G loss: 3.147004]\n",
            "******* 937 [D loss: 0.268837, acc: 90.62%] [G loss: 2.832555]\n",
            "******* 938 [D loss: 0.269940, acc: 91.41%] [G loss: 2.704020]\n",
            "******* 939 [D loss: 0.181811, acc: 95.31%] [G loss: 3.027947]\n",
            "******* 940 [D loss: 0.275493, acc: 91.41%] [G loss: 3.434563]\n",
            "******* 941 [D loss: 0.226213, acc: 92.19%] [G loss: 3.087207]\n",
            "******* 942 [D loss: 0.410248, acc: 82.03%] [G loss: 2.633403]\n",
            "******* 943 [D loss: 0.124567, acc: 99.22%] [G loss: 3.352615]\n",
            "******* 944 [D loss: 0.220704, acc: 94.53%] [G loss: 3.096648]\n",
            "******* 945 [D loss: 0.338636, acc: 92.19%] [G loss: 2.011146]\n",
            "******* 946 [D loss: 0.319950, acc: 85.16%] [G loss: 2.688254]\n",
            "******* 947 [D loss: 0.123053, acc: 98.44%] [G loss: 3.733384]\n",
            "******* 948 [D loss: 0.349168, acc: 88.28%] [G loss: 2.800391]\n",
            "******* 949 [D loss: 0.377202, acc: 81.25%] [G loss: 2.627768]\n",
            "******* 950 [D loss: 0.214416, acc: 95.31%] [G loss: 2.776918]\n",
            "******* 951 [D loss: 0.329958, acc: 89.84%] [G loss: 2.663395]\n",
            "******* 952 [D loss: 0.278995, acc: 89.06%] [G loss: 2.524138]\n",
            "******* 953 [D loss: 0.198703, acc: 96.09%] [G loss: 3.132289]\n",
            "******* 954 [D loss: 0.205018, acc: 92.97%] [G loss: 2.843019]\n",
            "******* 955 [D loss: 0.261396, acc: 90.62%] [G loss: 2.751102]\n",
            "******* 956 [D loss: 0.292131, acc: 90.62%] [G loss: 2.487096]\n",
            "******* 957 [D loss: 0.385447, acc: 88.28%] [G loss: 2.715741]\n",
            "******* 958 [D loss: 0.266468, acc: 91.41%] [G loss: 2.631951]\n",
            "******* 959 [D loss: 0.278960, acc: 90.62%] [G loss: 2.674873]\n",
            "******* 960 [D loss: 0.254611, acc: 93.75%] [G loss: 2.785141]\n",
            "******* 961 [D loss: 0.170166, acc: 94.53%] [G loss: 2.932354]\n",
            "******* 962 [D loss: 0.132216, acc: 95.31%] [G loss: 2.954899]\n",
            "******* 963 [D loss: 0.327821, acc: 93.75%] [G loss: 2.397804]\n",
            "******* 964 [D loss: 0.263990, acc: 92.19%] [G loss: 2.679782]\n",
            "******* 965 [D loss: 0.173952, acc: 96.88%] [G loss: 3.371312]\n",
            "******* 966 [D loss: 0.181572, acc: 96.09%] [G loss: 2.935740]\n",
            "******* 967 [D loss: 0.436857, acc: 82.03%] [G loss: 2.051167]\n",
            "******* 968 [D loss: 0.253571, acc: 92.19%] [G loss: 3.008468]\n",
            "******* 969 [D loss: 0.474115, acc: 85.94%] [G loss: 2.971427]\n",
            "******* 970 [D loss: 0.433407, acc: 89.06%] [G loss: 2.822188]\n",
            "******* 971 [D loss: 0.392920, acc: 85.94%] [G loss: 2.048404]\n",
            "******* 972 [D loss: 0.159152, acc: 97.66%] [G loss: 2.810242]\n",
            "******* 973 [D loss: 0.217205, acc: 92.97%] [G loss: 3.297320]\n",
            "******* 974 [D loss: 0.304739, acc: 90.62%] [G loss: 2.477499]\n",
            "******* 975 [D loss: 0.322298, acc: 89.06%] [G loss: 2.855250]\n",
            "******* 976 [D loss: 0.181766, acc: 93.75%] [G loss: 3.411136]\n",
            "******* 977 [D loss: 0.251030, acc: 89.84%] [G loss: 2.764502]\n",
            "******* 978 [D loss: 0.283297, acc: 91.41%] [G loss: 2.205441]\n",
            "******* 979 [D loss: 0.292986, acc: 89.06%] [G loss: 2.874316]\n",
            "******* 980 [D loss: 0.190163, acc: 92.97%] [G loss: 3.300969]\n",
            "******* 981 [D loss: 0.349858, acc: 89.06%] [G loss: 2.240144]\n",
            "******* 982 [D loss: 0.314021, acc: 85.16%] [G loss: 2.518743]\n",
            "******* 983 [D loss: 0.257389, acc: 93.75%] [G loss: 3.192053]\n",
            "******* 984 [D loss: 0.253167, acc: 92.97%] [G loss: 2.960649]\n",
            "******* 985 [D loss: 0.349763, acc: 87.50%] [G loss: 2.355983]\n",
            "******* 986 [D loss: 0.223948, acc: 90.62%] [G loss: 2.827141]\n",
            "******* 987 [D loss: 0.184302, acc: 94.53%] [G loss: 3.200437]\n",
            "******* 988 [D loss: 0.256460, acc: 90.62%] [G loss: 2.433471]\n",
            "******* 989 [D loss: 0.292025, acc: 92.97%] [G loss: 2.210475]\n",
            "******* 990 [D loss: 0.209761, acc: 92.97%] [G loss: 2.733139]\n",
            "******* 991 [D loss: 0.371072, acc: 85.16%] [G loss: 2.738703]\n",
            "******* 992 [D loss: 0.300524, acc: 89.84%] [G loss: 2.808620]\n",
            "******* 993 [D loss: 0.205058, acc: 93.75%] [G loss: 3.074234]\n",
            "******* 994 [D loss: 0.353087, acc: 88.28%] [G loss: 2.790548]\n",
            "******* 995 [D loss: 0.246768, acc: 93.75%] [G loss: 2.581908]\n",
            "******* 996 [D loss: 0.243050, acc: 92.97%] [G loss: 2.912315]\n",
            "******* 997 [D loss: 0.247570, acc: 92.97%] [G loss: 3.046666]\n",
            "******* 998 [D loss: 0.289144, acc: 89.06%] [G loss: 2.827820]\n",
            "******* 999 [D loss: 0.261944, acc: 91.41%] [G loss: 2.733837]\n"
          ]
        }
      ],
      "source": [
        "def train(epochs, batch_size=64, save_interval=200):\n",
        "  (X_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "  # print(X_train.shape)\n",
        "  #Rescale data between -1 and 1, currently and 3 dimensions (w,l,color)\n",
        "  X_train = X_train / 127.5 -1.\n",
        "  # X_train = np.expand_dims(X_train, axis=3)\n",
        "  # print(X_train.shape)\n",
        "\n",
        "  #Create our Y for our Neural Networks - # creating a loss\n",
        "  valid = np.ones((batch_size, 1))\n",
        "  fakes = np.zeros((batch_size, 1))\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    #Get Random Batch - initializing a seed\n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    imgs = X_train[idx]\n",
        "\n",
        "    #Generate Fake Images\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    #Train discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fakes)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim)) # reset noise\n",
        "    \n",
        "    #inverse y label\n",
        "    g_loss = GAN.train_on_batch(noise, valid)\n",
        "\n",
        "    print(\"******* %d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100* d_loss[1], g_loss))\n",
        "\n",
        "    if(epoch % save_interval) == 0:\n",
        "      save_imgs(epoch)\n",
        "\n",
        "  # print(valid)\n",
        "\n",
        "\n",
        "train(1000, batch_size=64, save_interval=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po-jSQoN1Azl"
      },
      "source": [
        "### **8) Making GIF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XPShgQpg1EMy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/fm/m7sbfrtn37dcz2lvxhvj6q0m0000gn/T/ipykernel_10926/1295497862.py:11: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  image = imageio.imread(filename)\n",
            "/var/folders/fm/m7sbfrtn37dcz2lvxhvj6q0m0000gn/T/ipykernel_10926/1295497862.py:13: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  image = imageio.imread(filename)\n"
          ]
        }
      ],
      "source": [
        "# Display a single image using the epoch number\n",
        "# def display_image(epoch_no):\n",
        "#   return PIL.Image.open('generated_images/%.8f.png'.format(epoch_no))\n",
        "\n",
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('generated_images/*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh37uv1torG5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "NumberGAN_Solution.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('lhl_env38')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "93c9cabeb8165e6a1575ace97e023eeebe73f88984ce88042818ce2a73501ce8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
