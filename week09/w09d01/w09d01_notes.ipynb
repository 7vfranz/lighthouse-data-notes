{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Lecture\n",
    "#### Time Series\n",
    "* data, changing over time \n",
    "* can have one number (e.g. stock) - or multi-dimensional (e.g. video)\n",
    "* data is ordered in some way - doesn't have to be 'time'\n",
    "* implement 'moving average' - for data preprocessing etc.\n",
    "* components:\n",
    "    * level, trend, seasonality, residual\n",
    "* stationarity\n",
    "* models: ARIMA (auto-regressive integrated moving average), Box-Jenkins \n",
    "    * though eventually replaced with recurrent neural network \n",
    "\n",
    "Ordered Data Points\n",
    "* X axis - is ordered (e.g. time) \n",
    "* reprecussion:\n",
    "    * linear regression - can do \n",
    "    * but time doesn't really have effect on y\n",
    "    * prediction data never overlap with previous - future time points never sampled\n",
    "* can use features from binning - pass few days or what's happening recently - can't be done in non-ordered data\n",
    "* can use dependent variable from the past as an independent variable \n",
    "* IV is a DV\n",
    "* get features as y(t), x(t-1), x(t-2) - similar to a recurrent NN \n",
    "* get mean, fourier transform etc - any computation from past values can be used as a feature\n",
    "* creating lag columns can be done in one line \n",
    "* can do a auto-correlation plot to see boundaries\n",
    "\n",
    "Analysis and Forecasting\n",
    "* predict the future from the past \n",
    "* auto-correlate - over how long does data correlate with itself \n",
    "* trend: how does it change over time\n",
    "* seasonality - timescales of repeating patterns\n",
    "* any random over time \n",
    "\n",
    "Moving Average\n",
    "* average from a temporal window size - e.g. a week, slide window \n",
    "* can be trailing (end) or centre \n",
    "* centre is sometimes more accurate - but trailing is more practical \n",
    "* independent var or x becomes the centre or the trail\n",
    "* smoothing/denoising - better see the trends by averaging out the noise\n",
    "* window size - larger - lose more detail/signal, more smoothing\n",
    "* smaller - more detail, less smoothing\n",
    "* depends on question at hand e.g. day trading vs investing over years\n",
    "* moving average - can be done in one line through pandas \n",
    "* used for feature engineering\n",
    "* multiple scales can improve performance - (1 day prior, 3 days prior, 7 days)\n",
    "* Assumes: future will look like the past (but less noisy) \n",
    "* prediction - will look like it is slightly delayed, but can be used as a baseline\n",
    "\n",
    "Time Series Components\n",
    "* dissectable to different parts\n",
    "* baseline value (average), overall trend (increasing, decreasing over time)\n",
    "* seasonality - repeating patterns \n",
    "* residual - remaining - not captured by other components - not always noise, can be predictable\n",
    "* e.g. weather from the past few days \n",
    "* additive model: y(data) = level (average) + trend (moving average) + seasonality () + residual\n",
    "* y - trend = level + seasonality (averaging across years - on a day) + residual\n",
    "* seasonality - january (2000, 2001, 2002) averaged\n",
    "* multiplicative - multiplying instead of adding\n",
    "* multiplicative can be seen - as it increases, the waves get spikier and spikier - increasing over time\n",
    "* time series is either multiplicative or additive \n",
    "\n",
    "Stationarity\n",
    "* statistical properties are constant over time\n",
    "* e.g. mean, variance are constant over time\n",
    "* requires that there is no trend or seasonal component\n",
    "* most classical time series models assume stationarity (not NN, RNN)\n",
    "* simplifying assumption about the data - easier to predict\n",
    "* strats:\n",
    "    * check stationary\n",
    "    * remove trend and seasonal components\n",
    "* mean stays constant over time\n",
    "* check plot (adhoc)\n",
    "* constant statistics - histogram, mean, variance, different segments (adhoc)\n",
    "* hypothesis testing \n",
    "    * null - default belief - no difference \n",
    "    * p-value - prob of observing the data you did if the null hypothesis were true \n",
    "    * lower p-value more likely to reject the null hypothesis\n",
    "* Augmented Dickey-Fuller test\n",
    "    * test for stationarity\n",
    "    * input time series\n",
    "    * null = data is non-stationary (most data are non-stationary - moves)\n",
    "    * output p<0.05 = data is stationary - good to go\n",
    "    * non-sig = non-stationary \n",
    "        * use different model (RNN)\n",
    "        * massage data to be stationary\n",
    "        * can restrict data to only stationary parts\n",
    "        * decomposition - eliminate non-stationary then only do modeling on stationary\n",
    "        * differencing - subtracting lagged version of the series to remove trend, can do multiple times (differencing order) - to further flatten\n",
    "            * too much differencing can lead to artifacts\n",
    "            * data just becomes your differencing \n",
    "            * can recover the original domain - by inversing - add the previous value\n",
    "\n",
    "ARIMA\n",
    "* Auto-Regression (linear regression) Integrated (differencing to stationary) Moving Average (not really moving-average though) \n",
    "* autocorrelation - correlation with lag \n",
    "* shift time series by time lag - then correlate\n",
    "* auto-correlation plot (further away - more lag, less correlation)\n",
    "* 95% confident interval \n",
    "* partial autocorrelation\n",
    "    * lag intervals x correlation\n",
    "    * correlation y(t-2) -> y(t-1) -> y(t)\n",
    "    * partial only takes direct path from y(t-2) to y(t) \n",
    "    * autocorrelation takes into account both correlations from indirect and direct path\n",
    "* hyperparameter = d - # of times of differencing\n",
    "* d decided based on how non-stationary the data is\n",
    "* differencing - related to autocorrelation\n",
    "* ARIMA does differencing itself - based on d - fit to the original data \n",
    "* does autoregression - past features in the time series - fit a linear regression using \n",
    "    * hyperparameter = p\n",
    "    * how many past values are being used as features\n",
    "    * e.g. past two values, p=2 \n",
    "    * chosen based on partial autocorrelation\n",
    "    * find lag - where past values don't have any effect \n",
    "    * above significance - significant r score\n",
    "    * increasing p autocorrelation - no longer significant (less than 95% CI)\n",
    "    * no indirect - use partial correlation\n",
    "* moving average model (not same as previous moving average) (q)\n",
    "    * based on past prediction errors - will propagate future predictions \n",
    "    * weighted combination of past prediction errors\n",
    "    * non-significant cutoff on autocorrelation to pick q \n",
    "* grid search can be done with cross-validation \n",
    "* differencing (based on d) -> prediction based on autoregression  + moving average model \n",
    "\n",
    "Box-Jenkins Method\n",
    "* all ARIMA based just jargon...\n",
    "* iterative 3-step approach using ARIMA\n",
    "* basically - machine learning approach\n",
    "* identificiation - use data to select p,d,q ARIMA\n",
    "* Estimation - use data to train parameters (weights)\n",
    "    * fitting ARIMA on training data\n",
    "* Diagnostic - evluate model for overfitting\n",
    "    * evaluate on newer data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Compass\n",
    "* time series - order dependence on observations - time dimension\n",
    "* constraint and structure - providing additional information\n",
    "* time series analysis (describe the data) vs forecasting (making prediction)\n",
    "\n",
    "Components of Time Series\n",
    "* level\n",
    "    * baseline value if it were a straight line\n",
    "* trend\n",
    "    * the optional, often linear increasing or decreasing behaviour over time\n",
    "* seasonality\n",
    "    * optional repeating patterns or cycles over time\n",
    "* noise\n",
    "    * optional variability in observations - can't explained by model\n",
    "\n",
    "Concerns of Forecasting\n",
    "* depends on data available, time horizon (short/long), updated forecasts or remain static, frequency of forecasting required - more data if less frequency of forecasting\n",
    "* Cleaning/Scaling/Transforming\n",
    "    * frequency - data is too frequent - uneven spacing\n",
    "    * outliers \n",
    "    * missing \n",
    "\n",
    "Examples\n",
    "* stock price, birthrate each year, sales per day for a store, each quarter, each hour etc.\n",
    "\n",
    "------------------\n",
    "\n",
    "Hypothesis Testing - Review\n",
    "* hypothesis: idea that can be tested, usually about the relationship between two or more variables, or explanation for something \n",
    "* null - no difference\n",
    "* innocent until proven guilty\n",
    "* two tailed test != X\n",
    "* one tailed test - > or < X\n",
    "\n",
    "P-value\n",
    "* e.g. p of 0.18 means 18% chance of observing that mean/statistic or less from a sample of size n, and given variation IF null is true \n",
    "* smaller p value - less likely due to luck \n",
    "* start with null true, get data and compute statistic - calculate how likely it is to get that value given that null is true\n",
    "\n",
    "--------------------\n",
    "\n",
    "Box-Jenkins Method \n",
    "* Implementing the ARIMA model\n",
    "* AR - autoregression: dependent relationship between observation and some number of lagged observations (p)\n",
    "* I - integrated: use of differencing of raw observations - to flatten (subtracting observation from observation in previous time step) and make stationary (d)\n",
    "* MA - Moving Average: model that uses dependency between observation and residual from moving average model - applied to lagged observation (q)\n",
    "* each specified as parameter in the model\n",
    "    * p: number of lag observations in the model \n",
    "    * d: number of times the raw observations are differenced\n",
    "    * q: size of moving average window \n",
    "\n",
    "The Method\n",
    "* identification - data and all related information to select sub-class of model to summarize data - selecting p,d,q\n",
    "* estimation - use data to train the model \n",
    "* diagnostic - evaluating the mdoel\n",
    "\n",
    "Identification\n",
    "* assess if stationary\n",
    "    * if not how many differences to make it stationary\n",
    "* identify the parameters for ARIMA\n",
    "* Differencing\n",
    "    * unit root tests - testing if stationary or not\n",
    "    * avoid over differencing - too much differencing\n",
    "* Configuring AR and MA\n",
    "    * two diagnostic plots for choosing p and q\n",
    "    * Autocorrelation Function (ACF) - p - plots correlation of an observation with lag values (x - lag and y shows the correlation coefficient between negative and 1) - find where it no longer improves\n",
    "    * Partial autocorrelation (PACF) - plots correlation for an observation with lag values that is not accounted for by lagged observations\n",
    "* bar charts - with 95-99% confidence intervals as horizontal lines, crossing CI = significant\n",
    "\n",
    "Estimation\n",
    "* using numerical methods to minimize loss or error\n",
    "\n",
    "Diagnostic Checking\n",
    "* look for evidence of overfitting or residual errors\n",
    "* complex model, little data, parameter optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhl_env38",
   "language": "python",
   "name": "lhl_env38"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
