{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Market Basket Analysis (MBA)\n",
    "* find relationships between items \n",
    "* what items are sold together at a school (flour, sugar) (eggs) \n",
    "* basis for recommender engines\n",
    "* unsupervised learning\n",
    "* used for data mining, understanding purchasing behaviour\n",
    "* aka: Affinity Analysis/Cross-sell recommendations \n",
    "* typically uses point of sale transaction data/invoices (products purchased at a store)\n",
    "* can lead to large data set or many unique items(features)\n",
    "* output is a set of association rules: if item1, item2 then item3\n",
    "* antecedent (item 1,2) -> consequent (item 3)\n",
    "\n",
    "Measures of Association\n",
    "* Support\n",
    "    * percentage of transactions with a given item combo\n",
    "    * P(item combo) / all transactions\n",
    "    * if combo is rare can be dropped\n",
    "* Confidence\n",
    "    * condition probability\n",
    "    * P(buying item B| bought A) = Support (A,B) / support (B)\n",
    "    * directional\n",
    "* Lift\n",
    "    * items A and C are co-occuring in many transactions\n",
    "    * lift answers if based on chance or real association\n",
    "    * filtered by support and confidence \n",
    "    * Lift (A|B) = support(A,B) / (support(A) * support (B))\n",
    "    * not directional value of 1+\n",
    "    * 1 = no association\n",
    "    * denominator - how often items co-occur if they had NO relationship (A*B)\n",
    "\n",
    "Python implementation\n",
    "* uses mlextend package - .frequent_patterns (apriori)\n",
    "* computational heavy with large amounts of data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python implementation - but no data available \n",
    "# will code as comments \n",
    "\n",
    "# import pandas as pd\n",
    "# from mlxtend.frequent_patterns import apriori\n",
    "# from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "\n",
    "# Group by columns to consider \n",
    "# market_basket = df[df['Country'] ==\"United Kingdom\"].groupby(\n",
    "#                 ['InvoiceNo', 'Description'])['Quantity']\n",
    "\n",
    "# one hot encode the data for mlxtend\n",
    "# market_basket = market_basket.sum().unstack().reset_index().fillna(0).set_index('InvoiceNo') \n",
    "# will end up with a lot of columns (one for each item) and rows with invoice number\n",
    "# will contain negative numbers - convert negative to 0, and anything above 1 to 1\n",
    "\n",
    "# #  def encode_data(datapoint):\n",
    "#     if datapoint <= 0:\n",
    "#         return 0\n",
    "#     if datapoint >= 1:\n",
    "#         return 1\n",
    "\n",
    "# market_basket = market_basket.applymap(encode_data)\n",
    "\n",
    "# to find what is sold together use apriori function to data set \n",
    "# takes \"minimum level of support\" - to filter the amount you want - baseline 5%\n",
    "# support = percentage of time that itemset appears in the data (e.g. 50% = itemset appears 50% of the time)\n",
    "# too high = few results\n",
    "# too lower = too many and will need a lot of memory\n",
    "\n",
    "# itemsets = apriori(market_basket, min_support=0.03, use_colnames=True)\n",
    "\n",
    "# final step is to set the metric of most interest\n",
    "# rules = association_rules(itemsets, metric=\"lift\", min_threshold=0.5)\n",
    "\n",
    "# will result in antecedents and consequents, support, conf, lift etc. \n",
    "# can be used to dig a bit deeper to find those rules with high confidence/help to recommend those that occur often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be implemented by yourself - since pandas has trouble with large amount of data \n",
    "\n",
    "# from itertools import combinations, groupby \n",
    "# from collections import Counter\n",
    "\n",
    "# grabbing only order data\n",
    "# orders = df_manual.set_index('InvoiceNo')['StockCode']\n",
    "\n",
    "# find support values for item and frequency\n",
    "# statistics = orders.value_counts().to_frame(\"frequency\")\n",
    "# statistics['support']  = statistics / len(set(orders.index)) * 100\n",
    "\n",
    "# Filter minimum support level\n",
    "# min_support=0.03 # same value we used above.\n",
    "# items_above_support = statistics[statistics['support'] >= min_support].index\n",
    "# orders_above_support = orders[orders.isin(items_above_support)]\n",
    "\n",
    "# filter out items that were only bought once  in one invoice \n",
    "# order_counts = orders.index.value_counts()\n",
    "# orders_over_two_index = order_counts[order_counts>=2].index\n",
    "# orders_over_two = orders[orders.index.isin(orders_over_two_index)]\n",
    "\n",
    "# calculate statistics again\n",
    "# statistics = orders_over_two.value_counts().to_frame(\"frequency\")\n",
    "# statistics['support']  = statistics / len(set(orders_over_two.index)) * 100\n",
    "\n",
    "# Calculate itemsets/itempairs. Function that generates item set \n",
    "# calculate freq of each item with each other (freqAC)\n",
    "# and support (SuppAC)\n",
    "# filter again below min_support\n",
    "\n",
    "# def itemset_generator(orders):\n",
    "#     orders = orders.reset_index().values\n",
    "#     for order_id, order_object in groupby(orders, lambda x: x[0]):\n",
    "#         item_list = [item[1] for item in order_object]\n",
    "#         for item_pair in combinations(item_list, 2):\n",
    "#             yield item_pair\n",
    "# itemsets_gen = itemset_generator(orders_over_two)\n",
    "# itemsets  = pd.Series(Counter(itemsets_gen)).to_frame(\"frequencyAC\")\n",
    "# itemsets['supportAC'] = itemsets['frequencyAC'] / len(orders_over_two_index) * 100\n",
    "# itemsets = itemsets[itemsets['supportAC'] >= min_support]\n",
    "\n",
    "# creating association rules - unstack itemsets - creating datacolumns for support and lift\n",
    "# Create table of association rules and compute relevant metrics\n",
    "# itemsets = itemsets.reset_index().rename(columns={'level_0': 'antecedents', 'level_1': 'consequents'})\n",
    "# itemsets = (itemsets\n",
    "#      .merge(statistics.rename(columns={'freq': 'freqA', 'support': 'antecedent support'}), left_on='antecedents', right_index=True)\n",
    "#      .merge(statistics.rename(columns={'freq': 'freqC', 'support': 'consequents support'}), left_on='consequents', right_index=True))\n",
    "\n",
    "# computing confidence and lift\n",
    "# itemsets['confidenceAtoC'] = itemsets['supportAC'] / itemsets['antecedent support']\n",
    "# itemsets['confidenceCtoA'] = itemsets['supportAC'] / itemsets['consequents support']\n",
    "# itemsets['lift'] = itemsets['supportAC'] / (itemsets['antecedent support'] * itemsets['consequents support'])\n",
    "# itemsets=itemsets[['antecedents', 'consequents','antecedent support', 'consequents support', 'confidenceAtoC','lift']]\n",
    "\n",
    "# take only confidence >0.5 \n",
    "# rules = itemsets\n",
    "# rules_over_50 = rules[(rules.confidenceAtoC >0.50)]\n",
    "# rules_over_50.set_index('antecedents',inplace=True)\n",
    "# rules_over_50.reset_index(inplace=True)\n",
    "# rules_over_50=rules_over_50.sort_values('lift', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online-Machine Learning\n",
    "* using data that is streaming\n",
    "* creme (similar to sklearn), Vowpal Wabbit\n",
    "* 'get better the more you use it' - usually means scheduled training of new models\n",
    "* still lags - one week, or day, not 'real time'\n",
    "* sklearn: partial_fit() gain faster time \n",
    "* online learning - new examples are learned from as fast as possible \n",
    "* example of  use in cases of emergency in news (where everyone wants at the same time)\n",
    "\n",
    "Issues\n",
    "* learning rate too high - will discard things that happened a few seconds ago\n",
    "* skewed classes problem\n",
    "* might overfit or underfit \n",
    "* harder to scale if updating second to second \n",
    "* requires fast access to new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### getting time elapsed on training\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "# clf.partial_fit(X, y)\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent - Review \n",
    "* applied to Online Gradient Descent \n",
    "* get sum of squares / loss function - plotted get slope and intercept\n",
    "* adjust slope and intercept by learning rate to get step size\n",
    "* repeat until max number of steps or convergence\n",
    "* find derivative of loss function\n",
    "* used stochastic - takes a random sample - reduce computation time\n",
    "* learning rate changes from large to small - smaller takes longer to converge\n",
    "* mini batch - takes a number of random samples - more stable estimates\n",
    "* new data can be taken to update - takes one more step - from the most recent estimate\n",
    "* leads to a new 'line' or fit with new data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lhl_env38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93c9cabeb8165e6a1575ace97e023eeebe73f88984ce88042818ce2a73501ce8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
