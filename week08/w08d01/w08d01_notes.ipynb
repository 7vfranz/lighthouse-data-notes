{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural networks\n",
    "* goes through layers of linear/non-linear function\n",
    "* connections are given weights to determine how much each input the unit propagates\n",
    "* weight adjustment happens during training\n",
    "* done through backpropogation and gradient descent \n",
    "* not always the best case especially with tabular data\n",
    "* better for unstructured data - images, speech, videos, text\n",
    "\n",
    "**Advantages**\n",
    "* mix of parametric and non-parametric\n",
    "* number of parameters and weights are determined beforehand BUT number of weights can be alot\n",
    "* weights can also zero out - blocking that path\n",
    "* makes NN more adaptable\n",
    "* essentially non-linear models better performance on real-world relationships \n",
    "* convolutional - can have broad/details inputs\n",
    "* recurrent - have feedback looks, can remember previous inputs, good for timeseries and NLP\n",
    "* can also be combined\n",
    "\n",
    "**Downsides**\n",
    "* black boxes - impossible to intuitively explain\n",
    "* needs large amounts of training data - can take a long time to learn \n",
    "\n",
    "**Deep Dive**\n",
    "* each node/neuron has an activation - higher number, more activated\n",
    "* first layer - is inputs\n",
    "* last layer - has activation also for prediction probability \n",
    "* activation of one layer changes activation of second layer\n",
    "* hidden layers - piece together different components, activated by different characteristics or subcomponents\n",
    "* picture an image - that have different components/characteristics\n",
    "* sigmoid curve to compress activation to 0-1, how positive the relative weighted sum is\n",
    "* also includes a 'bias' - to determine how inactive/active a node should be - a constant - similar to y-intercept\n",
    "* bias determines how high the weighted sub should be before it gets meaningfully activated \n",
    "* each node has has these weights and biases for activation\n",
    "* during learning - the right weights and biases are taken\n",
    "* weights (matrix) multiplied by inputs (vectors) + beta (constants) all multiplied by sigmoid function- which is basically a matrix vector multiplication (see chapter 3 again  from 3blue1brown)\n",
    "* ReLU instead of sigmoid (max(0,a)) rectified linear unit,\n",
    "* faster for deeper learning, (goes straight up as a line instead of a curve)\n",
    "\n",
    "**How a Neural Network Learns**\n",
    "* Gradient descent - adjust weights and constants/threshold \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Lecture\n",
    "\n",
    "Deep Learning\n",
    "* very data hungry - millions of data points\n",
    "* can train models on GPUs\n",
    "* all comes down to larget matrix multiplications\n",
    "* hidden layers - intermediate computations\n",
    "    * next set take in the previous as input\n",
    "* everything is set beforehand by the user: how many input features, intermediate layers, output predictions \n",
    "* connections have weights - parameters that are being tuned by the data \n",
    "* different patterns or designs and schemes that work better based on what data you have\n",
    "* deeper usually better performance - but more expensive to train and get predictions from\n",
    "* in industry - architecture is based on what is already out there depending on the data\n",
    "\n",
    "Intuition for deep neural networks\n",
    "* Why are deeper better\n",
    "* lower level features to edges to countours to object parts to classes - the deeper the level - the more lower level features - and more granularity, more abstract/complex features\n",
    "\n",
    "Some applications\n",
    "* chess, music generation, image generation (generative adversial network), self-driving cars, protein structures\n",
    "* can stack an ML on a neural network - but needs to be differentiable (find a derivative)\n",
    "* works mostly with non-structured data (audio, images, sensor/recordings, language/text)\n",
    "* usually for high-dimensional data\n",
    "* but is getting better than typical machine learning for structured data e.g. TabNet from Google [tabnet](https://towardsdatascience.com/tabnet-deep-neural-network-for-structured-tabular-data-39eb4b27a9e4)\n",
    "\n",
    "Advantages\n",
    "* little preprocessing and feature engineering\n",
    "* only need really one-hot encoding, standardization\n",
    "* within layers - does feature selection/feature engineering\n",
    "* standardization - 0 mean and 1 SD\n",
    "* normalization - dividing a value by its mean \n",
    "\n",
    "Caveats\n",
    "* needs more data\n",
    "* more functions to expression, more parameters, more data needed to learn them\n",
    "* upwards of millions for images, tens of millions of text, thousands of hours of audio\n",
    "* when data is small - no advantage to ML\n",
    "* but scale is much better when you have more data, can continuously get better \n",
    "* ML caps at large amounts of data - where adding more does not add \n",
    "* DL can also do some transfer learning - start with a NN that's pretrained with a similar/other data set - e.g. image classification \n",
    "\n",
    "Deeper Dive - Deep Neural Networks\n",
    "* how deep neural networks work\n",
    "* number of units, connection between units, and equations within units are set arbitrarily - by user, through a premade architecture\n",
    "* training changes the weights\n",
    "* visualized with a 'computation graph'\n",
    "\n",
    "Fully-connected layers \n",
    "* seen in multilayer perceptoron, linear regression\n",
    "* every unit in one layer is connected to every unit in the next layer\n",
    "* linear regression - X to Y \n",
    "* multilayer perceptron - has more layers \n",
    "* calculation is a weighted sum, which results in a dot product.\n",
    "* think of units are rows with multiple columns\n",
    "* weights as a matrix \n",
    "* so to get Y = matrix multiplication of W * X\n",
    "* can tell it's linear, matrix * x = y, can use substitution to end up with a linear equation\n",
    "* good usually for tabular data \n",
    "\n",
    "Non-linear DL\n",
    "* universal function approximator to get non-linearities\n",
    "* after weighted sum - apply a non-linear transformation - using a sigmoid or taking the log\n",
    "* non-linear transformation does not have to be the same at each layer or each unit within a layer\n",
    "* applied on weighted sums - \n",
    "* common functions:\n",
    "    * ReLU - no negative units, clamped down to 0\n",
    "        * used in intermediate layers - in all or some variation of it\n",
    "        * between 0 and +infinity \n",
    "    * Sigmoid\n",
    "        * usually at the output layer - clamped within some range 0-1\n",
    "        * negative values gets clamped to 0, and positive to 1\n",
    "        * good for binary\n",
    "    * TanH\n",
    "        * similar to sigmoid - but clamps to -1 to 1\n",
    "    * Softmax\n",
    "        * multiple output units, output sums to 1\n",
    "        * all positive \n",
    "        * good for multiclass\n",
    "        * less useful for hidden layers\n",
    "\n",
    "Architecture Decisions\n",
    "* architectures usually provided for free \n",
    "* take from academic\n",
    "\n",
    "Training Deep Neural Networks\n",
    "* loss function based on gradient descent\n",
    "* gradient - derivative of loss with respect to the parameters (steepest ascent) - use the opposite \n",
    "    * initialize parameters\n",
    "    * compute gradient of loss function with parameters - increases if moved to that direction\n",
    "    * move parameters in the opposite direction of the gradient - to decrease loss function\n",
    "    * repeat until loss gradually decreases \n",
    "* loss function must be differentiable - NNs usually are differentiable\n",
    "* accuracy - can not be used - because not differentiable\n",
    "* almost always a stochastic gradient descent \n",
    "* derivative trick - chain rule - to find the effect of weight on Loss \n",
    "* finds causal effect of weight (w) on the y - and (chain) effect of y on Loss \n",
    "* learning rate - changes the weight to minimize the gradient \n",
    "* all weights are adjusted in parallel, and updates\n",
    "\n",
    "TensorFlow - PyTorch\n",
    "* TF more scalable, and easier to deploy\n",
    "* TF v1 is hard to use, do not attempt for bootcamp\n",
    "\n",
    "Keras\n",
    "* used in bootcamp\n",
    "* easier to use\n",
    "* used by beginners and industry\n",
    "* similar to sklearn\n",
    "* creates tensorflow under the hood\n",
    "* Dense = full connectivity, how many units are outputed - this is the calculation that's done in each unit\n",
    "* activation function - for non-linearity \n",
    "* input_shape - how many are coming in - number of columns/features \n",
    "* first is .compile - specify loss='mean_squared_error', optimizer='adam' - for updating the weights - usually stochastic gradient descent - sgd with tweaks - always use adam\n",
    "* .fit - validation data - prints how model is doing on validation data - but not used \n",
    "* epochs, batch_size=32 - how many computations per gradient descent\n",
    "* EarlyStopping(patience=3) - stop if improvement is not increasing \n",
    "* architecture is each layer and functions - can be followed along from published stuff\n",
    "* preprocessing and scaling - less needed - layers assumed to take care of scaling, and imputing missings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhl_env38",
   "language": "python",
   "name": "lhl_env38"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
