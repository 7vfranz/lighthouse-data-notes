{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Lecture\n",
    "\n",
    "**Sentiment Analysis**\n",
    "* can be supervised or unsupervised\n",
    "* most of the time is supervised \n",
    "* binary (good/bad), multiclass (scales/neutral)\n",
    "* Opinion mining, sentiment mining, subjectivity \n",
    "* marketing, customer service, social media, psychology\n",
    "* problems with negation, sarcasm, writing style\n",
    "* use in many marketing, brand monitoring, product reviews\n",
    "* chat bots, stock prediction, suicide prevention\n",
    "* with NLP (as in bag of words) - words become features, if vocabulary is large - need just about or more data points (rows must be greater than columns)\n",
    "* can be extended to any classification problem\n",
    "\n",
    "SA - Process\n",
    "* dataset - preprocessing - vectorize - model - output\n",
    "* one vectorized - can be put through any model, van essentially do Naive Bayes, Logistic Regression, Decision Trees\n",
    "* Neural networks - for more complex sentence structures\n",
    "    * word2vec then fed through another NN for log/linear regression\n",
    "\n",
    "SA - Demo\n",
    "* during preprocessing can keep n most frequent words\n",
    "* binary count vectorizer - just counts appears or not\n",
    "* when binary used Bernoulli Naive Bayes\n",
    "\n",
    "**Topic Modelling**\n",
    "* unsupervised to identify topics that describe them\n",
    "* challenges: topic mixtures, dependent on training data, multiple word meanings (e.g. model), determine actual topic name afterwards, hyperparam of number of parameters, need some domain knowledge\n",
    "* essentially clustering\n",
    "* tagging documents, scientific papers\n",
    "* recommendations based on reading\n",
    "* search engines, labeling webpages with topics\n",
    "\n",
    "Latent Dirichlet Allocation (LDA)\n",
    "* each doc as distribution of topics\n",
    "* topics as distribution of words\n",
    "* most probable topic, and outcome of words\n",
    "* hierarchical bayesian model \n",
    "* p(topics|documents) prop to p(doc|topics) * p(topics)\n",
    "* learns words associated with a topic, distribution of words per topic\n",
    "* user sets number of words associated with topics, number of topics\n",
    "* alpha - topics per document \n",
    "    * thetaD - topic proports in document d\n",
    "* eta (n) - words for each topic\n",
    "* K - number of topics\n",
    "    * BetaK - words inside a topic - related to n\n",
    "* end with probability of topics (unlabeled)\n",
    "* can then plot top words - to visualize LDA results\n",
    "    * to see what words are associated with each topic\n",
    "* can also plot PCA - with each circle as a topic\n",
    "    * location along PC1 and PC2 - closer together - similar \n",
    "    * size is how common the topics come up\n",
    "    * can see which words are associated with each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Compass\n",
    "\n",
    "**NLP Modeling**\n",
    "Sentiment analysis \n",
    "* context mining text\n",
    "* used for supervised and unsupervised\n",
    "* social sentiment of brands, products, services, opinions, emotions\n",
    "* aka. opinion mining, sentiment mining, subjectivity analysis\n",
    "\n",
    "Topic Modeling\n",
    "* uncovering hidden structure in a collection of texts\n",
    "* involves dimensionality reduction - text to topics\n",
    "    * unsupervised learning - can be used for clustering text into different topics\n",
    "    * tagging - tagging abstract topics unknown before hand to represent the information in them\n",
    "* LDA most common (Latent Dirichlet Allocation) \n",
    "\n",
    "**Sentiment Analysis**\n",
    "* tokenize > filtering > negation handling > stem > classify > sentiment class\n",
    "* document - tweets, phrases, parts of news articles, whole articles \n",
    "* bag of words on certain words\n",
    "* each document is a row, each word (unique) in the all the documents are features \n",
    "* data arranged in this way is a corpus\n",
    "* once in this format can run Naive Bayes to determine sentiment (labeled) \n",
    "* bayesian conditional probability \n",
    "    * prior - probability of positive and negative\n",
    "    * likelihood - probability that a document is class c \n",
    "    * posterior - prior * likelihood - updated rule\n",
    "    * Normalization constant - denominator for probability distribution\n",
    "* tries to find 'cMAP' - Max A Posteriori - or maximum probability that document belongs to a certain class \n",
    "* argmax finding class where probability of being class 1 is greater than probability of being class 2 - then belongs to that class \n",
    "* end P(feature|class 1) * P(class 1)\n",
    "* Naive Bayes in SA \n",
    "    * bag of words assumption that position does not matter\n",
    "    * conditional independence features are independent of each other \n",
    "    * reduces number of parameters\n",
    "    * linear complexity instead of exponenetial\n",
    "    * aka 'Multinomial Naive Bayes\" when applied to text classification\n",
    "* done with NLTK - cleaning data to BoW then model\n",
    "* \n",
    "\n",
    "**Latent Dirichlet Allocation**\n",
    "* for Topic modeling \n",
    "* used in many recommendation systems\n",
    "* preprocess > train (LDA) > score > evaluate\n",
    "* can be done with wikipedia - set limit to number of words - remove meta data - gather all words sorted by popularity (features) - remove rare at top and bottom - remove based on word length - stoplist, parts of speech (nouns, verbs, adverb)\n",
    "* recommended to keep 50-100,000 of words (features)\n",
    "* go through tfidf - to rank by rareness \n",
    "* LDA - topic assigned to each word, surrounded by context\n",
    "* BoW model (no syntax rules)\n",
    "* based on word frequencies - model learns what are in the same context \n",
    "* hyperparameters\n",
    "    * alpha - how many topics per document\n",
    "        * high alpha - document represented in a lot of topics - documents will appear more similar\n",
    "        * low alpha, document represented in a few topics \n",
    "    * beta - word distribution per topic \n",
    "        * high - each topic contains mixture of words - topics will seem more similar \n",
    "        * low - each topic contains small specific key words\n",
    "* initial topics - depends on dataset \n",
    "* document is made up of topics, topics are made up of words\n",
    "* only understands words that are provided in the vocabulary\n",
    "* corpus/preprocessing most important step - if some words are out of the dictionary\n",
    "* only given numbers/vectors, probabilities that a document belongs to a given topic\n",
    "* tagging after of what those topics are is done by people for people\n",
    "* LDA - results in a lot of dimensions (from multiple features) - to plot and visualize - can use dimensionality reduction (PCA) to plot across space - and visualize the similarity of topics \n",
    "* distance threshold to see what is closed to each other\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhl_env38",
   "language": "python",
   "name": "lhl_env38"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
