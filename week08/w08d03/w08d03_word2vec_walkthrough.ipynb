{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/franzv/googleNews_vectors/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = model['easy'] #find numeric vector for a word\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 0.6836091876029968),\n",
       " ('lovely', 0.6676310896873474),\n",
       " ('neat', 0.6616737246513367),\n",
       " ('fantastic', 0.6569241881370544),\n",
       " ('wonderful', 0.6561347246170044),\n",
       " ('terrific', 0.6552367806434631),\n",
       " ('great', 0.6454657912254333),\n",
       " ('awesome', 0.6404187083244324),\n",
       " ('nicer', 0.6302445530891418),\n",
       " ('decent', 0.5993332266807556)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find most similar word to a word\n",
    "model.most_similar('nice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68360907"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('nice','good') # find similarity between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7190052"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# antonyms are also highly similar - can sub with each other\n",
    "model.similarity('bad','good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674735069275),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411403656006)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relationships between multiple words\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boy', 0.808031439781189),\n",
       " ('teenager', 0.6755870580673218),\n",
       " ('teenage_girl', 0.6386616826057434),\n",
       " ('man', 0.6255338191986084),\n",
       " ('lad', 0.616614043712616),\n",
       " ('schoolgirl', 0.6113480925559998),\n",
       " ('schoolboy', 0.6011566519737244),\n",
       " ('son', 0.5938458442687988),\n",
       " ('father', 0.5887871384620667),\n",
       " ('uncle', 0.5734449028968811)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['girl', 'dad'], negative=['mom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('madrid', 0.5295541286468506),\n",
       " ('dubai', 0.509259819984436),\n",
       " ('heidi', 0.48901548981666565),\n",
       " ('portugal', 0.48763689398765564),\n",
       " ('paula', 0.48557141423225403),\n",
       " ('alex', 0.480734646320343),\n",
       " ('lohan', 0.4801103472709656),\n",
       " ('diego', 0.48010095953941345),\n",
       " ('florence', 0.47695302963256836),\n",
       " ('costa', 0.4761490225791931)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['paris','spain'], negative=['france'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('daughter', 0.6066097021102905),\n",
       " ('niece', 0.5490824580192566),\n",
       " ('granddaughter', 0.540050745010376),\n",
       " ('aunt', 0.5397382974624634),\n",
       " ('husband', 0.5387389659881592),\n",
       " ('sister', 0.5360148549079895),\n",
       " ('son', 0.5356959104537964),\n",
       " ('wife', 0.5313628911972046),\n",
       " ('father', 0.5261732339859009),\n",
       " ('grandmother', 0.5253341197967529)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['chair','mother'], negative=['table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# includes some stop words but not all\n",
    "# includes misspelled words\n",
    "# commonly used words\n",
    "# includes ### to match digits, and slightly messy with some non-sense words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING BoW and TfIdf from scratch\n",
    "* goal: read text > preprocess > create data > one hot encoded matrix > train neural network > extract weights\n",
    "* word embeddings - turns words to vectors, and allows to see similar words in a semantic sense\n",
    "* pretrained NN have vectors for words (one input layer, one hidden, one output) - determines similarity\n",
    "* end with pairs of focus and context words with windows\n",
    "* creating training matrices for word embeddings\n",
    "    * hyperparameters:\n",
    "        * window size of context (w) - how many words to look forward and back\n",
    "        * create pairs for each 'focus' word and 'context' word\n",
    "* [link](https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating training matrix for word embedding from scratch \n",
    "import re\n",
    "\n",
    "def clean_text(\n",
    "    string: str, \n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=['the', 'a', 'and', 'is', 'be', 'will']) -> str:\n",
    "    \"\"\"\n",
    "    A method to clean text \n",
    "    \"\"\"\n",
    "    # Cleaning the urls\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "    # Cleaning the html elements\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL PIPELINE FOR CREATING WORD PAIRS GIVEN A LIST OF STRINGS \n",
    "# # Defining the window for context\n",
    "# window = 2\n",
    "\n",
    "# # Creating a placeholder for the scanning of the word list\n",
    "# word_lists = []\n",
    "# all_text = []\n",
    "\n",
    "# for text in texts:\n",
    "\n",
    "#     # Cleaning the text\n",
    "#     text = text_preprocessing(text)\n",
    "\n",
    "#     # Appending to the all text list\n",
    "#     all_text += text \n",
    "\n",
    "#     # Creating a context dictionary\n",
    "#     # for i, word in enumerate(text):\n",
    "#         for w in range(window):\n",
    "#             # Getting the context that is ahead by *window* words\n",
    "#             if i + 1 + w < len(text): \n",
    "#                 word_lists.append([word] + [text[(i + 1 + w)]])\n",
    "#             # Getting the context that is behind by *window* words    \n",
    "#             if i - w - 1 >= 0:\n",
    "#                 word_lists.append([word] + [text[(i - w - 1)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating index for each unique word, after initial creation of data points\n",
    "* used later for one-hot encoding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unique_word_dict(text:list) -> dict:\n",
    "    \"\"\"\n",
    "    A method that creates a dictionary where the keys are unique words\n",
    "    and key values are indices\n",
    "    \"\"\"\n",
    "    # Getting all the unique words from our text and sorting them alphabetically\n",
    "    words = list(set(text))\n",
    "    words.sort()\n",
    "\n",
    "    # Creating the dictionary for the unique words\n",
    "    unique_word_dict = {}\n",
    "    for i, word in enumerate(words):\n",
    "        unique_word_dict.update({\n",
    "            word: i\n",
    "        })\n",
    "\n",
    "    return unique_word_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_text = [\"word\", \"another\", \"king\", \"prince\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'another': 0, 'king': 1, 'prince': 2, 'word': 3}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_unique_word_dict(temp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data points using one hot encoding \n",
    "# vector size will be equal to a number of unique words in the document \n",
    "# code below gets the index, and adds a 1 for when it occurs\n",
    "# e.g. 'blue' = [1, 0, 0]\n",
    "# 'car' = [0, 1, 0]\n",
    "# 'sky' = [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating two matrices - one for X (focus words) - one for Y (context words)\n",
    "# from scipy import sparse\n",
    "# import numpy as np\n",
    "\n",
    "# # Defining the number of features (unique words)\n",
    "# n_words = len(unique_word_dict)\n",
    "\n",
    "# # Getting all the unique words \n",
    "# words = list(unique_word_dict.keys())\n",
    "\n",
    "# # Creating the X and Y matrices using one hot encoding\n",
    "# X = []\n",
    "# Y = []\n",
    "\n",
    "# for i, word_list in tqdm(enumerate(word_lists)):\n",
    "#     # Getting the indices\n",
    "#     main_word_index = unique_word_dict.get(word_list[0])\n",
    "#     context_word_index = unique_word_dict.get(word_list[1])\n",
    "\n",
    "#     # Creating the placeholders   \n",
    "#     X_row = np.zeros(n_words)\n",
    "#     Y_row = np.zeros(n_words)\n",
    "\n",
    "#     # One hot encoding the main word\n",
    "#     X_row[main_word_index] = 1\n",
    "\n",
    "#     # One hot encoding the Y matrix words \n",
    "#     Y_row[context_word_index] = 1\n",
    "\n",
    "#     # Appending to the main matrices\n",
    "#     X.append(X_row)\n",
    "#     Y.append(Y_row)\n",
    "\n",
    "# # Converting the matrices into an array\n",
    "# X = np.asarray(X)\n",
    "# Y = np.asarray(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays are fed into a NN to get the weights from input to hidden layers = embeddings\n",
    "* at this point the output of the NN is not relevant\n",
    "* each vector can be plotted to see relationship between words\n",
    "* revisit for more understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Word2Vec using Gensim Library\n",
    "* using word2vec for word embedding (vectorization) for creating word vectors with Gensim\n",
    "* word embedding approaches: BoW, TF-IDF, Word2Vec\n",
    "* BoW \n",
    "    * doesn't need a lot of data, \n",
    "    * but creates sparse data set\n",
    "    * no context\n",
    "* TfIDF \n",
    "    * Term frequence = (Number of Occurences of a word)/(Total words in the document)\n",
    "    * IDF(word) = Log((Total number of documents)/(Number of documents containing the word))\n",
    "    * IDF value for the word \"rain\" is 0.1760, since the total number of documents is 3 and rain appears in 2 of them, therefore log(3/2) is 0.1760.\n",
    "* Word2Vec - relationships between words are learned through a NN\n",
    "    * SkipGram Model\n",
    "        * context words predicted from base word\n",
    "        * love to dance - love and dance predicted from 'to'\n",
    "    * Continuous Bag of Words \n",
    "        * focus words predicted from context\n",
    "* Pro: semantic meaning retained, context is kept, no sparse vectors, Cons: more complex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lxml and bs4 for webscraping wikipedia \n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# downloaded wikipedia article, read article content, and parsed with beautiful soup\n",
    "#  take only ones from paragraphs\n",
    "scrapped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "article = scrapped_data.read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "# join together all the paragraphs\n",
    "for p in paragraphs:\n",
    "    article_text += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing - lower, remove non-letters, change whitespace to space\n",
    "processed_article = article_text.lower()\n",
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "\n",
    "#\n",
    "# Preparing the dataset\n",
    "all_sentences = nltk.sent_tokenize(processed_article) # to sentences \n",
    "\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences] # to words\n",
    "\n",
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_words not contains the list of all the words in the article \n",
    "# pass through word2Vec using gensim \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "# min count - words must appear at least twice in the corpus\n",
    "word2vec = Word2Vec(all_words, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai', 'intelligence', 'artificial', 'learning', 'human', 'used', 'research', 'machine', 'use', 'problems']\n"
     ]
    }
   ],
   "source": [
    "# to see the dictionary of unique words >=2 \n",
    "vocabulary = list(word2vec.wv.index_to_key)\n",
    "print(vocabulary[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words converted to vectors \n",
    "# to find vectors of a word\n",
    "# word2Vec not as affected by size of vocab as in BoW\n",
    "v1 = word2vec.wv['artificial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00194258,  0.00820788, -0.00509809, -0.00056012,  0.00799723,\n",
       "       -0.00197003, -0.00084592,  0.0153349 , -0.01140489,  0.00494688],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_words = word2vec.wv.most_similar('intelligence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ai', 0.5406545996665955),\n",
       " ('knowledge', 0.459836483001709),\n",
       " ('science', 0.4470387399196625),\n",
       " ('also', 0.445144921541214),\n",
       " ('artificial', 0.4414222538471222),\n",
       " ('logic', 0.43143799901008606),\n",
       " ('intelligent', 0.42686840891838074),\n",
       " ('used', 0.4217250645160675),\n",
       " ('rights', 0.4127027988433838),\n",
       " ('use', 0.4124809801578522)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lhl_env38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93c9cabeb8165e6a1575ace97e023eeebe73f88984ce88042818ce2a73501ce8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
