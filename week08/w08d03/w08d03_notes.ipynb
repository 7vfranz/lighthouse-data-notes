{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Compass\n",
    "**NLP**\n",
    "* messy data, with few rules\n",
    "* from linguistics\n",
    "* involves speech recognition, speech to text\n",
    "\n",
    "Linguistics/Computational Linguistics\n",
    "* grammar, semantics, phonetics\n",
    "* computational - applying computer science\n",
    "* Statistical Natural Language Processing\n",
    "* linguistic science \n",
    "\n",
    "Simple Introduction\n",
    "* typical interaction: human talks to machine > audio captured > converted to text > processed > data to audio conversion > response\n",
    "* behind gogole translation, grammatical accuracy, interactive voice responses, personal assistants\n",
    "* challenges: sarcasm, plurals, understanding context, lots of ambiguity\n",
    "\n",
    "Syntax\n",
    "* arrangement of words\n",
    "* assess how natural language aligns with grammatical rules \n",
    "* techniques: lemmatization, morphological segmentation, word segmentation, part of speech tagging, parsing, sentence breaking, stemming\n",
    "\n",
    "Semantics\n",
    "* meaning of the text\n",
    "* hardest part \n",
    "* techniques: named entity recognition (e.g. names of people, places), word sense disambiguation (meaning word based on context), Natural language generation (deriving semantic intentions and converting to human language)\n",
    "\n",
    "**Representations**\n",
    "* transforming text data into numerical features\n",
    "\n",
    "**Bag of words**\n",
    "* text as 'bag' - multiset of words\n",
    "* disregard grammar and word order - and keep frequency/multiplicity\n",
    "* common in document classification, frequency of words is used as a feature \n",
    "* can find if sentences have alot of common words\n",
    "* doesn't work when there's small changes in terminalogy, sentences with similar meaning with different words\n",
    "* results in vectors with lots of zeros (sparse)\n",
    "* can do some text cleaning before hand - ignoring: case, punctuation, stop words, misspelled words, stemming \n",
    "* N-Grams - BoW Model\n",
    "    * create vocab of grouped words, for more meaning \n",
    "    * each word or token = gram\n",
    "    * two word pairs - bi-gram\n",
    "* min_df in BoW for filtering only words that appear more than once\n",
    "\n",
    "**TF-IDF (term frequency-inverse document frequency)**\n",
    "* numerical stat, to reflect importance of a word\n",
    "* weighs values - used in many text-based recommender systems\n",
    "* increases based on number of times word appears in doc, and decrease by number of docs in the corpus that ocntain the word \n",
    "* term frequency (TF) = # of time word occurs in doc / total # of words in doc\n",
    "* higher TF - more times a word occurs in a doc\n",
    "* probability of finding a word in a document \n",
    "* Inverse document frequency (IDF) - how much information the word provides (rare or common in documents)\n",
    "    * rare words - have higher IDF scores\n",
    "    * inverse of TF and then log\n",
    "* TF-IDF together\n",
    "    * higher weights = higher frequency of word in a document, lower doc frequency of the word in the whole set of documents\n",
    "    * filters out common terms \n",
    "    * as term/word is more common - ratio in the log is closer to 1, bringing the tf-idf close to 0\n",
    "    * rare words = larger values TF and IDF, TF-IDF is high, word is rare in the whole collection of documents but frequent in one document\n",
    "    * common words = smaller values\n",
    "    * weighs down frequent terms - scale up rares\n",
    "    * TfidfVectorizer\n",
    "    * used in search engines \n",
    "\n",
    "**Word2Vec**\n",
    "[more info](https://jalammar.github.io/illustrated-word2vec/)\n",
    "* learning vector representations of words - called 'word embeddings'\n",
    "* done as a pre-processing step before running an RNN\n",
    "* better for semantic meaning of words \n",
    "* from Google \n",
    "* Embeddings\n",
    "* E.g. Big 5 Personality\n",
    "    * getting personality of person from 5 values - a vector\n",
    "    * can do 'cosine_similarity' to check how similar two vectors are \n",
    "* similar with words - can map vectors associated with words to other words - king-man, queen-woman, have vectors that are similar\n",
    "* used in auto-complete, pass a list of two words, and give a prediction, list of words with the highest probability\n",
    "* predictions are based on embeddings \n",
    "* slides a 'window' across a sentence, with output being the last word in that window\n",
    "* Continuous Bag of Words (CBOW) \n",
    "    * use surrounding words to predict current word \n",
    "* Skipgram\n",
    "    * uses current word to guess the context \n",
    "* when fed to a NN/model updates based on error\n",
    "* turning problem into logistic by just taking first whether two words are nighbours or not\n",
    "* while fed random words that are not neighbours (to address sampling)\n",
    "* word2vec central ideas:\n",
    "    * skipgram\n",
    "    * negative sampling\n",
    "* see link for more details on process of training\n",
    "* hyperparameters - window size and number of negative samples\n",
    "* Window size\n",
    "    * smaller window size (2-15) - similarity scores indicate words are interchangeable\n",
    "        * can mistake antonyms - if only looking at surrounding words\n",
    "    * larger window size(15-50) - more indicative of 'relatedness' \n",
    "    * default in Gensim is 5 (2 before, 2 after)\n",
    "* negative samples\n",
    "    * 5-20 is good\n",
    "    * 2-5 enough with large dataset, default is 5 negative samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Lecture\n",
    "* NLP Prep\n",
    "* analyzing large amount of speech/text data\n",
    "* NLU - natural language understanding \n",
    "* applications\n",
    "    * virtual assistants, chat bots, predictive text, email filters, sentiment analysis, topic modeling\n",
    "* old problem, now with better rools\n",
    "* preprocessing nlp - so that the computer can understand\n",
    "\n",
    "**Preprocessing text**\n",
    "* simplifying and converting to numerical values\n",
    "* Involves:\n",
    "    * remove punctuation (depending on the problem)\n",
    "    * remove common words (the, and, a)\n",
    "    * remove capitalization (depending on names)\n",
    "    * tokenize (breaks down sentences to tokens words or punctuation) - chunking\n",
    "    * stemming/lemmatization\n",
    "        * stemming - take only the stem word - remove suffixes, not used too often (changing -> stemmed to chang)\n",
    "        * lemmatization - works a bit better - finding root word, a trained model\n",
    "* package for these: 'NLTK', natural language tool kit\n",
    "* once tokenized - can iterate over easier\n",
    "\n",
    "Vectorizing Text\n",
    "* Bag of Words\n",
    "    * count how often a word appears\n",
    "    * loses structure of text - cat is not good, vs cat is not bad - doesn't take into account negation\n",
    "    * will end lots of columns for each token, very sparse\n",
    "    * lots of columns, not enough observations\n",
    "* TFIDF \n",
    "    * text-frequency inverse document frequency\n",
    "    * works like BoW weighted by how rare the word is across all documents\n",
    "    * very common words - lower weight\n",
    "    * rare words = higher weight/number - only appears in a few sentences - 'must be important'\n",
    "    * finds 'unique' words\n",
    "* Word2Vec\n",
    "    * neural network to learn word associations\n",
    "    * each word is repped as a Vector\n",
    "    * similar words are close together\n",
    "    * direction between words have meaning \n",
    "    * e.g. sex/gender, and present to past, country/capital\n",
    "    * needs a lot of data - but there are pre-trained NNs\n",
    "    * only for preprocessing\n",
    "\n",
    "Word2Vec Architectures\n",
    "* continuous bag of words (CBoW)\n",
    "    * tries to predict target word from surrounding words (context)\n",
    "    * target word (center) should be in same vector, e.g. color, animal\n",
    "* Skip gram\n",
    "    * predict surrounding words given target word\n",
    "    * opposite of CBoW\n",
    "\n",
    "See walkthrough for code\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhl_env38",
   "language": "python",
   "name": "lhl_env38"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
