{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep vs Machine learning\n",
    "* deep is a subset of machine learning, based on artificial neural networks ANN \n",
    "* needs alot of data but once trained needs no human intervention \n",
    "\n",
    "**Convolutional Neural Networks (CNN)**\n",
    "* works with images \n",
    "* useful for scanning high volume of images for a specific item or feature\n",
    "* e.g. shipwreck detection from stream of photos from the ocean\n",
    "* computer-vision type tasks \n",
    "\n",
    "**Recurrent Neural Networks**\n",
    "* has a memory component\n",
    "* keeping past data points and decisions in mind - and consider them when reviewing current data \n",
    "* introduces 'context'\n",
    "* popular in natural language processing, taking into account previous tone and content\n",
    "* other: driving directions\n",
    "\n",
    "**Main Differences**\n",
    "* human involvement (less in Deep once set up)\n",
    "* deep more powerful hardware and resources \n",
    "* Time - ML slower to generate results than Deep\n",
    "* Approach - ML takes in more structured data ~ traditional learning regression\n",
    "* Deep Learning - neural networks for large volume of unstructured data\n",
    "* Applications - more complex, robots, self-driving cars \n",
    "\n",
    "**How Neural Networks Learn**\n",
    "* weights and biases - initialized randomly\n",
    "* find cost function - mean sum of squares prediction\n",
    "* adjusted based on that - stochastic gradient descent - derivative\n",
    "* at everystep, take step on the opposite of the gradient \n",
    "* weights updated based on backpropagation - to minimize cost function\n",
    "* adjustment on some weights matter more so than others - have a higher magnitude \n",
    "* doesn't always pick up the patterns you think it does, sometimes almost completely random (blackbox element)\n",
    "* more modern variants - convulutional, recurrent\n",
    "\n",
    "**Backpropagation**\n",
    "* main way process for optimizing weight in Neural Networks \n",
    "* greater the weights - the greater the change after adjustment to the cost function\n",
    "* units that are most active with an output, also increase weights\n",
    "* adjusting weights - those that are active for predicting a certain class (e.g. number 2) increases, while those that are negative get dimmer (further to 0)\n",
    "* in proportion to weights, and how much they need to change \n",
    "* moves backwards throughout the network for every training example - averages together all desired changes\n",
    "* done through mini-batch (not all the data) - stochastic gradient descent \n",
    "* backprop - describes how one training example (class) wants to change weights and biases - one that will give the largest decrease to the cost \n",
    "    * randomly divide to minibatches (SGD) - and computer gradient descent step\n",
    "\n",
    "**Google Machine Learning**\n",
    "* only one unit in the first hidden layer - will not learn now matter how deep it is, output of the first layer only varies along one dimension\n",
    "    * will not be able to model non-linearity\n",
    "* too many units especially in the first HL - can lead to overfitting - fitting too many shapes\n",
    "* can actually lead to bad results - as it memorizes the dataset - creating clusters or classifying single data points \n",
    "* model can learn different shapes on each run if not enough hidden units/layers - adding more layers/extra nodes produces more repeatable results\n",
    "* feature engineering is still important - not just adding more hidden layers and units\n",
    "* regularization and activation function can be changed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Lecture \n",
    "\n",
    "#### Deep Learning\n",
    "* Gradient Descent - derivative/gradient\n",
    "* Machine Learnning - minimize error - difference between predicted and actual\n",
    "* Splitting data of two classses \n",
    "* in gradient descent - function is continuous - can find the derivative\n",
    "* issue with discrete function - can't find derivative \n",
    "* weight penalty on errors are higher than ones that are correctly classified \n",
    "* transforms data -inf to +inf into interval 0-1 (activation function)\n",
    "* 0=0.5, etc\n",
    "* fitting a sigmoid function\n",
    "* each point gets a probability - and multiplied to get likelihood, from each split - likelihood of getting those set is calculated, correct classification gets higher probability (maximum likelihood)\n",
    "* take the negative log of probability of each point - taken as error function/penalty for each point - adding results in total error \n",
    "* Can be applied to Non-Linear Regions (curves)\n",
    "    * can do two lines, and merge the results, ending with a curve\n",
    "* different probabilities from two predictions - added - then activation function to shrink back to 0-1\n",
    "* can add more weight to a different predictions, one prediction is multiplied by a constant - have more sway on final prediction (linear combination)\n",
    "* neural network ~ essentially a set of linear combinations to end up with a output layer \n",
    "* two inputs - linear, three inputs - 3d (planes) \n",
    "* can add even more hidden layers \n",
    "* inputs can be very high, such as different pixels \n",
    "\n",
    "**Use Cases**\n",
    "* self-driving cars, machine vision\n",
    "* tagging photos, theft detecting, speech recognition\n",
    "* sentiment analysis - movie ratings.. \n",
    "* clarifai, metamind\n",
    "* cancer detection, drug discovery, radiology \n",
    "* finance - buy/sell depending on training \n",
    "* digital advertising - segment users, customer intel\n",
    "* fraud detection\n",
    "\n",
    "**Deep Learning Frameworks**\n",
    "* tensor flow - popular\n",
    "    * python based, open source\n",
    "* Spark\n",
    "    * turns deep learning models into SQL functions\n",
    "* keras - simpler to use\n",
    "    * built on tensorflow\n",
    "    * little code, offers tensorflow backend workflows\n",
    "    * fast experimentation and lightweight\n",
    "* MS cognitive toolkit\n",
    "    * limited support for non-MS\n",
    "    * integrate with Azure\n",
    "* PyTorch - newer\n",
    "    * new competititor to tensorflow\n",
    "    * parth of python\n",
    "* Chainer\n",
    "* ApacheMXNet\n",
    "\n",
    "**PyTorch vs TensorFlow**\n",
    "* two abstractions\n",
    "    * low-level - primitive mathematical operations and neural networks (pytorch, tensorflow)\n",
    "    * high-level - low-levels are used to implement abstractions such as models and layers (keras)\n",
    "* keras is high-level API that works with tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhl_env38",
   "language": "python",
   "name": "lhl_env38"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
