{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Lecture\n",
    "* recommendations more personalized\n",
    "* finds similar users to you - recommend based on what you haven't 'seen' yet\n",
    "* goals: clicks, longer screen time, revenue, conversion rates\n",
    "* split half/half sample with new version vs old version of the website - compare screen time\n",
    "* often compared to baselines and alternatives using A/B experiments\n",
    "* even small increases - can be large for something else\n",
    "* note: final project no baseline for A/B experiments - just judge qualitatively - if recommendations are good...\n",
    "\n",
    "Recommendations\n",
    "* Content based recommender \n",
    "    * use knowledge of each item to recommend a similar\n",
    "    * item based recommendation\n",
    "    * need properties about products\n",
    "    * more specific recommendations\n",
    "    * doesn't need user data\n",
    "    * doesnt expand users interest\n",
    "* Collaborative filtering \n",
    "    * knowledge of user's past purchases/selections to recommend what similar users did\n",
    "    * user-based recommendation\n",
    "    * don't need information about the items\n",
    "    * needs user data\n",
    "    * difficult with new users (cold start)\n",
    "    * more diversity in different types of users\n",
    "\n",
    "Content Based Recommender\n",
    "* e.g. with Movie\n",
    "* 1. feature engineering\n",
    "    * genres, runtime, actors\n",
    "    * description word soup\n",
    "    * multiple different types of products\n",
    "    * not all the same - cross products... \n",
    "    * can be done in an unsupervised way\n",
    "* 2. defining distance metric\n",
    "    * e.g. euclidean distance between two vectors\n",
    "        * np.sqrt(np.sum((x-y) ** 2))\n",
    "        * larger = more dissimilar\n",
    "        * includes magnitude of distance and direction\n",
    "    * cosine similarity - making a triangle\n",
    "        * includes only direction\n",
    "        * 180 - as disimilar, cosine similarity = -1, angle 0 = cosine similarity =1\n",
    "        * 90 deg = 0\n",
    "        * range -1,1, higher is more similarity\n",
    "        * cosine_similarity(x,y) - np.dot(x,y) / (np.sqrt(np.dot(x,x)) * np.sqrt(np.dot(y,y)))\n",
    "        * 1-cosine similarity = cosine distance\n",
    "        * might not want to care about magnitude - if only care about ratio, e.g. bag of words, don't care if it appears more than once \n",
    "* can use and mix both if needed \n",
    "* try to see what's better with or without magnitude \n",
    "* can also do a distance metric learning - using some regression to calculate best distance metric\n",
    "* can do feature importance, feature interactions, reduces feature engineering \n",
    "* 3. recommendation\n",
    "    * based on distance from what they are currently viewing, what's on their cart, and history\n",
    "    * based on some clustering \n",
    "\n",
    "Case Study: Co-occurrence feature engineering\n",
    "* unsupervised - Word2Vec\n",
    "* market-basket \n",
    "* Word2Vec\n",
    "    * word can be substiuted in a context are similar\n",
    "    * context information can be used to learn\n",
    "    * CBOW - input context to output word\n",
    "    * Skipgram -  input word to output context \n",
    "    * can be used for arbitrary data\n",
    "    * anything with item+context can be used\n",
    "    * e.g. image of kitchen -> stove\n",
    "    * things that show up in the same environment - similar vectors\n",
    "    * e.g. object2vec - but uses word2vec exactly\n",
    "    * treated with language data - will give different embeddings\n",
    "    * can be used for recommendation\n",
    "    * context as a users 'cart' online\n",
    "    * good because - some content that are bought together - are not always similar in feature: e.g. water bottle vs shoes\n",
    "    * see how data is formatted during word2vec\n",
    "        * need list of strings\n",
    "        * each string is a word separated\n",
    "        * can do word2vec to fit\n",
    "        * once data 'cart data' is in the same format\n",
    "    * product ids as strings, and fit to word2vec\n",
    "\n",
    "Demo\n",
    "* see other notebook\n",
    "* add more weight to a feature can multiply after standard scaler - to increase the weight of that feature - more sway in the distance calculation\n",
    "* larger the multiplier - have to be more similar based on that\n",
    "* define a distance metric\n",
    "* argsort - sorts by value and gives the index\n",
    "* k-nearest neighbour similar to euclidean distance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Compass\n",
    "\n",
    "Recommendation Systems\n",
    "* collaborative\n",
    "    * based on similarity with other users\n",
    "    * what's liked between two users and one user has watched but not another\n",
    "* content based \n",
    "    * based on labelled content\n",
    "    * specific to what the user has liked in the past \n",
    "    * specific to user\n",
    "    * less diversity in recommendations\n",
    "* hybrid \n",
    "    * does both and then combines recommendations\n",
    "* works very similarly to word2vec and nlp type models\n",
    "\n",
    "Similarity\n",
    "* Cosine\n",
    "    * based on angle of two vectors\n",
    "    * perpendicular will equal 0, dissimilar\n",
    "    * 1 being more similar\n",
    "\n",
    "**APRIORI**\n",
    "* market basket analysis\n",
    "* association between different products \n",
    "* what products get bought together, and provide recommendations of third item \n",
    "* can do association rule mining\n",
    "    * given A (antecedent), likelihood of B (consequent)\n",
    "    * two items - single cardinality\n",
    "* bought A and B -> C - cardinality increases\n",
    "* more combos with high cardinality\n",
    "* will create a lot of association rules\n",
    "* association rule examples:\n",
    "    * A -> D (person buying A more likely to buy D)\n",
    "    * B&C -> A\n",
    "\n",
    "Math behind APRIORI\n",
    "* for each association rule - there's a measure of: \n",
    "* Support - Freq(A,B) / N (all occurrences)\n",
    "    * filters items that are bought less frequently\n",
    "    * e.g. Support(diaper) = (Transactions containing (diaper))/(Total Transactions)\n",
    "* Confidence - Freq(A,B) / freq(A)\n",
    "    * filter out rare items\n",
    "    * Confidence(A → B) = (Transactions containing both (A and B))/(Transactions containing A)\n",
    "    * similar to naive bayes \n",
    "* Lift - Support/ Sup(A) x Sup(B)\n",
    "    * strength of any given rule \n",
    "    * rule due to chance or not\n",
    "    * higher = more likely that there is similarity - rule is good\n",
    "    * Lift(A→B) = (Confidence (A→B))/(Support (B))\n",
    "    * how much more chance of buying A and B together than B alone\n",
    "    * increase in ratio for A given B\n",
    "\n",
    "Can be slow - to speed up:\n",
    "* set minimum value for support/confidence - certain existence (support) and has minimum value of co-occurring (confidence)\n",
    "* extract subsets with higher support than minimum\n",
    "* select rules where confidence is higher than minimum\n",
    "* order rules by descending of lift \n",
    "\n",
    "APRIOR ALGORITHM\n",
    "* frequent item sets to generate association rules\n",
    "* frequent item sets - item whos support value is greater than threshold\n",
    "    * start with item sets (A)\n",
    "    * then item sets of combos of pairs (A,B), (A,C) etc, increasing with each step\n",
    "    * at each step (increasing number in itemset) - anything below threshold is discarded \n",
    "    * at late iteration - those previously eliminated are also eliminated\n",
    "    * as items increases in step - so do the subsets\n",
    "* Subset creation\n",
    "    * once we have final item sets \n",
    "    * we create item subsets (e.g. 1,3,5 = 1,3; 1,5; 3,5; 1; 3; 5)\n",
    "* Confidence (support(1,3,5)/ support(1,3) = 2/3)\n",
    "* so e.g. given {1,3} -> ({1,3,5} - {1,3}) --> 5 predicts 5 with confidence of certain degree \n",
    "* confidence must be greater than threshold for rule to be kept \n",
    "* done with: mlxtend.frequent_patterns import apriori, association_rules\n",
    "* invoice number rows and items in columns - get support, lift etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhl_env38",
   "language": "python",
   "name": "lhl_env38"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
