{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Reinforcement Learning\n",
    "* takes a bunch of states as input - fed through a hidden layer - maps to output (action space)\n",
    "* neither supervised or unsupervised\n",
    "* usually need labelled data \n",
    "* reinforcement learning - plays by itself - doesn't require user input - no target label \n",
    "* grabbing objects, playing games\n",
    "* for games - data set is expensive and unfeasible, data sets don't exist, only learns from the best - not it's own optimal way\n",
    "* Policy Network: NN that takes input of states and outputs an action\n",
    "    * trained using policy gradients\n",
    "    * can apply convolutions\n",
    "    * sample with actions (30% chance UP vs 70% chance DOWN)\n",
    "    * only feedback = score (reward)\n",
    "    * entire goal is optimize policy to maximize reward\n",
    "* at first will start with random actions and eventually will select a sequency of actions that is rewarded \n",
    "* makes those actions more likely in the future \n",
    "* read more: pongs from pixels\n",
    "\n",
    "Credit Assignment Problem\n",
    "* Problem with policy gradients - if fails - likelihood of the whole action sequence is decreased\n",
    "* what actions led to a specific reward (assigning credit to a set of actions) \n",
    "* even if error - parts of the action sequence may still be useful\n",
    "* stems from sparse reward setting - rewards are only at the very end after the sequence \n",
    "* in the case of pong - the final reward doesn't matter so much - as 'hitting the ball back'\n",
    "* because of sparse reward - needs ALOT of training - and often just completely fails\n",
    "\n",
    "One Solution: Reward Shaping\n",
    "* approximate rewards\n",
    "* custom - needs one for each game \n",
    "* suffers from alignment problem \n",
    "    * policy could be overfitting - maximized shaped-rewards and never getting to the final reward\n",
    "    * not generalizing to the final behaviour\n",
    "\n",
    "#### from another reading - hackernoon.com\n",
    "RL as Markov Decision Processes\n",
    "* agent > action > environment > reward/observation \n",
    "* S (states) A (action) P (state transition probability) R (reward) gamma\n",
    "* task is to learn a policy function - find optimal policy that maximizes expected sum of rewards \n",
    "* aka: control problem\n",
    "* credit assignment problem - we don't know which action led to the outcome \n",
    "    * use sum of 'discounted' rewards\n",
    "* crediting each action with exponential decaying impact into the future \n",
    "\n",
    "Two types:\n",
    "* Policy gradient - learning the policy using Deep learning\n",
    "* Q-learning - learning a function of state-action pairs \n",
    "\n",
    "Why use Deep Learning\n",
    "* can't engineer unstructured data such as images or text\n",
    "* can feed raw data such as game pixels \n",
    "* creating a policy network\n",
    "\n",
    "#### Milestones in Reinforcement Learning\n",
    "* games - chinese game GO (2016), texas hold'em \n",
    "\n",
    "\n",
    "#### Deep RL\n",
    "* Building Blocks:\n",
    "    * Agent\n",
    "    * Action\n",
    "    * Discount factor - multipled by future rewards\n",
    "        * discovered by the agent \n",
    "        * make future rewards worth less than immediate rewards\n",
    "        * gamma (y) \n",
    "        * if y=0.8, and reward=10 after 3 time steps\n",
    "        * value = 0.8^3 * 10\n",
    "    * Environment \n",
    "        * world that agent moves, and responds to agent\n",
    "        * environment - takes agents current state and action as input\n",
    "    * State (S)\n",
    "        * immediate situation in which the agent finds itself\n",
    "        * specific place and moment \n",
    "    * Reward (R)\n",
    "        * feedback to measure success/failure of an agents action in a given state\n",
    "        * immediate\n",
    "    * Policy (pi)\n",
    "        * strategy that agent uses to determine the next action based on current state -\n",
    "        * mapping actions that promise the highest reward in a given state\n",
    "    * Value (V)\n",
    "        * expected long-term return with discount \n",
    "        * expected long-term return of current state under the policy\n",
    "        * can be sum of all reward\n",
    "    * Q-value (Q) \n",
    "        * similar to value - but takes on current action* different from value \n",
    "        * long-term return of an action taking an action a under policy from the current state\n",
    "        * maps state-action pairs to reward\n",
    "    * Trajectory\n",
    "        * sequence of states and actions that influence those states\n",
    "* Environment is often a black box\n",
    "* Agent tries to approximate the environments function - what actions changes a state that gives most rewards \n",
    "* [more information](https://wiki.pathmind.com/deep-reinforcement-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.120000000000001"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8**3 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lhl_env38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93c9cabeb8165e6a1575ace97e023eeebe73f88984ce88042818ce2a73501ce8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
