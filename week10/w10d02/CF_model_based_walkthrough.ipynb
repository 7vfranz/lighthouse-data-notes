{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* works with matrix factorization into two matrices\n",
    "* made hard by a lot of missing data \n",
    "* imputation is very slow on large data sets\n",
    "* predictions will be bad at first because of overfitting - but gets around it using regularisation \n",
    "* U and V factors \n",
    "* using surprise package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import surprise\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import urllib \n",
    "# import io\n",
    "# import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: link no longer walks from walk through will just follow along with code and get intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmpFile = urllib.request.urlopen('https://www.librec.net/datasets/filmtrust.zip')\n",
    "# LINK no longer works from walkthrough\n",
    "\n",
    "\n",
    "# # unzip file\n",
    "# tmpFile = zipfile.ZipFile(io.BytesIO(tmpFile.read()))\n",
    "\n",
    "# # open desired data file as pd df, close the zip\n",
    "# # dataset = pd.read_table(io.BytesIO(tmpFile.read('ratings.txt')), sep = ' ', names = ['uid','iid', 'rating'])\n",
    "# # tmpFile.close()\n",
    "\n",
    "# # dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set is first column: USERID, 2: movie ID, 3: review score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use Reader class to load dataset into surprise package \n",
    "\n",
    "# # get the lower and upper rating bounds - defualt in surprise is 1-5\n",
    "# lower_rating=dataset['rating'].min()\n",
    "# upper_rating=dataset['rating'].max()\n",
    "\n",
    "# # change this after loading\n",
    "# reader=surprise.Reader(rating_scale=(0.5,4.))\n",
    "# data=surprise.Dataset.load_from_df(dataset,reader)\n",
    "\n",
    "# # use SVD++ as model to fit \n",
    "# alg = surprise.SVDpp()\n",
    "# output = alg.fit(data.build_full_trainset())\n",
    "\n",
    "# # running predict with ids set as strings\n",
    "# pred = alg.predict(uid='50', iid='52')\n",
    "# score = pred.est\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Making Recommendations\n",
    "# find movies that user has not yet rated \n",
    "\n",
    "# # first get list of all movie ids\n",
    "# iids = dataset['iid'].unique()\n",
    "\n",
    "# # list of iids that uid 50 has rated\n",
    "# iids50 = dataset.loc[dataset['uid']==50, 'iid']\n",
    "\n",
    "# # remove iids that uid has rated from the list of all movie ids\n",
    "# iids_to_pred = np.setdiff1d(iids, iids50)  # finds difference of two arrays, returns unique values in array 1 that's not in array 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predict scores for eahc movie that user 50 didn't rate and find the best ones \n",
    "# settinga ll movies ratings to 4 - and output first prediction\n",
    "\n",
    "# testset = [[50, iid, 5.] for iid in iids_to_pred]\n",
    "# predictions = alg.test(testset)\n",
    "# predictions[0]\n",
    "\n",
    "# each prediction is an object  can be used to find iid with best rating\n",
    "\n",
    "# pred_ratings=np.array([pred.est for pred in predictions])\n",
    "\n",
    "# find index of maximum predicted rating\n",
    "# i_max = pred_ratings.argmax()\n",
    "# can use argpartition() - to find top n items rather than just the top item \n",
    "\n",
    "# used to find corresponding iid to recommend\n",
    "# iid = iids_to_pred[i_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning constants  of SVD++\n",
    "* DD affects size of UU and VV\n",
    "* learning rate - performance of optimization\n",
    "* regularization term - affects overfitting\n",
    "* number of epochs - number of iterations\n",
    "* tuned via gridsearch for surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid ={'lr_all' : [0.001, 0.5], 'reg_all':[0.1,0.5]}\n",
    "# gs = surprise.model_selection.GridSearchCV(surprise.SVDpp, paramgrid, measures=['rmse', 'mae'], cv=3)\n",
    "# gs.fit(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lhl_env38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93c9cabeb8165e6a1575ace97e023eeebe73f88984ce88042818ce2a73501ce8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
