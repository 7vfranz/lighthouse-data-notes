{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lecture\n",
    "* overfitting like memorizing - won't be able to predict on test data\n",
    "     * usually means model is too complex\n",
    "     * training score higher than test score\n",
    "* underfitting\n",
    "    * low performance on training and test\n",
    "\n",
    "* minimizing approximation error: Eapprox = (Etest - Etrain)\n",
    "* better than approximation\n",
    "\n",
    "* Linear Regression\n",
    "    * most basic and popular ML/Stat technique\n",
    "    * can be used to predict\n",
    "    * linear relationship between DV and IV\n",
    "    * yhat - predicted (dv) \n",
    "    * gives a line y=wx + b\n",
    "    * w = weights, and b = constant that the algo gives\n",
    "\n",
    "* Multiple Linear Regression\n",
    "    * more than one feature - each with their own weights\n",
    "    * one constant \n",
    "    * greater the weight (w) the more influence that x has on target y \n",
    "\n",
    "* See walkthrough for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Compass\n",
    "* goal of ML is to find the function that predicts target, given a feature/s\n",
    "* notation = L,l for \"loss function\" - how far a function h is from the target\n",
    "* risk vs loss function\n",
    "    * loss function \n",
    "        * how far an estimate value of a quantity from a true value \n",
    "        * y vs y^\n",
    "        * examples - squared error (always positive) - sensitive to outliers, absolute error loss - not smooth, LP loss (absolute error raised to power of p), more complex Kullback-Leibler Loss\n",
    "        * loss function depends on selected data \n",
    "    * risk function\n",
    "        * average measure of the loss\n",
    "        * average of the loss function (mean squared error?)\n",
    "        * expected value of loss - expected value of loss function, calculated on the 'integral'\n",
    "        * 'arg min' - looking for a function h from a pool of functions H to minimize risk function\n",
    "    \n",
    "* what can go wrong - generalization error\n",
    "    * approximation error\n",
    "        * choose an H or l that are too simple, accuracy of model is low\n",
    "        * not enough complexity to predict the target \n",
    "    * estimation error\n",
    "        * lack of data points \n",
    "        * complex functions need more data to train\n",
    "        * 100 pictures not enough for deep neural networks\n",
    "    * optimization error\n",
    "        * loss function is too complex\n",
    "        * too many observations (data points) \n",
    "\n",
    "#### Linear Regression\n",
    "* looking for a vector 'w' (weights) that will minimize our loss\n",
    "* done by setting the **gradient = 0** and find the derivative using 0\n",
    "* method of 'least squares' \n",
    "* getting sum of square residuals\n",
    "    * subtract each value from the fitted line, square, then sum\n",
    "    * sum of squared residuals should keep decreasing until smallest, end up with y=ax+b\n",
    "* optimize a and b (slope and y int) to end up with smallest SSresiduals\n",
    "* for each point added ((a*x1+b)-y1)^2, repeat for each\n",
    "* takes the derivative of the function, and find point where slope = 0 \n",
    "* takes the derivative of both the slope and intercepts - to know where the optimal values are for the best fit\n",
    "* 1. minimize the square distance between observed values and the line\n",
    "* 2. done by taking the derivative and finding where = 0\n",
    "* line minimizes sum of squares i.e. the least squares\n",
    "* seeing how good the guess of the line calculate the R^2\n",
    "\n",
    "* calculate R^2, and p-value for R^2 \n",
    "* first project the data onto one mean > find the variance (average sum of squares) \n",
    "* then on original plot - find variance for the fit (average SS(data-line))\n",
    "* smaller residuals from the line/fit Var(fit) < Var(mean)\n",
    "* therefore some of the variance in varX can be 'explained' by taking varY into account\n",
    "* R^2 = Var(mean) - Var(fit) / Var(mean)\n",
    "* R^2 = SS(mean) - SS(fit) / SS(mean) - same ratio\n",
    "* e.g. 0.6 = 60% reduction in varX when taking varY into account\n",
    "* in other words varY explains 60% of the variance in varX\n",
    "* no residuals, R^2, varY explains all the variance of varX\n",
    "\n",
    "* same thing for more complex models (multiple variables)\n",
    "* for three variables, fit a plane instead of a line but still a least square fit \n",
    "* coefficient/weight for each variable, calculates residuals\n",
    "* if variable doesn't contribute anything - i.e doesn't make the SS(fit) smaller - then coefficient becomes 0\n",
    "* meaning more variables in the model is not penalized, if they are not good predictors they just become zeros based on SSfit\n",
    "* but somewhat bad - because everything has some chance in adjusting the SSfit even a little - leading to a better R^2\n",
    "* adjusted R^2 to scale R^2 based on the number of parameters \n",
    "\n",
    "* R^2 statistically significant \n",
    "* comes from F - variation(x) / var(notX) i.e. variation not explained by X, not explained by the fit\n",
    "* based on a degrees of freedom\n",
    "* F = variation explained by fit / variation not explained by fit\n",
    "* compare to random normal data\n",
    "* linear regression quantifies the relationship in your data\n",
    "* large R^2, lower p value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Linear Regression - using Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* predictive model for linear relationships between dv and iv\n",
    "* simple linear regression - one iv for the dv\n",
    "* more complex - has multiple ivs Y = C + M1*X1 + M2*X2 + â€¦\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:      Stock_Index_Price   R-squared:                       0.898\n",
      "Model:                            OLS   Adj. R-squared:                  0.888\n",
      "Method:                 Least Squares   F-statistic:                     92.07\n",
      "Date:                Sun, 29 May 2022   Prob (F-statistic):           4.04e-11\n",
      "Time:                        13:18:35   Log-Likelihood:                -134.61\n",
      "No. Observations:                  24   AIC:                             275.2\n",
      "Df Residuals:                      21   BIC:                             278.8\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const              1798.4040    899.248      2.000      0.059     -71.685    3668.493\n",
      "Interest_Rate       345.5401    111.367      3.103      0.005     113.940     577.140\n",
      "Unemployment_Rate  -250.1466    117.950     -2.121      0.046    -495.437      -4.856\n",
      "==============================================================================\n",
      "Omnibus:                        2.691   Durbin-Watson:                   0.530\n",
      "Prob(Omnibus):                  0.260   Jarque-Bera (JB):                1.551\n",
      "Skew:                          -0.612   Prob(JB):                        0.461\n",
      "Kurtosis:                       3.226   Cond. No.                         394.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "Stock_Market = {'Year': [2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016],\n",
    "                'Month': [12, 11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1],\n",
    "                'Interest_Rate': [2.75,2.5,2.5,2.5,2.5,2.5,2.5,2.25,2.25,2.25,2,2,2,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75],\n",
    "                'Unemployment_Rate': [5.3,5.3,5.3,5.3,5.4,5.6,5.5,5.5,5.5,5.6,5.7,5.9,6,5.9,5.8,6.1,6.2,6.1,6.1,6.1,5.9,6.2,6.2,6.1],\n",
    "                'Stock_Index_Price': [1464,1394,1357,1293,1256,1254,1234,1195,1159,1167,1130,1075,1047,965,943,958,971,949,884,866,876,822,704,719]        \n",
    "                }\n",
    "\n",
    "df = pd.DataFrame(Stock_Market,columns=['Year','Month','Interest_Rate','Unemployment_Rate','Stock_Index_Price']) \n",
    "\n",
    "X = df[['Interest_Rate','Unemployment_Rate']] # here we have 2 variables for the multiple linear regression. If you just want to use one variable for simple linear regression, then use X = df['Interest_Rate'] for example\n",
    "Y = df['Stock_Index_Price']\n",
    "\n",
    "X = sm.add_constant(X) # adding a constant\n",
    "\n",
    "model = sm.OLS(Y, X).fit() ### sm, OLS linear regression \n",
    "predictions = model.predict(X) \n",
    "\n",
    "print_model = model.summary()\n",
    "print(print_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adjusted R-squared reflects fit of model from 0 to 1\n",
    "* const coeff - y intercept, y value when coefficients are zero\n",
    "* coefficients for each variable - change in output y from one unit change in x\n",
    "* std error - accuracy of coefficients \n",
    "* can then be plugged in for prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression with House Pricing (df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('df_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>BsmtQual</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>KitchenQual</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>OverallGrade</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>856.0</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1262.0</td>\n",
       "      <td>1262.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>920.0</td>\n",
       "      <td>1786.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1915.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>756.0</td>\n",
       "      <td>1717.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>2198.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OverallQual  YearBuilt  ExterQual  BsmtQual  TotalBsmtSF  GrLivArea  \\\n",
       "0          7.0     2003.0        4.0       4.0        856.0     1710.0   \n",
       "1          6.0     1976.0        3.0       4.0       1262.0     1262.0   \n",
       "2          7.0     2001.0        4.0       4.0        920.0     1786.0   \n",
       "3          7.0     1915.0        3.0       3.0        756.0     1717.0   \n",
       "4          8.0     2000.0        4.0       4.0       1145.0     2198.0   \n",
       "\n",
       "   FullBath  KitchenQual  GarageCars  OverallGrade  SalePrice  \n",
       "0       2.0          4.0         2.0          35.0     208500  \n",
       "1       2.0          3.0         2.0          48.0     181500  \n",
       "2       2.0          4.0         2.0          35.0     223500  \n",
       "3       1.0          4.0         3.0          35.0     140000  \n",
       "4       2.0          4.0         3.0          40.0     250000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OverallQual', 'YearBuilt', 'ExterQual', 'BsmtQual', 'TotalBsmtSF',\n",
       "       'GrLivArea', 'FullBath', 'KitchenQual', 'GarageCars', 'OverallGrade',\n",
       "       'SalePrice'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[['OverallQual', 'YearBuilt', 'ExterQual', 'BsmtQual', 'TotalBsmtSF',\n",
    "       'GrLivArea', 'FullBath', 'KitchenQual', 'GarageCars', 'OverallGrade']]\n",
    "Y = df_train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(X) # adding the constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = sm.OLS(Y,X) # creates a linear regression object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              SalePrice   R-squared:                       0.835\n",
      "Model:                            OLS   Adj. R-squared:                  0.834\n",
      "Method:                 Least Squares   F-statistic:                     732.0\n",
      "Date:                Sun, 29 May 2022   Prob (F-statistic):               0.00\n",
      "Time:                        13:33:52   Log-Likelihood:                -17206.\n",
      "No. Observations:                1458   AIC:                         3.443e+04\n",
      "Df Residuals:                    1447   BIC:                         3.449e+04\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "const        -8.992e+05   8.93e+04    -10.069      0.000   -1.07e+06   -7.24e+05\n",
      "OverallQual   5507.5419   1492.215      3.691      0.000    2580.406    8434.678\n",
      "YearBuilt      392.2864     47.578      8.245      0.000     298.958     485.615\n",
      "ExterQual     1.447e+04   2498.908      5.789      0.000    9564.917    1.94e+04\n",
      "BsmtQual       920.7862   1443.233      0.638      0.524   -1910.266    3751.839\n",
      "TotalBsmtSF     42.1385      2.705     15.581      0.000      36.833      47.444\n",
      "GrLivArea       66.8550      2.624     25.476      0.000      61.707      72.003\n",
      "FullBath     -1.122e+04   2268.762     -4.945      0.000   -1.57e+04   -6768.180\n",
      "KitchenQual   1.147e+04   1981.665      5.788      0.000    7582.652    1.54e+04\n",
      "GarageCars    9314.4359   1565.232      5.951      0.000    6244.069    1.24e+04\n",
      "OverallGrade  1078.1960    150.946      7.143      0.000     782.100    1374.292\n",
      "==============================================================================\n",
      "Omnibus:                      574.228   Durbin-Watson:                   1.967\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5878.203\n",
      "Skew:                           1.540   Prob(JB):                         0.00\n",
      "Kurtosis:                      12.342   Cond. No.                     2.87e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.87e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "model = lin_reg.fit()\n",
    "print_model = model.summary()\n",
    "print(print_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = LinearRegression() # args that can be set (copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) #fit_intercept is the constant\n",
    "regressor.fit(X, Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0.           5507.54189138    392.2863556   14466.78601472\n",
      "    920.78618122     42.13854481     66.85496149 -11218.59562134\n",
      "  11469.89475761   9314.43585305   1078.19597724]\n"
     ]
    }
   ],
   "source": [
    "print(regressor.coef_) # np array, 0 is the constant, can be removed when False - and no p-values harder to read - but more consistent with all other models and models in the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8349492071391901"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.score(X,Y) #R^2 for regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "* target variable is categorical (not numeric) - with at least 2 categories\n",
    "* based on number of categories - can be binary (2 cats) or multinomial classification (3+)\n",
    "* MNIST - for images (computer vision) - classifying hand-written digits\n",
    "* text emails - natural language processing \n",
    "* internet is made up of these two types of text\n",
    "* spam vs real emails\n",
    "* online advertising targetted (click or no click/buy or no buy)\n",
    "* figures out pattern between inputs and targets -> then predict\n",
    "\n",
    "* typically use randomforestclassifier\n",
    "``` python\n",
    "import RandomForestClassifier from sklearn\n",
    "model = RnadomForestClassifier()\n",
    "model.fit(X,Y) #learning\n",
    "predictions = model.predict(X)\n",
    "model.score(X,Y) # to get accuracy\n",
    "```\n",
    "* accuracy/classification rate = #correct/#total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Linear Regression\n",
    "* similar to multinomial - exception is that the features are transformed\n",
    "* still falls under linear regression e.g. b1x1 + b2x1^2 - one variable but the different powers of that var\n",
    "* good for fitting curves in data\n",
    "* class of regression is based on coefficients - which are still linear - only the variables are being transformed\n",
    "* can still be expressed as a linear equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhl_env38",
   "language": "python",
   "name": "lhl_env38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
