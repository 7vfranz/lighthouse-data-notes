{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Lecture\n",
    "#### Training and Evaluation\n",
    "* training to learn underlying concepts - not memorize\n",
    "* allows for generalization from training to test \n",
    "* underfitting - model is too simple, only looks at basic patterns\n",
    "    * high error on training and testing data\n",
    "* overfitting - too complex of a model, matches the training data too perfectly - very little error\n",
    "    * too many parameters/degrees of freedom\n",
    "    * low error on training, high error on testing data\n",
    "* for linear regression compare R2 from train and test\n",
    "    * always want to get higher R2 for test (better accuracy)\n",
    "    * check for overfitting difference between training R2 and test R2 - (better at training than test e.g.)\n",
    "    * underfitting - is just checking test R2 - difference between the two, if model1 has lower R2 than model2 - then model1 might be underfitting \n",
    "* too complex (overfitting) - train R2= 1, and test becomes negative\n",
    "    * not enough data points per features\n",
    "* can plot R2 scores per complexity on train and test\n",
    "* main goal is to get a good R2 score on test - try to maximize performance on test\n",
    "\n",
    "#### Sampling Bias\n",
    "* when data comes from different distribution than the new data (test)\n",
    "* e.g. studying past exams test on  new exams with different teacher\n",
    "* old time periods to new time periods,  different locations, sexes, phone/email/polling data\n",
    "\n",
    "#### Cross validation\n",
    "* get different training and test set everytime\n",
    "* smaller data sets = more likely to suffer from sampling bias\n",
    "* vary which data you are using as test data- then average the resulting error (loss function)\n",
    "* if high standard deviation of variance - we probably need more data, lots of variation between errors\n",
    "* take fifth of every data 'k-fold' (20%) until you use 100% of the data as your test\n",
    "* other method: leave out 1, take test everytime but with different \n",
    "* average R2 - because there's multiple test-data used and cross-validated on - more accurate measure of R2 \n",
    "* if model is not hard to run - cross validation can be done easily\n",
    "* done manually or through KFold on sklearn\n",
    "* more robust estimate of true sample error\n",
    "\n",
    "#### After cross-validating\n",
    "* none of them are actually used - but gives us estimate of how model will perform\n",
    "* but now we can train our model on the entire data set\n",
    "* based on cross-validation\n",
    "\n",
    "#### Hyperparameter tuning\n",
    "* parameters set - that are not learned during training \n",
    "* usually done via guessing and checking - try hyperparameters and set to whatever gives the lowest error\n",
    "* common techniques: grid search (looks at every possible combination), random search (selects at random based on some boundary)\n",
    "* can evaluate which achieves the lowest loss  can be done with cross validation\n",
    "* can become computationally expensive \n",
    "* other advanced techniques: bayesian optimization, genetic algorithms\n",
    "* goal is choose the best model: first step choose the right model, and then choose the right hyperparameters to fine tune even more\n",
    "* hyperparameters is being trained on cross validation data - somewhat overfitting\n",
    "* can do an initial split again with train/test, and then subset train again for k-folds\n",
    "* create a 'validation' dataset so hyperparameters aren't being trained on **any** test data  \n",
    "\n",
    "\n",
    "#### Classification\n",
    "* same thing can be done for categorical variables\n",
    "* evaluate using accuracy/precision - predicted vs actual (see lecture for more details)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Compass\n",
    "* Validation - deciding whether numerical results about the relationships between variables are acceptable \n",
    "* common types: holdout, k-fold, leave one out, bootstrap\n",
    "* Cross Validation - testing performance of the model on 'unseen' data\n",
    "    * holdout method (splitting training and test)\n",
    "    * k-fold cross validation \n",
    "\n",
    "Holdout Method\n",
    "* split data for train and test\n",
    "* test and 'unseen' data must be similar, test must be close approximation of unseen\n",
    "* a lot rides on the test data \n",
    "* representation of training and test - may not be equal \n",
    "* instead we can use k-fold Cross Validation\n",
    "\n",
    "K-Fold Cross Validation\n",
    "* essentially repeating the holdout method k times, k subsets of the data gets used as test/validation set\n",
    "* creates and evaluates multiple models on multiple subsets of the dataset\n",
    "* reduces selection bias \n",
    "* reduces bias and variance as most of the data is being used in validation\n",
    "* general rule of 5-10 Ks\n",
    "* creating a 'population' of performance measures (loss functions)\n",
    "* we can then calcualte the mean/st.d of these measures to get an idea of how well the procedure does on average and how much they vary\n",
    "* resampling method\n",
    "* goal is find a model that has maximized estimated skill\n",
    "* once a model is finalized - saved for us - and make predictions on new data\n",
    "* cross-validation models and train-test datasets can be discarded\n",
    "* final model is then trained on all available data\n",
    "\n",
    "Stratified K-fold cross validation\n",
    "* each fold contains same percentage of samples of each target class as the complete set (\n",
    "* e.g. more blues than reds in dataset, will bias classification to blue\n",
    "* ensures equal representation in training data set\n",
    "* these are non-exhaustive cross validation - do not compute all ways of splitting the data, number of subsets is decided \n",
    "\n",
    "**Exhaustive Methods of Cross Validation**\n",
    "\n",
    "Leave-P-out Cross validation\n",
    "* leaves p number of data points out of training data \n",
    "* n-p used for validation set\n",
    "* repeated for all combinations in which original sample can be separated \n",
    "* similar to k-fold but specific case, takes all data except 1, and calculate average error \n",
    "* good way to validate because each data point becomes part of validation, but very intensive because must compute every time\n",
    "* popular: p=1, \"Leave one out cross validation\n",
    "    * intensive computation\n",
    "    * number of possible combinations = number of data points\n",
    "* useful technique for getting effectiveness of model\n",
    "* mitigates overfitting, and deciding hyper parameters \n",
    "\n",
    "**Best Practices**\n",
    "* K-fold cross validation \n",
    "    * for small amount of data, and can't afford to drop values\n",
    "* Holdout set and Cross Validation\n",
    "    * splits data into train/test\n",
    "    * apply cross-validation on training set to get optimal model\n",
    "    * test model on test set to see performance on \"new\"/\"unseen\" data\n",
    "\n",
    "\n",
    "Bootstrap Methods\n",
    "* randomly draw datasets from the training sample - with replacement\n",
    "* data point can be selected more than once\n",
    "* each sample is same size as the training sampple\n",
    "* refit the model with bootstrap samples\n",
    "* examine the model\n",
    "\n",
    "\n",
    "#### Overfitting\n",
    "* noise interferes with signal, algorithm that's too complex ends up memorizing the noise instead of finding the signal\n",
    "* 'goodness of fit' - linked to approximation error\n",
    "    * how closely a model's predicted values match the observed (true) values)\n",
    "* Overfit\n",
    "    * model that learned the noise instead of the signal is considered overfit \n",
    "    * more variance in their predictions\n",
    "    * too high = too complex\n",
    "* Underfitting - model is too simple\n",
    "    * less variance in their predictions - but more biased towards wrong outcomes\n",
    "    * too much bias - too simple \n",
    "* want to get sweet spot  with low bias and low variance\n",
    "* Detecting overfitting\n",
    "    * can't know until testing\n",
    "    * can holdout method to train-test split to see performance on 'new' data\n",
    "    * does well on training but sig drop off at test data\n",
    "    * start with a simple model and make more complex\n",
    "* Preventing overfitting\n",
    "    * 1. cross-validation - use initial training data to generate multiple mini train-test splits, use these to tune your model\n",
    "    * standard k-fold cross validation - partition the data into k subsets (folds)\n",
    "    * iterate on folds while holding out as a test \n",
    "    * 2. train with more data, - can help algorithms detect the signal better\n",
    "    * make sure data you are adding is relevant and clean\n",
    "    * 3. Remove features\n",
    "    * removing irrelevant input features\n",
    "    * make sure the features make sense \n",
    "    * 4. Early stopping\n",
    "    * how well each iteration performs, stop training process early\n",
    "    * usually for deep learning\n",
    "    * 5. Regularization\n",
    "    * used commonly for classical machine learning\n",
    "    * forcing model to be simpler (ridge, lasso)\n",
    "    * pruning decision tree, dropout on neural network, penalty parameters on the cost function in regression\n",
    "    * can be tuned through cross validation (because it is a hyperparameter)\n",
    "    * 6. Ensembling\n",
    "    * combining predictions from multiple separate models\n",
    "    * Bagging - reduce change - trains a large number of strong learners in parallel\n",
    "    * combines all 'strong' learners together to smooth out predictions\n",
    "    * strong to smooth \n",
    "    * Boosting - trains a large number of weak learners (constrained model)\n",
    "    * each one in the sequence focuses on learning from mistakes of the one before it\n",
    "    * combines all weak learners into a single strong learner \n",
    "    * simple to complex (boost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walkthrough - Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# initialize a list\n",
    "X = list(range(10))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "source": [
    "# squared values with list comprehension\n",
    "y = [x*x for x in X]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  [4, 9, 3, 5, 7, 6, 1]\n",
      "y_train:  [16, 81, 9, 25, 49, 36, 1]\n",
      "X_test: [8, 2, 0]\n",
      "y_test:  [64, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "# splitting the data to train and test\n",
    "import sklearn.model_selection as model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.75,test_size=0.25, random_state=101)\n",
    "print (\"X_train: \", X_train)\n",
    "print (\"y_train: \", y_train)\n",
    "print(\"X_test:\", X_test)\n",
    "print (\"y_test: \", y_test)\n",
    "X_train:  [4, 9, 3, 5, 7, 6, 1]\n",
    "y_train:  [16, 81, 9, 25, 49, 36, 1]\n",
    "X_test:  [8, 2, 0]\n",
    "y_test:  [64, 4, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note by default the train_test_split doesn't follow the same ascending order\n",
    "# i.e. the split shuffles the data, shuffle=True (by default) can be False\n",
    "# this does the same thing as above - old way\n",
    "import sklearn.model_selection as cross_validation\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, train_size=0.75, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using KFold cross validation\n",
    "* note that a 0.25 subset of X is used as a test\n",
    "* shuffle is default False for KFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test:  [2 8]\n",
      "X_test:  [4 9]\n",
      "X_test:  [0 7]\n",
      "X_test:  [1 3]\n",
      "X_test:  [5 6]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"X_test: \", X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the score of each of your cross validation\n",
    "* can use model_selection.cross_val_score  - see tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class imbalance\n",
    "* over representation of one class of data, not equally distributed\n",
    "* e.g. fraud prediction - 1% of frauds, and the rest are normal observations, Churn/NoChurn\n",
    "* models will be biased in selecting normal observations than 1% frauds\n",
    "* can use 'resampling' \n",
    "* can start with undersampling the class with bigger occurrence\n",
    "* remember to always test the model on a test set with the right proportions must have correct representation\n",
    "\n",
    "Ways to Combat\n",
    "* 1. collect more data, more examples of minor classes\n",
    "* 2. change performance metric. - can't use accuracy here\n",
    "    * confusion matrix\n",
    "        * showing correct (diagonal) and incorrect \n",
    "    * precision - measure of classifier exactness\n",
    "    * recall - measure of classifier completeness (out of trues, which one was right)\n",
    "    * F1 Score (F-score) weighted average of precision and recall\n",
    "    * Cohen's kappa\n",
    "    * ROC curves\n",
    "* 3. Resampling the data\n",
    "    * more balanced data\n",
    "    * add copies of instances from underrepresented class (oversampling) (< 10k data)\n",
    "    * delete instances from over-represented class called under sampling (100k+ data)\n",
    "    * stratified sampling\n",
    "    * resampled ratios \n",
    "* 4. Synthetic samples\n",
    "    * naive bayes to sample each attribute independently when run in reverse\n",
    "    * data non-linear relations may not be preserved\n",
    "    * SMOTE (synthetic minority over-sampling technique) - instead of creating copies it creates synthetic samples\n",
    "* 5. Try different algorithms\n",
    "    * decision trees are often good for imbalanced data sets\n",
    "    * if in doubt try a random forest\n",
    "* 6. Penalized models\n",
    "    * give same models a different perspective on the problem\n",
    "    * SVM, LDA have penalized version\n",
    "* 7. Different perspective\n",
    "    * anomaly or change detection\n",
    "    * anomaly detection of rare events - detecting outliers\n",
    "    * change detection - looks for change/difference e.g. bank transactions\n",
    "* 8. creativity\n",
    "    * break down to smaller problems \n",
    "    * different approahces possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Bias\n",
    "* the way you find your respondents affects the questions you can ask them\n",
    "* who you are asking will bias the response you will get \n",
    "* some members of intended population have lower sampling probability\n",
    "* sample is not representative of the population you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "* quantifying model performance\n",
    "* depends on machine learning task, such as classification, regression, or clustering\n",
    "* Regression\n",
    "    * MSE (mean squared)\n",
    "        * most preferrred\n",
    "        * preferred more than other metrics \n",
    "        * can be optimized easier \n",
    "    * Root mean sqaures error (RMSE)\n",
    "        * square root of averaged squared difference\n",
    "        * leads to high penalty for large errors - useful when you don't want large errors \n",
    "    * mean abs error (MAE)\n",
    "        * absolute difference between actual and predicted\n",
    "        * robust to outliers (doesn't penalize errors as much)\n",
    "        * all weighted equally (linear)\n",
    "        * bad if you want to look at outliers \n",
    "    * R2 - coefficient of determination\n",
    "        * 1-MSE(model)/MSE(baseline)\n",
    "        * compares current model with constant baseline - and how much our model is better \n",
    "        * baseline is just the mean of the data \n",
    "        * scale free - less or equal to 1\n",
    "        * can be negative (neg infinity to 1) - usually means the trend doesn't actually follow the data \n",
    "        * MSE of baseline is actually lower than MSE of the model (numerator) - high numerator (> 1)\n",
    "            * could be due to outliers, intercept is missing \n",
    "    * adjusted R2\n",
    "        * *doesn't actually adjust the model* - but adjusts the R2\n",
    "        * always lower than R2 because it's a penalty for number of predictors \n",
    "\n",
    "* Classification\n",
    "    * actual vs predicted - get TP, FP, FN, TN. T/F is actual, P/N is prediction \n",
    "    * accuracy\n",
    "        * TP+TN / TP + FP + FN + TP\n",
    "        * may not always be what you want\n",
    "        * can be very accurate if labeled everyone Negative - if TP is really rare or it's what you care about detecting \n",
    "        * for example COVID+ people\n",
    "    * recall\n",
    "        * better evaluation of how model performs in finding true positives\n",
    "        * gives fraction of correctly identified positive out of all the positive \n",
    "        * Recall = TP / TP + FN \n",
    "        * note false negative - meaning labeled negative, but actually positive\n",
    "        * falls short if you  end up with  a lot of false positives - ignored by calculation\n",
    "    * precision\n",
    "        * TP/ TP+FP \n",
    "        * fraction of correctly identified positive out of all labeled positive \n",
    "        * corrects against false positives, will have low score \n",
    "        * good when detecting rare TP cases, and have good way of detecting TNs \n",
    "        * leads to a precision/recall trade off \n",
    "    * summary:\n",
    "        * all negative - accuracy: high, recall: low, precision: low\n",
    "        * all positive - accuracy: low, recall: high, precision: low\n",
    "        * max probability as positive: acc: high, rec: low, prec: low\n",
    "        * increase in precision, reduces recall (and vice versa)\n",
    "        * depends on problem if you want to maximize detection of TP, and don't care about false positives for e.g. \n",
    "    * precision/recall tradeoff\n",
    "        * can strategize to predict positive if output reliability is > 0.3 (higher recall, lower precision) - maximize positive detection\n",
    "        * can strategize for higher precision - predict positive if output reliability >0.9 (low recall, high precision) \n",
    "    * F1-score\n",
    "        * combines precision and recall scores into one \n",
    "        * 2* precision*recall / precision + recall\n",
    "        * harmonic mean - less sensitive to large values (1 vs 0 will average as 0.5)\n",
    "        * good when you have similar precision/recall \n",
    "    * ROC curve/AUC score\n",
    "        * receiver operator characteristic \n",
    "        * classification problems with probability outputs \n",
    "        * convert probability outputs to classifications to be more or less selective of what's labeled positive/negative\n",
    "        * ROC plots false positive rate vs true positive rate \n",
    "        * adjusting threshold can see effects on TPr and FPr\n",
    "        * plot TPr and FPr against each other - will get a curve, find area under the curve (AUC)\n",
    "        * largest AUC means it increases more on TPr while FPr remains low\n",
    "    * Lift/Lift-curve [revisit] (https://algolytics.com/tutorial-how-to-establish-quality-and-correctness-of-classification-models-part-5-lift-curve/)\n",
    "        * pictures gains from applying a classifer for a section of data compared to a random classifer\n",
    "        * with percentage scale (accumulated/nonaccumulated)\n",
    "        * with quotient scale (acc/nonacc)\n",
    "        * first sort data (descending) \n",
    "        * divide into quantiles - number of quantiles is important, too small - non-reliable, too large and not detailed enough \n",
    "        * plots density of positive observations over quantiles (step function going down)\n",
    "        * accumulated - will show more a curve, non-accumulated lift will be more stepwise \n",
    "        * can get a quotient scale\n",
    "        * (??) needs revisiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_true = np.random.normal(0,1,10)\n",
    "# generate random errors\n",
    "errors = np.random.normal(0,0.02,10)\n",
    "y_pred = y_true + errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004382144625764541\n"
     ]
    }
   ],
   "source": [
    "# import MSE from sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# compute MSE takes two arrays, true values and prediction values\n",
    "MSE = mean_squared_error(y_true,y_pred)  \n",
    "# print MSE\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020933572618558306\n",
      "0.020933572618558306\n"
     ]
    }
   ],
   "source": [
    "# for RMSE\n",
    "# RMSE by Numpy\n",
    "RMSE = np.sqrt(MSE)\n",
    "print(RMSE)\n",
    "# RMSE by sklearn\n",
    "RMSE = mean_squared_error(y_true,y_pred,squared=False)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note ONLY for binary\n",
    "# ground truth\n",
    "y_true = [1,1,0,1,0,0,1,0,0,1]\n",
    "\n",
    "# simulate probabilites of positive class - # probability of observations\n",
    "y_proba = [0.9,0.7,0.2,0.99,0.7,0.1,0.5,0.2,0.4,0.6]\n",
    "\n",
    "# set the threshold to predict positive class - anything above will be labelled as positive \n",
    "thres = 0.5\n",
    "\n",
    "# class predictions\n",
    "y_pred = [int(value > thres) for value in y_proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# for accuracy \n",
    "# import accuracy_score from sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = accuracy_score(y_true,y_pred)\n",
    "\n",
    "# print accuracy\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8000000000000002\n"
     ]
    }
   ],
   "source": [
    "# for f1_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# compute F1-score\n",
    "f1_score = f1_score(y_true,y_pred)\n",
    "\n",
    "# print F1-score\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "# ROC_AUC scores - ** uses probabilities instead of class labels, predictions with binary will give inaccurate scores (y in auc score is probability)\n",
    "# import roc_auc_score from sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# compute AUC-score\n",
    "auc = roc_auc_score(y_true,y_proba)\n",
    "\n",
    "# print AUC-score\n",
    "print(auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search in sklearn\n",
    "* used for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import datasets, svm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digit data - darkness of pixel in an 8x8 image of handwritten digit\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "        15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "        12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "         0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "        10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the features of the first observation - handwritten 0\n",
    "digits.data[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data for later cross validation\n",
    "# Create dataset 1\n",
    "data1_features = digits.data[:1000]\n",
    "data1_target = digits.target[:1000]\n",
    "\n",
    "# Create dataset 2\n",
    "data2_features = digits.data[1000:]\n",
    "data2_target = digits.target[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_candidates = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=SVC(), n_jobs=-1,\n",
       "             param_grid=[{&#x27;C&#x27;: [1, 10, 100, 1000], &#x27;kernel&#x27;: [&#x27;linear&#x27;]},\n",
       "                         {&#x27;C&#x27;: [1, 10, 100, 1000], &#x27;gamma&#x27;: [0.001, 0.0001],\n",
       "                          &#x27;kernel&#x27;: [&#x27;rbf&#x27;]}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=SVC(), n_jobs=-1,\n",
       "             param_grid=[{&#x27;C&#x27;: [1, 10, 100, 1000], &#x27;kernel&#x27;: [&#x27;linear&#x27;]},\n",
       "                         {&#x27;C&#x27;: [1, 10, 100, 1000], &#x27;gamma&#x27;: [0.001, 0.0001],\n",
       "                          &#x27;kernel&#x27;: [&#x27;rbf&#x27;]}])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=SVC(), n_jobs=-1,\n",
       "             param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
       "                         {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001],\n",
       "                          'kernel': ['rbf']}])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out all combination of parameters that gets the highest score - default is 3-fold KFold \n",
    "# Create a classifier object with the classifier and parameter candidates\n",
    "clf = GridSearchCV(estimator=svm.SVC(), param_grid=parameter_candidates, n_jobs=-1)\n",
    "\n",
    "# Train the classifier on data1's feature and target data\n",
    "clf.fit(data1_features, data1_target)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for data1: 0.966\n"
     ]
    }
   ],
   "source": [
    "# check the accuracy \n",
    "# View the accuracy score\n",
    "print('Best score for data1:', clf.best_score_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 10\n",
      "Best Kernel: rbf\n",
      "Best Gamma: 0.001\n"
     ]
    }
   ],
   "source": [
    "# View the best parameters for the model found using grid search\n",
    "print('Best C:',clf.best_estimator_.C) \n",
    "print('Best Kernel:',clf.best_estimator_.kernel)\n",
    "print('Best Gamma:',clf.best_estimator_.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhl_env38",
   "language": "python",
   "name": "lhl_env38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
