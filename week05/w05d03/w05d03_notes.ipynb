{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Lecture\n",
    "**Decision trees**\n",
    "* flow chart based on features\n",
    "* programmed using if-else statements - done automatically in an optimal fashion\n",
    "* 'classification tree'\n",
    "* best way to split data \n",
    "* roots - internal - leaf nodes (lowest - contains decision)\n",
    "* goes down a level each\n",
    "* splitting data along boundary lines on features, x < x1 - representing a root/internal node\n",
    "* each region represents a leaf node\n",
    "* can be bad for overfitting compared to logistic regression\n",
    "* maximum depth - can be too high and will create region for every single data point\n",
    "* easy to interpret and visualized, requires little data preparation, can work with NaNs, doesn't require normalization\n",
    "* handles multi-class classification well, can print out decision tree\n",
    "* Cons: tendency to overfit (prune number of leafs), can be unstable little changes makes it unstable\n",
    "* each node is locally optimized (not global) - just looks at next step\n",
    "\n",
    "**Deciding the optimal split**\n",
    "* proportions after the split - a good amount of the data is chosen\n",
    "* population: gini, entropy, missclassification\n",
    "* Gini impurity\n",
    "    * lower score is better\n",
    "    * lowest at very low and very high proportions\n",
    "    * sum of proportion for each class\n",
    "    * basically loss function\n",
    "    * maximum is 0.5 (for 2 classes)\n",
    "\n",
    "* **Regression Trees**\n",
    "    * calculated on values in each region based on: loss-function MSE, MAbsoluteE, half-poisson deviance\n",
    "    * minimized across all possible splits\n",
    "    * regression trees vs linear regression - regression trees give a stepwise function that splits the data to two different groups\n",
    "    * good over logistic when data is non-linear (go back to feature selection/engineering, or verify with PCA)\n",
    "    * can perform both to see how they do on the test data\n",
    "\n",
    "* **Random Forest**\n",
    "    * address overfitting in decision trees - but reduces interpretability\n",
    "    * fit diverse set of trees - and inject randomness\n",
    "    * use the most common/or average of all the presictions as our single prediction\n",
    "    * creates multiple decision trees (i.e. forest) \n",
    "    * get the most common prediction from all the trees\n",
    "    * 1. injecting randomness - creating 'bootstrap samples' - build a tree for each bootstrap sample\n",
    "    * select data 'with replacement' meaning a data point can be selected twice in the bootstrap sample\n",
    "    * 2. at each split - consider only a random subset of the features, first decision could be only on feature 1,2 and not all 1,2,3,4\n",
    "    * averaging many over-fitted models reduces variance - can get a more generalized model (by taking the average)\n",
    "    * improves accuracy - more accurate when compared to decision trees, one of the best performing classifiers\n",
    "    * slower - trains multiple trees, but can be done in parallel because they are independent\n",
    "    * reduce overfitting, interpretability reduced - don't know which tree \n",
    "    * number of bootstrap samples to use is a hyperparameter to optimize \n",
    "\n",
    "* **Ensemble Methods**\n",
    "    * taking multiple (weak models) combined to improve results\n",
    "    * 1. Bagging\n",
    "        * bootstrapping/aggregating, combined in deterministic process\n",
    "        * addresses over-fitting\n",
    "    * 2. Boosting\n",
    "        * adding one model at a time that addresses the shortcomings of the current ensemble (iterative process) \n",
    "        * aggregation (averaging) is done *during training* not after\n",
    "        * addresses under-fitting\n",
    "    * 3. Stacking\n",
    "        * using a variety of weak models as input to a 'meta-model'\n",
    "        * similar to bagging - but can use different types of models\n",
    "\n",
    "* **Boosting** \n",
    "    * AdaBoost (Adaptive Boosting)\n",
    "        * all models have the same weight\n",
    "        * takes what is wrongly classified - and adds weight to those points\n",
    "        * can also add more weight to the models themselves\n",
    "    \n",
    "See walkthrough from lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From compass - Ensemble Algorithms and Decision Trees\n",
    "\n",
    "**Ensemble Algorithms**\n",
    "* better prediction than linear logsitic\n",
    "* more of a black box\n",
    "\n",
    "**Decision Trees**\n",
    "* classification and regression task\n",
    "* MUST know \n",
    "* approximate curve based on a set of if/then/else rules\n",
    "* breaks down dataset to smaller and smaller subsets until the tree is developped\n",
    "* ending with decision nodes and leaf nodes\n",
    "* top = root\n",
    "* between = nodes/internal nodes (points to and points out)\n",
    "* leaf nodes = last, points to but not from out\n",
    "* splits on one variable (root) - will have 'impurity' meaning a mix of yes/no on your target \n",
    "* impurity measured by Gini (Gini impurity)\n",
    "* Gini = 1 - (probability of yes^2) - (probability of no^2)\n",
    "* measured for each yes/no for node\n",
    "* total Gini impurity for one node/factor = weighted average of Gini impurities for the leaf nodes (average * Gini impurity)\n",
    "* lowest impurity means it separate the data the best - and gets used as the root of the decision tree\n",
    "* repeat for each factor or node \n",
    "* if impurity is lower after a split, than splitting again - that node becomes a leaf node - doesn't split any further \n",
    "* can also be done with numerical data \n",
    "    * first sort the data by lowest to greatest\n",
    "    * calculate average of adjacent scores \n",
    "    * calculate the impurity for each average weight\n",
    "    * lowest gini from those averages is used as cut off for impurity values\n",
    "* for ordinal/rank data\n",
    "    * calculates gini scores based on all possible rank\n",
    "    * for things like color - we need to also calculate the combinations, red or green, blue or green etc. \n",
    "* summary: when selecting nodes - you're selecting the ones with the lowest impurity - splits the data on the target as much as possible\n",
    "\n",
    "**More on Trees**\n",
    "* non-parametric and distribution-free - doesn't depend on probability distributions\n",
    "* can handle high dimensional data\n",
    "* faster, and more white-box - can see exactly what decision is on\n",
    "* how it works:\n",
    "    * select best attribute to split data (attribute selection measures ASM)\n",
    "        * popular measures: information gain, gain ratio, gini index\n",
    "        * information gain - decrease in entropy\n",
    "            * difference in entropy before and after the split \n",
    "            * biased when there's many outcomes for an attribute, for example IDs will split the data too much\n",
    "        * gain ratio\n",
    "            * normalizes the information gain using SplitInfo\n",
    "        * Gini index\n",
    "            * considers a binary split for each attribute\n",
    "            * attribute with minimum gini is chosen as splitting attribute \n",
    "    * make attribute a decision node - break the data to smaller subset\n",
    "    * build the tree by repeating this process until:\n",
    "        * all tuples belong to the same attribute value\n",
    "        * no more remaining attributes\n",
    "        * no more instances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree classifier in scikit learn\n",
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "# load dataset\n",
    "pima = pd.read_csv(\"diabetes 2.csv\", header=None, names=col_names)\n",
    "pima = pima.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>glucose</th>\n",
       "      <th>bp</th>\n",
       "      <th>skin</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pregnant glucose  bp skin insulin   bmi pedigree age label\n",
       "1        6     148  72   35       0  33.6    0.627  50     1\n",
       "2        1      85  66   29       0  26.6    0.351  31     0\n",
       "3        8     183  64    0       0  23.3    0.672  32     1\n",
       "4        1      89  66   23      94  28.1    0.167  21     0\n",
       "5        0     137  40   35     168  43.1    2.288  33     1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima.label # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.670995670995671\n"
     ]
    }
   ],
   "source": [
    "# checking accuracy \n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning parameters to improve accuracy\n",
    "# first graph the data using export_graphviz - converts tree into a dot file, pydotplus turns it to png\n",
    "# not working from tutorial - see lecture notebook to see how to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing Decision Tree performance\n",
    "* criterion: choose attribute selection measure, default is gini, 'entropy' for information gain\n",
    "* splitter: default is 'best', chooses the split strategy , can be 'random'\n",
    "* max_depth: into or None, default is 'None'. nodes expanded until all leaves contain less than min_samples_split\n",
    "    * higher depth causes overfitting, lower = underfitting \n",
    "* max_depth is most commonly used to control parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7705627705627706\n"
     ]
    }
   ],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3) # reducing depth increased accuracy - less complex\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros: \n",
    "* easy to interpret\n",
    "* can be non-linear patterns\n",
    "* not a lot of preprocessing, no normalization required\n",
    "* suitable for variable selection\n",
    "* no assumptions\n",
    "Cons:\n",
    "* sensitive to noisy data - can overfit\n",
    "* small variations can create a different tree (improved with bagging or boosting)\n",
    "* biased with imbalance dataset - make sure data is balanced "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "* flexible and easy to use\n",
    "* similar to decision tree - no hyperparameter tuning necessary\n",
    "* most frequently used due to simplicity and diversity \n",
    "* used in both classification and regression \n",
    "* addresses con of decision tree not good for classifying new samples ~ tend to be overfitted\n",
    "* how to random forest :\n",
    "    * bootstrap dataset - sample with replacement\n",
    "    * create a decision tree with each bootstrapped data BUT only use a subset of features for each step - not all - e.g. randomly select 2\n",
    "    * poll each tree - see decision from each tree - see which option received more votes\n",
    "    * 'bagging' **b**ootstrapping the data then **agg**regating \n",
    "    * 1/3 of data usually doesn't end up in the bootstrap - that data is 'out-of-bag' dataset, more data will be out with more samples\n",
    "    * not included in bootstrapping - to avoid overfitting? \n",
    "    * can calculate 'out of bag error' - see how accurate the random forest is based on proportion of OOB samples that were correctly classified \n",
    "    * compare OOB from 2 feature vs 3 feature - select what's best\n",
    "    * essentially: build random forest, estimate accuracy, change number of variables used per step - choose best accuracy\n",
    "    * use the square of the number of variables - then try settings below that to find optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Learning\n",
    "* combining models \n",
    "* done by:\n",
    "    * bootstrapping and bagging as in random forests\n",
    "    * boosting - adaptive and gradient boosting\n",
    "    * stacking \n",
    "\n",
    "Single Weak learner\n",
    "* choice of model varies depending on the problem: quantity of data, dimensionality, distribution\n",
    "* comes back down to bias-variance tradeoff\n",
    "* weak learners/base models - poor accuracy, usually have high bias, or too much variance\n",
    "* try to reduce bias/variance of weak learners by combining several of them together to create either a strong/ensemble model\n",
    "\n",
    "Combining weak learners\n",
    "* homogenous - use the same base model/algorithm that are trained in different ways\n",
    "* heterogenous - different types of base learning algorithms \n",
    "* when combining methods - pick ones that complement each other (one that overfits + one that underfits) (high bias with low bias)\n",
    "* goal is to reduce bias and reduce variance\n",
    "* Falls in three categories\n",
    "    * Bagging\n",
    "        * typically homogeneous weak learners\n",
    "        * independent learning then combined via some averaging process\n",
    "        * usually aimed to reduce variance (more accurate at test)\n",
    "        * bootstrapping and aggregating, producing an ensemble model that is more robust\n",
    "    * Boosting\n",
    "        * usually homogeneous\n",
    "        * learns sequentially in an adaptive way\n",
    "        * next model depends on previous ones\n",
    "        * usually try to reduce bias (overfitting, between test and train)\n",
    "    * Stacking\n",
    "        * heterogenous weak learners\n",
    "        * learning in parallel and combining them on a meta-model\n",
    "        * prediction depends on a bunch of different weak model predictions \n",
    "        * usually try to reduce bias (overfitting)\n",
    "\n",
    "Bagging\n",
    "* generate bootstrap samples from initial data set by randomly drawing with replacement \n",
    "* samples have good statistical properties - essentially making representative/independent samples of true data distribution\n",
    "* relies on:\n",
    "    * representativity\n",
    "        * size of N (initial dataset) should be large enough to get a good distribution\n",
    "        * good approximation of sampling from real distribution\n",
    "    * independence\n",
    "        * size of N should be large enough compared to the size B (bootstrap samples)\n",
    "        * otherwise just sampling the same thing? \n",
    "        * ensures the samples are not correlated with each other\n",
    "* allows for statistical estimation by creating 'almost representative' and 'almost independent' samples\n",
    "* way of handling getting more independent samples - to fit several models \n",
    "* then average their predictions or do a majority vote \n",
    "* for regression can be actually averaged\n",
    "* for classification - can have votes (hard voting) or soft-voting with highest probabilities \n",
    "* can be done in parellel - as the models are being fitted independently \n",
    "* see random forests for example - random forests also randomly samples on features to reduce correlation between outputs of each tree\n",
    "\n",
    "Boosting\n",
    "* fit models iteratively - such that training model at given step depends on models from previous step\n",
    "* used for both regression and classification\n",
    "* main focus is reducing bias \n",
    "* base models are typically ones with low variance but high bias\n",
    "    * these models are easier to fit\n",
    "    * models can't be done in parallel, so a sequence of complex models will be computationally expensive\n",
    "* **Adaboosting**\n",
    "    * in decision tree starts with trying to different ways to split\n",
    "    * updates the weights attached to each training data set\n",
    "    * ensemble model as weighted sum of L weak learners \n",
    "    * iterative optimization process, adds weak learners one by one\n",
    "    * evaluates the loss function - and tries to optimize locally after adding each model\n",
    "    * updates the observation weights - those that are misclassified or mis-predicted hold more weight\n",
    "    * adds the weak learner to the weighted sum based on a coefficient\n",
    "    * the better weak learner contributes more to the strong learner\n",
    "* **Gradient boosting**\n",
    "    * in decision tree starts with number of final leaves\n",
    "    * updates the values of the observations \n",
    "    * uses gradient descent\n",
    "    * at each iteration we fit a weak learner to the opposite of the gradient\n",
    "    * calculate 'pseudoresiduals' gradient function applied to data set (at the beginning this is the same as the actual values)\n",
    "    * pseudoresiduals - error from prediction by model and observed\n",
    "        * builds a tree based on those residuals (tree might have different features each time)\n",
    "        * error is average of the residuals in that leaf\n",
    "        * scaled by some 'learning rate'\n",
    "    * this process of getting pseudoresiduals is repeated/iterated - from the previous predictions\n",
    "    * residuals should decrease with each iteration/tree made\n",
    "    * tries to fit weak learner to the pseudoresiduals\n",
    "    * considered as a generalization of adaboost to 'arbitrary' differentiable loss functions (derivable)\n",
    "    * see towardsdatascience.com for more info\n",
    "    * in regression this starts with average of target variable\n",
    "\n",
    "Stacking \n",
    "* difference from last two is it considers heterogenous weak learners\n",
    "* different algorithms combined\n",
    "* combines based models to use a 'meta-model'\n",
    "* steps:\n",
    "    * split the data to two folds\n",
    "    * choose L weak learners fit on the first fold\n",
    "    * for each L weak learner - predicts value on the second fold\n",
    "    * fits a meta model on the second fold - based on predictions of the weak learners\n",
    "* multi-level stacking\n",
    "    * L-weak learners predictions fed into a number of meta-models then those predictions get fed into a final meta-model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XG Boost (eXtreme gradient boosting)\n",
    "* widely used predictive model\n",
    "* speed and accuracy\n",
    "* can be more effective than Deep NN\n",
    "* minimize training time and has good accuracy\n",
    "* used for large and complex data sets\n",
    "* based on gradient boost \n",
    "    * in decision tree\n",
    "    * sets the number of final leafs\n",
    "    * finds a way to split the data, and does so many times to improve fit or reach max iterations\n",
    "\n",
    "#### For Regression\n",
    "* uses a XGboost tree instead of a standard regression tree (as in normal gradient boost)\n",
    "* calculates **similarity score** \n",
    "    * very similar to MSE\n",
    "    * calculated by (sum of residuals)**2, divided by n residuals + lambda(hyperparameter)\n",
    "    * except sums all residuals first before squaring (allows for cancelling out of residuals)\n",
    "    * note that similarity score is large when residuals are similar, or there is just one of them, and they don't cancel out\n",
    "* then splits the data on a feature, to get residuals after the split \n",
    "* calculate the similarity score for each leaf again\n",
    "* **gain**\n",
    "    * compare similarity scores between leafs and root\n",
    "    * gain = left sim + right sim - root sim\n",
    "* shift threshold to split on, get gain for that tree\n",
    "* shift threshold until we can't anymore - no longer splits the data\n",
    "* larger gain = means better at splitting the data - so we use that split\n",
    "* then we repeat for the next node that can be split (if no longer able to split - it becomes a leaf)\n",
    "    * compare similarity and gain again for the branch\n",
    "* default amount of levels = 6\n",
    "* for less complicated data - used a lower levels\n",
    "* **Pruning**\n",
    "    * XG boost trees prunes based on its gain values \n",
    "    * start with value 'gamma'\n",
    "    * calculate difference between (gain at lowest branch - gamma)\n",
    "    * if negative - remove branch\n",
    "    * if positive stays\n",
    "    * removal bottom up - from branches all the way to the root\n",
    "    * if branch is not removed - even if root is negative it is not removed \n",
    "    * can actually result in removal of the whole tree\n",
    "    * gamma to zero does NOT turn off pruning - gain can be negative and can still be pruned \n",
    "* **lambda**\n",
    "    * regularization parameter\n",
    "    * reduce prediction sensitivity to individual observations (dividing by more for calculating similarity score)\n",
    "    * lowers the similarity score\n",
    "    * when lambda  is greater than 0 - lowers gains - becomes easier to prune trees based on gamma \n",
    "    * inversely proportional to number of residuals in a leaf (lower number of residuals in a leaf = greater reduction by lambda)\n",
    "    * prevents overfitting the data\n",
    "* similar to gradient boost\n",
    "    * makes intial tree with initial predictions \n",
    "    * add the output of the Tree scaled by a learning rate ('eta') - default 0.3\n",
    "    * new prediction = original prediction (just the mean) + 0.3 * output of tree\n",
    "    * should have lower residual then use those residuals to make another tree \n",
    "    * repeat process until residual become smaller or reach max iterations\n",
    "\n",
    "#### For Classification\n",
    "* default prediction is 0.5, e.g. 50% chance drug is effective\n",
    "* similarity score calculated = sum of residuals squared (same as regression)\n",
    "* denominator has lambda\n",
    "* rest of denominator is different but similar to gradient boost\n",
    "* sum of previous predicted probability x (1- prev probability)\n",
    "* each tree starts as a leaf.. calculate similarity score \n",
    "* set a threshold and calcualte similarity score of each leaf again\n",
    "* calculate gain (try to maximize) - threshold with biggest gain is root\n",
    "* limit levels stops at specified levels\n",
    "* 'cover' = denominator of similarity score + lambda \n",
    "    * minimum value is 1\n",
    "    * in regression - number of residuals (at least 1) - default minimum value has no effect on the tree\n",
    "    * BUT in classification because cover depends on previously predicted probability of the leaf \n",
    "    * when left at minimum of 1  - can prune the whole tree\n",
    "    * can instead set to 0\n",
    "* prunning based on gamma\n",
    "    * gain - gamma = +ve not pruned \n",
    "    * -ve is pruned \n",
    "* values with lambda greater than 0 reduces the sensitivity of the tree to individual values - pruning them and combining with other observations \n",
    "* output of each leaf = sum residual / sum previous prob * 1-prev probability + lambda\n",
    "* similar to normal gradient boost \n",
    "* lambda > 0 reduces the predictions sensitivity to isolated observations (single obs at a leaf node)\n",
    "* learning rate eta (0.3) \n",
    "* output = log(odds) previous Prediction should give a probability\n",
    "* get new residuals from that probability \n",
    "* build new tree from those residuals\n",
    "* previously predicted probabilties now change - from all previous\n",
    "* repeats process - until residuals are lowest \n",
    "* summary\n",
    "    * calculate similarity score + gain - to see how to split the data \n",
    "    * prune tree with difference of gain - tree complexity parameter (gamma) \n",
    "    * positives not pruned, negative pruned\n",
    "    * compare to next higher level node \n",
    "    * calculate output of the leaves\n",
    "    * more pruning done based on lambda\n",
    "    * min residuals of leaf is based on cover "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93c9cabeb8165e6a1575ace97e023eeebe73f88984ce88042818ce2a73501ce8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lhl_env38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
