{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Lecture - Naive Bayesian Classification and Support Vector Machines (SVM)\n",
    "\n",
    "Bayes Theorem\n",
    "* conditional probability review\n",
    "* P(A|B) = P(A) * P(B|A) / P(B)\n",
    "\n",
    "Naive Bayes\n",
    "* nest spam filtering methods\n",
    "* think of learning a problem of 'statistical inference' \n",
    "* based on bayes theorem\n",
    "* assumes that predictors are independent\n",
    "    * presence of one feature unrelated to any other feature\n",
    "    * this is only an assumption - some features might actually be related\n",
    "* useful for large data sets\n",
    "* outperforms even some highly sophisticated classification methods (e.g. text data)\n",
    "* first as classifier but can also be used for regression\n",
    "\n",
    "Naive Bayes Classifier\n",
    "* CountVectorizer - convert text data into feature vectors, counts the occurrence of each word\n",
    "* gives a \"bag of words\"\n",
    "* side note (pipeline) - used for organizing the steps of the data - chains models \n",
    "* called 'MultinomialNB()' and fit on your data\n",
    "* can find accuracy with .score\n",
    "* can then use to predict new messages\n",
    "* can be done probabilistic classifiers \n",
    "* in NLP - given a message, and words - calculate probability of being spam given a message, and same for non-spam\n",
    "* higher probability (spam vs non-spam) gets labelled with that \n",
    "* by hand:\n",
    "    * denominator often ignored - (P(B)) - somewhat irrelevant and difficult to calculate \n",
    "    * essentially calculating marginal probability \n",
    "    * multiplies all conditional probability for each feature given target * marginal prob of target \n",
    "    * some cases if one of the probabilities = 0 - it will all equal 0 - need to convert to probability\n",
    "* when done through sklearn:\n",
    "    * can calculate soft predictions\n",
    "    * laplace smoothing - adds a constant, so zeroes are not coded as zero but 1, and 1 becomes 2\n",
    "        * alpha=1 (hyperparameters)\n",
    "        * high alpha - can lead to underfitting dilutes the information adjusts scores \n",
    "        * low alpha - overfitting - what's seen in training will be your test (?? does it literally just add 1)\n",
    "\n",
    "Gaussian Naive Bayes \n",
    "* calculates probability based on continuous variables, just different formula \n",
    "* usually very accurate, fast for learning corresponding parameters\n",
    "* scales greate - matter of counting how many times each attribute co-occurs with each class\n",
    "* can be used for multiclass classifications\n",
    "* Draw backs\n",
    "    * oversimplifies - though could be useful\n",
    "\n",
    "#### Support Vector Machines\n",
    "* can be applied instead of logistic regression, doesn't need boundary between classes to be linear\n",
    "* similarity based algo, more like weighted K-NNs (supervised cousin of K-means)\n",
    "* boundaries are non-linear - more curves\n",
    "* each training example either is or is not a support vector, decided during 'fit'\n",
    "* decision boundary only depends on the support vectors\n",
    "* model learns weights and biases - similar to logistic reg\n",
    "* separates based on largest margin on a hyperplane \n",
    "* selects important support vectors - once that are closest together but in separate class\n",
    "* maximize distance from support vector to the plane\n",
    "* hyperparameters - what we choose (tunes/configures models) - can be optimized with gridsearch\n",
    "* parameters - what the model learns from the features (weights, intercepts)\n",
    "* hyperparams of SVC affects fundamental trade-off:\n",
    "    * gamma\n",
    "        * controls complexity\n",
    "        * larger = more complex, leading to overfitting\n",
    "        * smaller = less complexity = can lead to underfitting \n",
    "    * C\n",
    "        * larger C = more complex (overfitting)\n",
    "        * smaller = less complex (underfitting)\n",
    "        * changes which SVs are chosen to develop the hyperplane\n",
    "* kernel - can be chosen to not be linear \n",
    "* can do kernel trick - transforms the dimensions, does a linear boundary, and retransforms back to old dimensions \n",
    "* better in terms of prediction but more complex - logistic may be better if done with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Compass\n",
    "\n",
    "Bayes Theorem of Probability\n",
    "* basis for naive bayes\n",
    "* P(A|B) = P(A and B) / P(B)\n",
    "* think about numbers in A and B and create a venn diagram\n",
    "\n",
    "Naive Bayes\n",
    "* probabilistic machine learning - that is used for classification task\n",
    "* based on bayes theorem\n",
    "* assumes all variables are naive and not correlated to each other\n",
    "* each feature is independent and equal (same weight)\n",
    "* these assumptions are usually never correct, but still works in practice\n",
    "* computes conditional probability of *each* variable- multiplied with one another to calculate P(0|A) and P(1|A) note that\n",
    "* compare probabilities P(0|A) vs P(1|A) - which ever is greater is the prediction\n",
    "\n",
    "Gaussian Naive Bayes Classifier\n",
    "* when dealing with normally distributed continuous data\n",
    "* make assumptions regarding the distribution of values for each feature (normal)\n",
    "* select different 'kernels'/'distributions' based on your data\n",
    "* *multinomial* - for count data\n",
    "* *bernoulli/binomial* naive bayes - multivariate bernoulli event model, features are indepednent binary variables, good for determining whether a word is in a document or not (see class example) \n",
    "* very popular for spam detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes model accuracy(in %): 95.0\n"
     ]
    }
   ],
   "source": [
    "# load the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    " \n",
    "# store the feature matrix (X) and response vector (y)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    " \n",
    "# splitting X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    " \n",
    "# training the model on training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    " \n",
    "# making predictions on the testing set\n",
    "y_pred = gnb.predict(X_test)\n",
    " \n",
    "# comparing actual response values (y_test) with predicted response values (y_pred)\n",
    "from sklearn import metrics\n",
    "print(\"Gaussian Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM - Support Vector Machine \n",
    "* used for both classification and regression purposes\n",
    "* more commonly used in classification problems\n",
    "* each data is plotted on n-dimensional space (number of features)\n",
    "* perform classification based on optimal hyper-plane that differentiates the two classes very well \n",
    "\n",
    "From Statquest\n",
    "* finds the hyperplane that separates the two closest values from different classes where distance ('margin') of each value is farthest from the hyperplane\n",
    "* when threshold is halfway - margin is largest it can be (threshold is not really a point if more than one dimension)\n",
    "* in 2 dimensional data - threshold or support vector classifer is a line\n",
    "* in 3 dimensional - it's a plane  \n",
    "* in 4+ dimensions - the classifer is a hyperplane (all are technically hyperplanes)\n",
    "* 'maximal margin classifer' point where they are separated \n",
    "* maximum/margin classifer falls short when there are outlier observations that are close to one class but belong to another \n",
    "* to address - *allow misclassiciations*\n",
    "* allows for bias/variance tradeoff\n",
    "* distance between points to threshold w/ misclassification = 'soft margin'\n",
    "* crossvalidation to get the best soft-margin - how many missclass/observations allowed inside to get the best classification\n",
    "* i.e we are using 'soft margin classifer' AKA support vector machine \n",
    "* support vectors: the observations on the edge* and within the soft margin classifier (picture number line)\n",
    "* values inside the soft margin can be misclassified - use cross validation - to make sure that allowing misclassificaiton is better for the long run \n",
    "\n",
    "Sandwiched data i.e. lots of overlap\n",
    "* can't be handled by maximal margin classifer or support vector classifiers\n",
    "* BUT can be handled by support vector MACHINES \n",
    "* start with data in low dimension (e.g. 1 dimension) - then moved the data to higher dimension (e.g. squaring the value, giving it a y-axis) - then find the support vector classifer to separate the higher dimensional data\n",
    "* how to decide to transform the data - finds specific \"kernel functions\" to systematically find support vector classifers in higher dimensions (i.e. should data be squared, cubed, etc)\n",
    "* using kernel functions:\n",
    "* polynomial degree 1 point, 2 line - finds relationship between each pairs to find the support vector classifer\n",
    "* systematically increases degree to find support vector classifer - and can find a good value for D with cross validation\n",
    "* common kernel: radial kernel in infinite dimensions (weighted nearest neighbours)\n",
    "* 'kernel trick'- transformation doesn't actually transform the data - but only for computation of correct SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhl_env38",
   "language": "python",
   "name": "lhl_env38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
